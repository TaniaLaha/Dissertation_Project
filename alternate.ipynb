{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction of title from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = '2306.06804.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract title of a pdf\n",
    "\n",
    "def extract_title(pdf_file):\n",
    "\n",
    "  pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "  document_info = pdf_reader.metadata\n",
    "  #print(document_info)\n",
    "  pdf_title = document_info.get('/Title')\n",
    "  return pdf_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = extract_title(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', PyPDF2.generic._base.TextStringObject)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title, type(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for fontsize and fontname of each of lines within pdf\n",
    "import fitz\n",
    "\n",
    "def scrape(filePath):\n",
    "    results = [] # list of tuples that store the information as (text, font size, font name) \n",
    "    pdf = fitz.open(filePath) # filePath is a string that contains the path to the pdf\n",
    "    for page in pdf:\n",
    "        dict = page.get_text(\"dict\")\n",
    "        blocks = dict[\"blocks\"]\n",
    "        for block in blocks:\n",
    "            if \"lines\" in block.keys():\n",
    "                spans = block['lines']\n",
    "                for span in spans:\n",
    "                    data = span['spans']\n",
    "                    for lines in data:\n",
    "                            results.append((lines['text'], lines['size'], lines['font']))\n",
    "                            # lines['text'] -> string, lines['size'] -> font size, lines['font'] -> font name\n",
    "    pdf.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Neural Machine Translation for the Indigenous Languages of the',\n",
       "  14.346199989318848,\n",
       "  'NimbusRomNo9L-Medi'),\n",
       " ('Americas: An Introduction.', 14.346199989318848, 'NimbusRomNo9L-Medi'),\n",
       " ('Manuel Mager', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('♡', 7.970099925994873, 'CMSY8'),\n",
       " ('∗', 7.970099925994873, 'CMSY8'),\n",
       " (' ', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Rajat Bhatnagar', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('♠', 7.970099925994873, 'CMSY8'),\n",
       " ('Graham Neubig', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('♯', 7.970099925994873, 'CMMI8'),\n",
       " ('Ngoc Thang Vu', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('♢', 7.970099925994873, 'CMSY8'),\n",
       " (' ', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Katharina Kann', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('♠', 7.970099925994873, 'CMSY8'),\n",
       " ('♡', 7.970099925994873, 'CMSY8'),\n",
       " ('AWS AI Labs', 11.9552001953125, 'NimbusRomNo9L-Regu'),\n",
       " ('♯', 7.970099925994873, 'CMMI8'),\n",
       " ('Carnegie Mellon University', 11.9552001953125, 'NimbusRomNo9L-Regu'),\n",
       " ('♠', 7.970099925994873, 'CMSY8'),\n",
       " ('University of Colorado Boulder', 11.9552001953125, 'NimbusRomNo9L-Regu'),\n",
       " ('♢', 7.970099925994873, 'CMSY8'),\n",
       " ('University of Stuttgart', 11.9552001953125, 'NimbusRomNo9L-Regu'),\n",
       " ('Abstract', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Neural models have drastically advanced state',\n",
       "  9.962599754333496,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('of the art for machine translation (MT) be-',\n",
       "  9.962599754333496,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('tween high-resource languages. Traditionally,',\n",
       "  9.962599754333496,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('these models rely on large amounts of train-',\n",
       "  9.962599754333496,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('ing data, but many language pairs lack these',\n",
       "  9.962599754333496,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('resources. However, an important part of the',\n",
       "  9.962599754333496,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('languages in the world do not have this amount',\n",
       "  9.962599754333496,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('of data. Most languages from the Americas are',\n",
       "  9.962599754333496,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('among them, having a limited amount of par-',\n",
       "  9.962599754333496,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('allel and monolingual data, if any. Here, we',\n",
       "  9.962599754333496,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('present an introduction to the interested reader',\n",
       "  9.962599754333496,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('to the basic challenges, concepts, and tech-',\n",
       "  9.962599754333496,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('niques that involve the creation of MT systems',\n",
       "  9.962599754333496,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('for these languages. Finally, we discuss the',\n",
       "  9.962599754333496,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('recent advances and findings and open ques-',\n",
       "  9.962599754333496,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('tions, product of an increased interest of the',\n",
       "  9.962599754333496,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('NLP community in these languages.',\n",
       "  9.962599754333496,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('1', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Introduction', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('More than 7 billion people on Earth communicate',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('in nearly 7000 different languages (',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('Pereltsvaig', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('2020', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (').', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Of these, approximately 900 languages',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('are native of the American continent (',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('Campbell', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('2000', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('). Most of these indigenous languages of the',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('Americas (ILA) are endangered at some degree',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('(', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Thomason', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2015', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('). This huge variety in languages',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('is simultaneously a rich treasure for humanity and',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('also a barrier to communication among people',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('from different backgrounds.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Human translators', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('have been important in overcoming language bar-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('riers. However, trained translators are not acces-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('sible to everyone on Earth and even scarcer for',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('endangered and minority languages.',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('The need', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('for translations is even written in the constitutions',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('of several countries like Mexico, Peru, Paraguay,',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('Venezuela, and Bolivia (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Zajícová', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2017', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (') to allow', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('native speakers to have equal language rights re-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('garding law.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('This is why developing MT is crucial: it helps',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('humanity overcome language barriers while si-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('multaneously allowing people to continue using',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('∗', 5.97760009765625, 'CMSY6'),\n",
       " ('Work done while at the University of Stuttgart.',\n",
       "  8.966400146484375,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('their native tongue. However, the challenges to',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('achieving these problems are not trivial. It is not',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('only the amount of available data (a common the-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('sis among the NLP community) but also a set',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('of challenging issues (dialectical and orthographic',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('variations, noisy texts, complex morphology, etc.)',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('that must be addressed.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('MT has always been an important task within',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('the larger area of natural language processing',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('(NLP). In 1954, the Georgetown–IBM experiment',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('(', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Hutchins', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2004', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (') was the first that showed at least',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('some effectiveness of MT. Further research re-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('sulted in rule-based systems and statistical models.',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('In 2023, neural models define state of the art for',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('MT if training data is plentiful – i.e., for so-called',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('high-resource languages (HRLs) – and have also',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('achieved impressive results for low-resource lan-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('guages (LRLs). MT is also the most studied NLP',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('task for the ILA (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Mager et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2018b', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (';', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' Littell et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('2018', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('). The common issue among these languages',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('is the extreme low-resource conditions they are',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('confronted with. The research interest for these',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('languages has increased in the last years, including',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('the recent AmericasNLP 2021 shared task (',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('Mager', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2021', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (') on 10 indigenous languages to Span-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('ish, and the WMT (Conference on Machine Trans-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('lation) shared task for Inuktitut–English (',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('Barrault', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2020', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (').', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('In this work we aim to provide a comprehensive',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('introduction to the challenges that involve creat-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('ing MT systems for ILA, and the current status',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('of the existing work. We organize this work as',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('follows: We start by introducing state-of-the-art',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('NMT models (§', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('2', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('). Then, we discuss the current', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('challenges for these languages (§',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('3', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('); and we in-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('troduce the key concepts related to low-resource',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('NMT and the implications for endangered lan-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('guages of the Americas(§', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('3', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('). This is followed by', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('a discussion of available data (§',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('4', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('). Afterwards,', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('we introduce the important concepts for LRL and',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('endangered languages (§', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('5', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('); then we introduce the', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('arXiv:2306.06804v1  [cs.CL]  11 Jun 2023', 20.0, 'Times-Roman'),\n",
       " ('main strategies aimed at improving NMT with',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('limited training data (§', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('6', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('); and finally we give an', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('overview of the work done for ILA on MT (§',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('7', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (').', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('In doing so, we provide insights into the follow-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('ing questions: Which systems define the state of',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('the art on low-resource NMT applied to the ILA?',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('What is the route that ahead to improve the trans-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('lations of the ILA?', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('2', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Background and Definitions', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Formally, the task of MT consists of converting',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('text', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' X', 10.909099578857422, 'CMMI10'),\n",
       " (' in a source language', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' L', 10.909099578857422, 'CMMI10'),\n",
       " ('x', 7.970099925994873, 'CMMI8'),\n",
       " (' into text', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' Y', 10.909099578857422, 'CMMI10'),\n",
       " (' in a', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('target language', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' L', 10.909099578857422, 'CMMI10'),\n",
       " ('y', 7.970099925994873, 'CMMI8'),\n",
       " (' that conveys the same mean-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('ing.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('1', 7.970099925994873, 'NimbusRomNo9L-Regu'),\n",
       " (' ', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Translating text', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' X', 10.909099578857422, 'CMMI10'),\n",
       " (' ∈', 10.909099578857422, 'CMSY10'),\n",
       " (' L', 10.909099578857422, 'CMMI10'),\n",
       " ('x', 7.970099925994873, 'CMMI8'),\n",
       " (' into', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' Y', 10.909099578857422, 'CMMI10'),\n",
       " (' ∈', 10.909099578857422, 'CMSY10'),\n",
       " (' L', 10.909099578857422, 'CMMI10'),\n",
       " ('y', 7.970099925994873, 'CMMI8'),\n",
       " (' can be', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('described as a function (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Neubig', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2017', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('):', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Y', 10.909099578857422, 'CMMI10'),\n",
       " (' =', 10.909099578857422, 'CMR10'),\n",
       " (' MT', 10.909099578857422, 'NimbusMonL-Regu'),\n",
       " ('(', 10.909099578857422, 'CMR10'),\n",
       " ('X', 10.909099578857422, 'CMMI10'),\n",
       " (')', 10.909099578857422, 'CMR10'),\n",
       " ('.', 10.909099578857422, 'CMMI10'),\n",
       " ('(1)', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('X', 10.909099578857422, 'CMMI10'),\n",
       " (' and', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' Y', 10.909099578857422, 'CMMI10'),\n",
       " (' can be of variable length, such as',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('phrases, sentences, or even documents.',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('If other languages are used during the transla-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('tion process, e.g., as pivots, we denote them as',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('L', 10.909099578857422, 'CMMI10'),\n",
       " ('1', 7.970099925994873, 'CMR8'),\n",
       " (', . . . , L', 10.909099578857422, 'CMMI10'),\n",
       " ('n', 7.970099925994873, 'CMMI8'),\n",
       " ('. We refer to a corpus of monolingual',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('sentences in language', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' L', 10.909099578857422, 'CMMI10'),\n",
       " ('i', 7.970099925994873, 'CMMI8'),\n",
       " (' as', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' M', 10.909099578857422, 'CMMI10'),\n",
       " ('L', 7.970099925994873, 'CMMI8'),\n",
       " ('i', 5.97760009765625, 'CMMI6'),\n",
       " (' ', 10.909099578857422, 'CMR10'),\n",
       " ('=', 10.909099578857422, 'CMR10'),\n",
       " (' S', 10.909099578857422, 'CMMI10'),\n",
       " ('1', 7.970099925994873, 'CMR8'),\n",
       " (', ..., S', 10.909099578857422, 'CMMI10'),\n",
       " ('n', 7.970099925994873, 'CMMI8'),\n",
       " ('.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Probabilistic Modeling and Data', 10.909099578857422, 'NimbusRomNo9L-Medi'),\n",
       " ('When us-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('ing probabilistic MT models, the goal is to find',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('Y', 10.909099578857422, 'CMMI10'),\n",
       " (' ∈', 10.909099578857422, 'CMSY10'),\n",
       " (' L', 10.909099578857422, 'CMMI10'),\n",
       " ('y', 7.970099925994873, 'CMMI8'),\n",
       " (' with the highest conditional probability,',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('given', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' X', 10.909099578857422, 'CMMI10'),\n",
       " (' ∈', 10.909099578857422, 'CMSY10'),\n",
       " (' L', 10.909099578857422, 'CMMI10'),\n",
       " ('x', 7.970099925994873, 'CMMI8'),\n",
       " ('. Under the supervised machine', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('learning paradigm, a parallel corpus',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " (' C', 10.909099578857422, 'CMMI10'),\n",
       " ('parallel', 7.970099925994873, 'CMMI8'),\n",
       " (' =', 10.909099578857422, 'CMR10'),\n",
       " ('(', 10.909099578857422, 'CMR10'),\n",
       " ('X', 10.909099578857422, 'CMMI10'),\n",
       " ('1', 7.970099925994873, 'CMR8'),\n",
       " (', Y', 10.909099578857422, 'CMMI10'),\n",
       " ('1', 7.970099925994873, 'CMR8'),\n",
       " (')', 10.909099578857422, 'CMR10'),\n",
       " (', ...,', 10.909099578857422, 'CMMI10'),\n",
       " (' (', 10.909099578857422, 'CMR10'),\n",
       " ('X', 10.909099578857422, 'CMMI10'),\n",
       " ('n', 7.970099925994873, 'CMMI8'),\n",
       " (', Y', 10.909099578857422, 'CMMI10'),\n",
       " ('n', 7.970099925994873, 'CMMI8'),\n",
       " (')', 10.909099578857422, 'CMR10'),\n",
       " (' is used to learn a set of pa-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('rameters', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' θ', 10.909099578857422, 'CMMI10'),\n",
       " (', which define a probability distribution',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('over possible translations.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Given', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' C', 10.909099578857422, 'CMMI10'),\n",
       " ('parallel', 7.970099925994873, 'CMMI8'),\n",
       " (', the', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('training objective of an NMT model is generally',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('to maximize the log-likelihood', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' L', 10.909099578857422, 'CMSY10'),\n",
       " (' with respect to', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('θ', 10.909099578857422, 'CMMI10'),\n",
       " (':', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('L', 10.909099578857422, 'CMSY10'),\n",
       " ('θ', 7.970099925994873, 'CMMI8'),\n",
       " (' =', 10.909099578857422, 'CMR10'),\n",
       " ('�', 10.909099578857422, 'CMEX10'),\n",
       " ('(', 7.970099925994873, 'CMR8'),\n",
       " ('X', 7.970099925994873, 'CMMI8'),\n",
       " ('i', 5.97760009765625, 'CMMI6'),\n",
       " (',Y', 7.970099925994873, 'CMMI8'),\n",
       " ('i', 5.97760009765625, 'CMMI6'),\n",
       " (')', 7.970099925994873, 'CMR8'),\n",
       " ('∈', 7.970099925994873, 'CMSY8'),\n",
       " ('C', 7.970099925994873, 'CMMI8'),\n",
       " ('parallel', 5.97760009765625, 'CMMI6'),\n",
       " ('log', 10.909099578857422, 'CMR10'),\n",
       " (' p', 10.909099578857422, 'CMMI10'),\n",
       " ('(', 10.909099578857422, 'CMR10'),\n",
       " ('Y', 10.909099578857422, 'CMMI10'),\n",
       " ('i', 7.970099925994873, 'CMMI8'),\n",
       " ('|', 10.909099578857422, 'CMSY10'),\n",
       " ('X', 10.909099578857422, 'CMMI10'),\n",
       " ('i', 7.970099925994873, 'CMMI8'),\n",
       " (';', 10.909099578857422, 'CMR10'),\n",
       " (' θ', 10.909099578857422, 'CMMI10'),\n",
       " (')', 10.909099578857422, 'CMR10'),\n",
       " ('.', 10.909099578857422, 'CMMI10'),\n",
       " ('(2)', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Within this overall framework, there are a num-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('ber of design decisions one has to make, such as',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('which model architecture to use, how to generate',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('translations, and how to evaluate.',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('Decoding', 10.909099578857422, 'NimbusRomNo9L-Medi'),\n",
       " ('Decoding refers to the generation of',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('output', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' ', 10.909099578857422, 'CMR10'),\n",
       " ('ˆ', 10.909099578857422, 'CMR10'),\n",
       " ('Y', 10.909099578857422, 'CMMI10'),\n",
       " (' , given the parameters', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' θ', 10.909099578857422, 'CMMI10'),\n",
       " (' and an input', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' X', 10.909099578857422, 'CMMI10'),\n",
       " ('.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Often, decoding is done by approximately solving',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('the following maximization problem:',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('argmax', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' ˆ', 7.970099925994873, 'CMR8'),\n",
       " ('Y', 7.970099925994873, 'CMMI8'),\n",
       " (' p', 10.909099578857422, 'CMMI10'),\n",
       " ('( ', 10.909099578857422, 'CMR10'),\n",
       " ('ˆ', 10.909099578857422, 'CMR10'),\n",
       " ('Y', 10.909099578857422, 'CMMI10'),\n",
       " (' |', 10.909099578857422, 'CMSY10'),\n",
       " ('X', 10.909099578857422, 'CMMI10'),\n",
       " (';', 10.909099578857422, 'CMR10'),\n",
       " (' θ', 10.909099578857422, 'CMMI10'),\n",
       " (')', 10.909099578857422, 'CMR10'),\n",
       " ('(3)', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('1', 5.97760009765625, 'NimbusRomNo9L-Regu'),\n",
       " ('This is an approximation, since it is in general not possi-',\n",
       "  8.966400146484375,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('ble to map the meaning of text exactly into another language',\n",
       "  8.966400146484375,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('(', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('Nida', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (',', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (' 1945', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (';', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (' Sechrest et al.', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (',', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (' 1972', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (';', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (' Baker', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (',', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (' 2018', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (').', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('Most NMT systems factorize the probability of',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('ˆ', 10.909099578857422, 'CMR10'),\n",
       " ('Y', 10.909099578857422, 'CMMI10'),\n",
       " (' = ˆ', 10.909099578857422, 'CMR10'),\n",
       " ('y', 10.909099578857422, 'CMMI10'),\n",
       " ('1', 7.970099925994873, 'CMR8'),\n",
       " (', ...,', 10.909099578857422, 'CMMI10'),\n",
       " (' ˆ', 10.909099578857422, 'CMR10'),\n",
       " ('y', 10.909099578857422, 'CMMI10'),\n",
       " ('T', 7.970099925994873, 'CMMI8'),\n",
       " (' in a left-to-right fashion:', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('p', 10.909099578857422, 'CMMI10'),\n",
       " ('( ', 10.909099578857422, 'CMR10'),\n",
       " ('ˆ', 10.909099578857422, 'CMR10'),\n",
       " ('Y', 10.909099578857422, 'CMMI10'),\n",
       " (' ) =', 10.909099578857422, 'CMR10'),\n",
       " ('T', 7.970099925994873, 'CMMI8'),\n",
       " ('�', 10.909099578857422, 'CMEX10'),\n",
       " ('t', 7.970099925994873, 'CMMI8'),\n",
       " ('=1', 7.970099925994873, 'CMR8'),\n",
       " ('p', 10.909099578857422, 'CMMI10'),\n",
       " ('(ˆ', 10.909099578857422, 'CMR10'),\n",
       " ('y', 10.909099578857422, 'CMMI10'),\n",
       " ('t', 7.970099925994873, 'CMMI8'),\n",
       " ('|', 10.909099578857422, 'CMSY10'),\n",
       " ('ˆ', 10.909099578857422, 'CMR10'),\n",
       " ('y', 10.909099578857422, 'CMMI10'),\n",
       " ('<t', 7.970099925994873, 'CMMI8'),\n",
       " (', X, θ', 10.909099578857422, 'CMMI10'),\n",
       " (')', 10.909099578857422, 'CMR10'),\n",
       " ('(4)', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Thus, the probability of token', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' ˆ', 10.909099578857422, 'CMR10'),\n",
       " ('y', 10.909099578857422, 'CMMI10'),\n",
       " ('t', 7.970099925994873, 'CMMI8'),\n",
       " (' at time step', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' t', 10.909099578857422, 'CMMI10'),\n",
       " (' is', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('computed using the previously generated tokens',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('ˆ', 10.909099578857422, 'CMR10'),\n",
       " ('y', 10.909099578857422, 'CMMI10'),\n",
       " ('<t', 7.970099925994873, 'CMMI8'),\n",
       " (', the source sentence', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' X', 10.909099578857422, 'CMMI10'),\n",
       " (' and the model param-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('eters', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' θ', 10.909099578857422, 'CMMI10'),\n",
       " ('. Common algorithms for finding a high-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('probability translation are greedy decoding, i.e.,',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('picking the token with the highest probability at',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('each time step, and beam search (',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('Lowerre', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 1976', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (').', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('2.1', 10.909099578857422, 'NimbusRomNo9L-Medi'),\n",
       " ('Input Representations', 10.909099578857422, 'NimbusRomNo9L-Medi'),\n",
       " ('The texts', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' X', 10.909099578857422, 'CMMI10'),\n",
       " (' and', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' Y', 10.909099578857422, 'CMMI10'),\n",
       " (' are input into an NMT sys-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('tem as sequences of continuous vectors.',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('How-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('ever, defining which units should be represented',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('as such vectors is non-trivial.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('The classic way', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('is to represent each', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' word', 10.909099578857422, 'NimbusRomNo9L-ReguItal'),\n",
       " (' within', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' X', 10.909099578857422, 'CMMI10'),\n",
       " (' and', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' Y', 10.909099578857422, 'CMMI10'),\n",
       " (' as', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('a vector (or embedding).', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('However, in a low-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('resource setting, often not all vocabulary items ap-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('pear in the training data (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Jean et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2015', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (';', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' Lu-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('ong et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2015', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('). This issue especially effects lan-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('guages with a rich inflectional morphology (',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('Sen-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('nrich et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2016c', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('): as many word forms can', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('represent the same lemma, the vocabulary cover-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('age decreases drastically. Furthermore, for many',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('LRLs, boundaries between words or morphemes',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('are not easy to obtain or not well defined in the',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('case of languages without a standard orthography.',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('Alternative input units have been explored, such as',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('characters (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Ling et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2015', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('), byte pair encoding', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('(BPE;', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' Sennrich et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2016a', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('), morphological rep-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('resentations (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Vania and Lopez', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2017', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (';', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' Ataman and', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Federico', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2018', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('), syllables (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Zhang et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2019', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('), or,', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('recently, a visual representation of rendered text',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('(', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Salesky et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2021', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('). No clear advantage has been', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('discovered for using morphological segmentations',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('over BPEs when testing them on LRLs (',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('Saleva and', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Lignos', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2021', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (').', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Input representations can be pretrained.',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('The', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('two most common options are:', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('i) word em-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('beddings, where each type is represented by a',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('vector, e.g., Word2Vec (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Mikolov et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2013', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('),', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Glove (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Pennington et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2014', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('), or Fasttext (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Bo-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('janowski et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2017', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (')) embeddings, and ii) contex-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('tualized word representations, where entire sen-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('tences are being encoded at a time, e.g., ELMo',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('(', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Peters et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2018', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (') or BERT (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Devlin et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('2019', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (').', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('However, training of these methods re-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('quires large monolingual training corpora, which',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('may not be readily available for LRLs. As most',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('ILA have rich morphology, this topic has gath-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('ered special interest. The discussion about the us-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('age of morpholigical segmented input for NMT',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('models is recurrent.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('(', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Mager et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2022', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (') show', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('that the unsupervised morphologically inspired',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('models outperform BPE pre-processing (experi-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('mented on 4 language pares).', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Similar experi-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('ments done on Quechua–Spanish and Inuktitut–',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('Enlgish (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Schwartz et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2020', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('), comparing BPEs', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('against Morfessor (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Smit et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2014', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('). Also (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Or-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('tega et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2020a', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (') improves the SOTA (state-of-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('the-art) for Quechua–Spanish MT using a mor-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('phological guided BPE algorithm.',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('2.2', 10.909099578857422, 'NimbusRomNo9L-Medi'),\n",
       " ('Architectures', 10.909099578857422, 'NimbusRomNo9L-Medi'),\n",
       " ('NMT models typically are sequence-to-sequence',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('models. They encode a variable-length sequence',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('into a vector or matrix representation, which they',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('then decode back into a variable-length sequence',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('(', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Cho et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2014', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('). The two most frequent archi-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('tectures are: i) recurrent neural networks (RNN),',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('such as LSTMs (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Hochreiter and Schmidhuber', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('1997', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (') or GRUs (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Cho et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2014', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('), and ii) trans-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('formers (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Vaswani et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2017', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('), which define the', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('current state of the art in the high-resource setting.',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('As for most neural network models, training an',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('NMT system on a limited number of instances',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('is challenging (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Fernández-Delgado et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2014', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (').', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('There are common problems that arise from lim-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('ited data in the training set. One major advan-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('tage of neural models is their ability to learn rep-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('resentations from raw data, in contrast to manu-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('ally engineered features (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Barron', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 1993', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('). However,', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('problems arise when not enough data is provided',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('to enable effective learning of features. Another',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('strength of neural networks is their generalization',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('capacity (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Kawaguchi et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2017', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('). However, train-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('ing a neural network on a small dataset easily leads',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('to overfitting (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Rolnick et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2017', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('). Recent stud-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('ies, however, show empirically that this does not',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('necessarily happen if the network is tuned cor-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('rectly (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Olson et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2018', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (').', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('2.3', 10.909099578857422, 'NimbusRomNo9L-Medi'),\n",
       " ('Evaluation', 10.909099578857422, 'NimbusRomNo9L-Medi'),\n",
       " ('Accurately judging translation quality is difficult',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('and, thus, often still done manually: bilingual',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('speakers assign scores according to provided crite-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('ria such as fluency and adequacy (',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('Does the output', 10.909099578857422, 'NimbusRomNo9L-ReguItal'),\n",
       " ('have the same meaning as the input?',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-ReguItal'),\n",
       " ('). However,', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('manual evaluation is expensive and slow. More-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('over, in the case of endangered languages, bilin-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('gual speakers can be hard or impossible to find.',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('Automatic metrics provide an alternative.',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('2', 7.970099925994873, 'NimbusRomNo9L-Regu'),\n",
       " ('These metrics assign a score to system output,',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('given one or more ground truth reference transla-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('tions. The most widely used metric is BLEU (',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('Pa-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('pineni et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2002', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('), which relies on token-level', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' n', 10.909099578857422, 'CMMI10'),\n",
       " ('-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('gram matches between the translation to be rated',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('and one or more gold-standard translations. For',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('morphologically rich languages, character-level',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('metrics, such as chrF (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Popovi´c', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2017', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('), are often', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('more suitable, as they allow for more flexibility.',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('In the AmericasNLP ST (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Mager et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2021', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (') this', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('metric was used over BLEU, as it fits better to the',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('rich morphology of many ILA.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('To have a concrete example, lets have the fol-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('lowing Wixarika phrase with an English transla-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('tion:', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('yu-huta-me', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('ne-p+-we-’iwa', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('an-two-ns', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('1sg:s-asi-2pl:o-brother', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('I have two brothers', 10.909099578857422, 'NimbusRomNo9L-ReguItal'),\n",
       " ('As discussed in (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Mager et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2018c', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (') it is dif-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('ficult to translate back from Spanish (or other Fu-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('sional language) the morpheme', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' p+', 10.909099578857422, 'NimbusRomNo9L-ReguItal'),\n",
       " (' as it has not', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('equivalent in these languages. So if we would ig-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('nore these morpheme at all, BLEU would penal-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('ize the entire word', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' nep+we’iwa', 10.909099578857422, 'NimbusRomNo9L-ReguItal'),\n",
       " ('. In contrast, chrF', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('would give credit to the translation, even if the',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " (' p+', 10.909099578857422, 'NimbusRomNo9L-ReguItal'),\n",
       " ('is missing.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('One shortcoming of these evaluation metrics is',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('that the evaluation is very dependent on the sur-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('face forms and not on the ultimate goal of seman-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('tic similarity and fluency. Recent work uses pre-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('trained models to evaluate semantic similarity be-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('tween translations and the gold standard (',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('Zhang', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2020d', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('), but these methods are limited to lan-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('guages for which such models are available. This',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('is not possible for the ILA, as the amount of mono-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('lingual data is not enough to train a reliable pre-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('trained language model', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('3', 7.970099925994873, 'NimbusRomNo9L-Regu'),\n",
       " ('.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('3', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Challenges and open questions', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('In an overview of the datasets and recent studies',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('of MT for the ILA, we found the following main',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('issues to be handled.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('2', 5.97760009765625, 'NimbusRomNo9L-Regu'),\n",
       " ('For a detailed overview of automatic metrics for MT we',\n",
       "  8.966400146484375,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('refer the interested reader to specialized reviews (',\n",
       "  8.966400146484375,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('Han', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (',', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (' 2016', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (';', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('Celikyilmaz et al.', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (',', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (' 2020', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (';', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (' Chatzikoumi', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (',', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (' 2020', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (').', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('3', 5.97760009765625, 'NimbusRomNo9L-Regu'),\n",
       " ('One exception to this is Quechua, that has a large enough',\n",
       "  8.966400146484375,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('monolingual dataset to train a BERT like model (',\n",
       "  8.966400146484375,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('Zevallos', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('et al.', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (',', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (' 2022', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (')', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('Extreme low-resource parallel datasets',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Medi'),\n",
       " ('Even', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('with the recent advances, the resources available',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('to train MT systems are extremely scarce, hav-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('ing training set between 4k and 20k sentences (see',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('§', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('4', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('), with notable exceptions for Inuktitut, Guarani',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('and Quechua (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Joanis et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2020', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (';', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' Ortega et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('2020a', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (').', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Lack of monolingual data', 10.909099578857422, 'NimbusRomNo9L-Medi'),\n",
       " ('Most of these lan-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('guages are mostly used in spoken form.',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('In re-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('cent years, with the advancement and democra-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('tization of mobile technologies, indigenous lan-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('guages have seen a slight increase in massaging',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('systems and private spheres (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Rosales et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('). How-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('ever, the usage of these languages on the internet',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('is rather limited. Even Wikipedia has a limited',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('amount of these languages (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Mager et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2018b', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (').', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Low domain diversity', 10.909099578857422, 'NimbusRomNo9L-Medi'),\n",
       " ('.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('As most parallel', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('datasets are scarce, they are restricted to a small',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('number of domains, making it challenging to',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('adapt it, or try to aim for general translation mod-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('els. This has been recognized as a major problem',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('during the AmericasNLP ST (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Mager et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2021', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (').', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Rich morphology', 10.909099578857422, 'NimbusRomNo9L-Medi'),\n",
       " ('An important number of', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('these languages are morphological highly rich. In',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('many cases, we find polysynthetic, with or highly',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('agglutinative languages (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Kann et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2018', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (') or even', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('fusional phenomenon (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Mager et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2020', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (').', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Distant paired language', 10.909099578857422, 'NimbusRomNo9L-Medi'),\n",
       " ('The most common', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('languages that we find that ILA is translated into',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('are Spanish, English, and Portuguese. However,',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('these languages are distantly related to the ILA,',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('and have completely different linguistically phe-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('nomenons (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Campbell', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2000', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (';', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' Romero et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2016', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (').', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Noisy text environments', 10.909099578857422, 'NimbusRomNo9L-Medi'),\n",
       " ('Monolingual texts, if', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('exist, are found in social media that often use a',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('non-canonical witting (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Rosales et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (').', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Code-Swithing', 10.909099578857422, 'NimbusRomNo9L-Medi'),\n",
       " ('This phenomenon is strongly', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('present in ILA, as all of these languages are mi-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('nority languages in their own countries.',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('The', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('bilingualism among their communities is strong',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('(and CS is a common phenomenon in this setup',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('(', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Çetino˘glu', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2017', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (')). The final result of this phe-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('nomenon is the inclusion of code-switching on a',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('common base (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Mager et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2019', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (') in their lan-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('guage.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Lack of orthographic normalization',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Medi'),\n",
       " ('The us-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('age of ILA faces the problem of having a unified',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('orthographic standard. This is not always possi-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('ble, as the suggestions of linguists and official en-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('tities do not always match the day-by-day writ-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('ing of the speakers.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Moreover, in some cases,', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('special symbols present in the orthographic stan-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('dards are not accessible in English or Spanish key-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('board and need to be replaced with other symbols.',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('The winner of the AmericasNLP ST got important',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('improvements using orthographic normalizers de-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('veloped specifically for each American language',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('(', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Vázquez et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2021', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (').', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Dialectal variety', 10.909099578857422, 'NimbusRomNo9L-Medi'),\n",
       " ('The indigenous languages', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('have a strong dialectal variety, making it hard for',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('native speakers to understand even speakers from',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('neighboring villages. The linguistic richness of',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('entire regions is so diverse that even a single state',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('like the Mexican Oaxaca could correspond to the',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('diversity in the whole Europe (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('McQuown', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 1955', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (').', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('4', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Available MT datasets for ILA', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('The parallel datasets available for MT have been',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('increasing during the last years. At this moment,',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('we can show in two folds the development of these',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('resources: as shown in table', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' work on specific', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('language has emerged; but also broader datasets',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('have started to cover the ILA (see table',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " (' 1', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (').', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Language-specific corpus collection work has',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('been done for many languages, where parallel',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('corpus has been the main component.',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('In re-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('cent time we have seen Cherokee–English (OPUS)',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('(', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Zhang et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2020c', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('), Wixarika–Spanish (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Mager', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2018a', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('), Shipio–Konibo (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Feldman and Coto-', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Solano', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2020', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('), and others (see table', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('). The most', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('prominent of these datasets has been the Inuktitut–',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('English parallel data.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('The last version of this', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('dataset corpora (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Joanis et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2020', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (') is has medium', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('size with 1,450,094 sentences. Previous versions',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('of this corpus are (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Martin et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2003', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('). This data', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('set was used for the WMT 2020 Shared Task on',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('Unsupervised, and Low Resourced MT (',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('Barrault', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2020', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (').', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('For wide-spoken languages like Guarani, it is',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('even possible to collect a web crawled dataset,',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('including news articles and social media parallel',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('aligned data (', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('Chiruzzo et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' 2020', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (';', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (' Góngora et al.', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (',', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('2021', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (') This dataset also includes monolingual data.',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('This is possible as Guaraní is one of the most spo-',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('ken indigenous languages of the continent.',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('In contrast to the language-specific datasets,',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('we find broader approaches (see table',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " (' 1', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " (').', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('The', 10.909099578857422, 'NimbusRomNo9L-Regu'),\n",
       " ('broadest multilingual dataset, which contains the',\n",
       "  10.909099578857422,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('Dataset', 8.966400146484375, 'NimbusRomNo9L-Medi'),\n",
       " ('Paired-languages', 8.966400146484375, 'NimbusRomNo9L-Medi'),\n",
       " ('Authors', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('AmericasNLI', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('Aymara, Asháninka, Bribri, Guaraní,',\n",
       "  8.966400146484375,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('Nahuatl, Otomí, Quechua, Rarámuri,',\n",
       "  8.966400146484375,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('Shipibo-Konibo, Wixarika', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('(', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('Ebrahimi et al.', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (',', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (' 2022', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (')', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('CPML', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('Ch’ol, Maya, Mazatec, Mixtec, Nahu-',\n",
       "  8.966400146484375,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('atl and Otomi', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('(', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('Sierra Martínez et al.', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (',', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (' 2020', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (')', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('OPUS', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('*', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('(', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('Tiedemann', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (',', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (' 2016', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (')', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('New testament Bible *', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('(', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('McCarthy et al.', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (',', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (' 2020', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (')', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('Table 1: Parallel dataset collections that contain one or more indigenous languages of the Americas',\n",
       "  9.962599754333496,\n",
       "  'NimbusRomNo9L-Regu'),\n",
       " ('Language', 8.966400146484375, 'NimbusRomNo9L-Medi'),\n",
       " ('Paried-language ISO', 8.966400146484375, 'NimbusRomNo9L-Medi'),\n",
       " ('Family', 8.966400146484375, 'NimbusRomNo9L-Medi'),\n",
       " ('Sentences', 8.966400146484375, 'NimbusRomNo9L-Medi'),\n",
       " ('Domain', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('Authors', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('Asháninka', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('Spanish', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('cni', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('Arawak', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('3883', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('(', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('Ortega et al.', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (',', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (' 2020b', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (')', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('Bribri', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('Spanish', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('bzd', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('Chibchan', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('5923', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('(', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('Feldman', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('and', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('Coto-', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('Solano', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (',', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (' 2020', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " (')', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('Guarani', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('Spanish', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ('gn', 8.966400146484375, 'NimbusRomNo9L-Regu'),\n",
       " ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = scrape(pdf_path)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF title is:\n",
      "\n",
      " DATA\n"
     ]
    }
   ],
   "source": [
    "# Checking is Title was extracted correctly\n",
    "if title is None or title == '':\n",
    "      new_title = ''\n",
    "      max_font_size = max(output, key=lambda x: x[1])[1]\n",
    "      elements_with_max_font = [element for element in output if element[1] == max_font_size]\n",
    "\n",
    "      print(\"PDF title is:\\n\")\n",
    "      for element in elements_with_max_font:\n",
    "            new_title += ' ' + element[0]\n",
    "      print(new_title)\n",
    "else:\n",
    "      new_title = title\n",
    "      print(new_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     WRONG_VERSION_NUMBER] wrong version number\n",
      "[nltk_data]     (_ssl.c:1122)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     WRONG_VERSION_NUMBER] wrong version number\n",
      "[nltk_data]     (_ssl.c:1122)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     WRONG_VERSION_NUMBER] wrong version number\n",
      "[nltk_data]     (_ssl.c:1122)>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process the extracted text\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "  sentences = sent_tokenize(text)  # Split into sentences\n",
    "  tokens = [word_tokenize(sentence.lower()) for sentence in sentences]  # Tokenize each sentence\n",
    "  preprocessed_tokens = []\n",
    "  for sentence_tokens in tokens:\n",
    "    filtered_tokens = [token for token in sentence_tokens if token not in stop_words]  # Remove stop words\n",
    "    preprocessed_tokens.extend(filtered_tokens)\n",
    "  return preprocessed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "clean_title = preprocess_text(new_title)\n",
    "print(clean_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text without punctuations: \n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuations(text):\n",
    "\n",
    "  punctuations = string.punctuation + \"/_*\"\n",
    "\n",
    "  no_punct_text = \" \".join([c for c in text if c not in punctuations])\n",
    "\n",
    "  return no_punct_text\n",
    "\n",
    "clean_title = remove_punctuations(clean_title)\n",
    "print(f\"Text without punctuations: {clean_title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction of text from other sections of pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting section headers using fontsize and fontname attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.9552001953125 NimbusRomNo9L-Medi\n"
     ]
    }
   ],
   "source": [
    "# Finding the fontsize and fontname of 'Abstract'\n",
    "abstract_index = next(i for i, item in enumerate(output) if item[0] == \"Abstract \" or item[0] == 'ABSTRACT ' or item[0] == \"Abstract\" or item[0] == 'ABSTRACT')\n",
    "\n",
    "font_size = output[abstract_index][1]\n",
    "font_name = output[abstract_index][2]\n",
    "print(font_size, font_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Manuel Mager', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " (' ', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Rajat Bhatnagar', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Graham Neubig', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Ngoc Thang Vu', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " (' ', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Katharina Kann', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Abstract', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('1', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Introduction', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('2', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Background and Definitions', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('3', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Challenges and open questions', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('4', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Available MT datasets for ILA', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('5', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Low-resource MT', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('6', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Low-resource MT paradigms', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('7', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Advances in MT for the indigenous', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('languages of the Americas', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('8', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Ethical aspects', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('9', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Conclusion', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Limitations', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Ethical statement', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('References', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('A', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Appendix', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('B', 11.9552001953125, 'NimbusRomNo9L-Medi')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_items = [item for item in output if item[1] == font_size and item[2] == font_name]\n",
    "filtered_items = filtered_items[:-1] # Excluding references from headers\n",
    "filtered_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Abstract', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('1', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Introduction', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('2', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Background and Definitions', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('3', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Challenges and open questions', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('4', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Available MT datasets for ILA', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('5', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Low-resource MT', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('6', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Low-resource MT paradigms', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('7', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Advances in MT for the indigenous', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('languages of the Americas', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('8', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Ethical aspects', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('9', 11.9552001953125, 'NimbusRomNo9L-Medi'),\n",
       " ('Conclusion', 11.9552001953125, 'NimbusRomNo9L-Medi')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing unwanted parts of section headers\n",
    "new_abstract_index = next((i for i, item in enumerate(filtered_items) if item[0] == 'Abstract 'or item[0] == 'ABSTRACT' \n",
    "                           or item[0] == 'Abstract' or item[0] == 'ABSTRACT '), None)\n",
    "\n",
    "new_items = filtered_items[new_abstract_index:] if new_abstract_index is not None else filtered_items\n",
    "\n",
    "new_con_index = next((i for i, element in enumerate(new_items) if element[0] == 'CONCLUSIONS' or element[0] == 'CONCLUSIONS ' \n",
    "                      or element[0] == 'Conclusions' or element[0] == 'Conclusions ' or element[0] == 'Conclusion'\n",
    "                      or element[0] == 'Conclusion '), None)\n",
    "\n",
    "new_items = new_items[:new_con_index + 1]\n",
    "\n",
    "new_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the entire text from pdf\n",
    "\n",
    "def extract_text_from_pdf(pdf_file_path):\n",
    "    \"\"\"Extracts text from a PDF file and returns it as a string.\"\"\"\n",
    "\n",
    "    with open(pdf_file_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "\n",
    "        full_text = \"\"\n",
    "        for page_num in range(num_pages):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            page_text = page.extract_text()\n",
    "            full_text += page_text\n",
    "\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Machine Translation for the Indigenous Languages of the\n",
      "Americas: An Introduction.\n",
      "Manuel Mager♡∗Rajat Bhatnagar♠Graham Neubig♯\n",
      "Ngoc Thang Vu♢Katharina Kann♠\n",
      "♡AWS AI Labs♯Carnegie Mellon University\n",
      "♠University of Colorado Boulder♢University of Stuttgart\n",
      "Abstract\n",
      "Neural models have drastically advanced state\n",
      "of the art for machine translation (MT) be-\n",
      "tween high-resource languages. Traditionally,\n",
      "these models rely on large amounts of train-\n",
      "ing data, but many language pairs lack these\n",
      "resources. However, an important part of the\n",
      "languages in the world do not have this amount\n",
      "of data. Most languages from the Americas are\n",
      "among them, having a limited amount of par-\n",
      "allel and monolingual data, if any. Here, we\n",
      "present an introduction to the interested reader\n",
      "to the basic challenges, concepts, and tech-\n",
      "niques that involve the creation of MT systems\n",
      "for these languages. Finally, we discuss the\n",
      "recent advances and findings and open ques-\n",
      "tions, product of an increased interest of the\n",
      "NLP community in these languages.\n",
      "1 Introduction\n",
      "More than 7 billion people on Earth communicate\n",
      "in nearly 7000 different languages (Pereltsvaig,\n",
      "2020). Of these, approximately 900 languages\n",
      "are native of the American continent (Campbell,\n",
      "2000). Most of these indigenous languages of the\n",
      "Americas (ILA) are endangered at some degree\n",
      "(Thomason, 2015). This huge variety in languages\n",
      "is simultaneously a rich treasure for humanity and\n",
      "also a barrier to communication among people\n",
      "from different backgrounds. Human translators\n",
      "have been important in overcoming language bar-\n",
      "riers. However, trained translators are not acces-\n",
      "sible to everyone on Earth and even scarcer for\n",
      "endangered and minority languages. The need\n",
      "for translations is even written in the constitutions\n",
      "of several countries like Mexico, Peru, Paraguay,\n",
      "Venezuela, and Bolivia (Zajícová, 2017) to allow\n",
      "native speakers to have equal language rights re-\n",
      "garding law.\n",
      "This is why developing MT is crucial: it helps\n",
      "humanity overcome language barriers while si-\n",
      "multaneously allowing people to continue using\n",
      "∗Work done while at the University of Stuttgart.their native tongue. However, the challenges to\n",
      "achieving these problems are not trivial. It is not\n",
      "only the amount of available data (a common the-\n",
      "sis among the NLP community) but also a set\n",
      "of challenging issues (dialectical and orthographic\n",
      "variations, noisy texts, complex morphology, etc.)\n",
      "that must be addressed.\n",
      "MT has always been an important task within\n",
      "the larger area of natural language processing\n",
      "(NLP). In 1954, the Georgetown–IBM experiment\n",
      "(Hutchins, 2004) was the first that showed at least\n",
      "some effectiveness of MT. Further research re-\n",
      "sulted in rule-based systems and statistical models.\n",
      "In 2023, neural models define state of the art for\n",
      "MT if training data is plentiful – i.e., for so-called\n",
      "high-resource languages (HRLs) – and have also\n",
      "achieved impressive results for low-resource lan-\n",
      "guages (LRLs). MT is also the most studied NLP\n",
      "task for the ILA (Mager et al., 2018b; Littell et al.,\n",
      "2018). The common issue among these languages\n",
      "is the extreme low-resource conditions they are\n",
      "confronted with. The research interest for these\n",
      "languages has increased in the last years, including\n",
      "the recent AmericasNLP 2021 shared task (Mager\n",
      "et al., 2021) on 10 indigenous languages to Span-\n",
      "ish, and the WMT (Conference on Machine Trans-\n",
      "lation) shared task for Inuktitut–English (Barrault\n",
      "et al., 2020).\n",
      "In this work we aim to provide a comprehensive\n",
      "introduction to the challenges that involve creat-\n",
      "ing MT systems for ILA, and the current status\n",
      "of the existing work. We organize this work as\n",
      "follows: We start by introducing state-of-the-art\n",
      "NMT models (§2). Then, we discuss the current\n",
      "challenges for these languages (§3); and we in-\n",
      "troduce the key concepts related to low-resource\n",
      "NMT and the implications for endangered lan-\n",
      "guages of the Americas(§3). This is followed by\n",
      "a discussion of available data (§4). Afterwards,\n",
      "we introduce the important concepts for LRL and\n",
      "endangered languages (§5); then we introduce thearXiv:2306.06804v1  [cs.CL]  11 Jun 2023main strategies aimed at improving NMT with\n",
      "limited training data (§6); and finally we give an\n",
      "overview of the work done for ILA on MT (§7).\n",
      "In doing so, we provide insights into the follow-\n",
      "ing questions: Which systems define the state of\n",
      "the art on low-resource NMT applied to the ILA?\n",
      "What is the route that ahead to improve the trans-\n",
      "lations of the ILA?\n",
      "2 Background and Definitions\n",
      "Formally, the task of MT consists of converting\n",
      "textXin a source language Lxinto text Yin a\n",
      "target language Lythat conveys the same mean-\n",
      "ing.1Translating text X∈LxintoY∈Lycan be\n",
      "described as a function (Neubig, 2017):\n",
      "Y=MT(X). (1)\n",
      "XandYcan be of variable length, such as\n",
      "phrases, sentences, or even documents.\n",
      "If other languages are used during the transla-\n",
      "tion process, e.g., as pivots, we denote them as\n",
      "L1, . . . , L n. We refer to a corpus of monolingual\n",
      "sentences in language LiasMLi=S1, ..., S n.\n",
      "Probabilistic Modeling and Data When us-\n",
      "ing probabilistic MT models, the goal is to find\n",
      "Y∈Lywith the highest conditional probability,\n",
      "given X∈Lx. Under the supervised machine\n",
      "learning paradigm, a parallel corpus Cparallel =\n",
      "(X1, Y1), ...,(Xn, Yn)is used to learn a set of pa-\n",
      "rameters θ, which define a probability distribution\n",
      "over possible translations. Given Cparallel , the\n",
      "training objective of an NMT model is generally\n",
      "to maximize the log-likelihood Lwith respect to\n",
      "θ:\n",
      "Lθ=X\n",
      "(Xi,Yi)∈Cparallellogp(Yi|Xi;θ).(2)\n",
      "Within this overall framework, there are a num-\n",
      "ber of design decisions one has to make, such as\n",
      "which model architecture to use, how to generate\n",
      "translations, and how to evaluate.\n",
      "Decoding Decoding refers to the generation of\n",
      "output ˆY, given the parameters θand an input X.\n",
      "Often, decoding is done by approximately solving\n",
      "the following maximization problem:\n",
      "argmax ˆYp(ˆY|X;θ) (3)\n",
      "1This is an approximation, since it is in general not possi-\n",
      "ble to map the meaning of text exactly into another language\n",
      "(Nida, 1945; Sechrest et al., 1972; Baker, 2018).Most NMT systems factorize the probability of\n",
      "ˆY= ˆy1, ...,ˆyTin a left-to-right fashion:\n",
      "p(ˆY) =TY\n",
      "t=1p(ˆyt|ˆy<t, X, θ ) (4)\n",
      "Thus, the probability of token ˆytat time step tis\n",
      "computed using the previously generated tokens\n",
      "ˆy<t, the source sentence Xand the model param-\n",
      "eters θ. Common algorithms for finding a high-\n",
      "probability translation are greedy decoding, i.e.,\n",
      "picking the token with the highest probability at\n",
      "each time step, and beam search (Lowerre, 1976).\n",
      "2.1 Input Representations\n",
      "The texts XandYare input into an NMT sys-\n",
      "tem as sequences of continuous vectors. How-\n",
      "ever, defining which units should be represented\n",
      "as such vectors is non-trivial. The classic way\n",
      "is to represent each word within XandYas\n",
      "a vector (or embedding). However, in a low-\n",
      "resource setting, often not all vocabulary items ap-\n",
      "pear in the training data (Jean et al., 2015; Lu-\n",
      "ong et al., 2015). This issue especially effects lan-\n",
      "guages with a rich inflectional morphology (Sen-\n",
      "nrich et al., 2016c): as many word forms can\n",
      "represent the same lemma, the vocabulary cover-\n",
      "age decreases drastically. Furthermore, for many\n",
      "LRLs, boundaries between words or morphemes\n",
      "are not easy to obtain or not well defined in the\n",
      "case of languages without a standard orthography.\n",
      "Alternative input units have been explored, such as\n",
      "characters (Ling et al., 2015), byte pair encoding\n",
      "(BPE; Sennrich et al., 2016a), morphological rep-\n",
      "resentations (Vania and Lopez, 2017; Ataman and\n",
      "Federico, 2018), syllables (Zhang et al., 2019), or,\n",
      "recently, a visual representation of rendered text\n",
      "(Salesky et al., 2021). No clear advantage has been\n",
      "discovered for using morphological segmentations\n",
      "over BPEs when testing them on LRLs (Saleva and\n",
      "Lignos, 2021).\n",
      "Input representations can be pretrained. The\n",
      "two most common options are: i) word em-\n",
      "beddings, where each type is represented by a\n",
      "vector, e.g., Word2Vec (Mikolov et al., 2013),\n",
      "Glove (Pennington et al., 2014), or Fasttext (Bo-\n",
      "janowski et al., 2017)) embeddings, and ii) contex-\n",
      "tualized word representations, where entire sen-\n",
      "tences are being encoded at a time, e.g., ELMo\n",
      "(Peters et al., 2018) or BERT (Devlin et al.,\n",
      "2019). However, training of these methods re-\n",
      "quires large monolingual training corpora, whichmay not be readily available for LRLs. As most\n",
      "ILA have rich morphology, this topic has gath-\n",
      "ered special interest. The discussion about the us-\n",
      "age of morpholigical segmented input for NMT\n",
      "models is recurrent. (Mager et al., 2022) show\n",
      "that the unsupervised morphologically inspired\n",
      "models outperform BPE pre-processing (experi-\n",
      "mented on 4 language pares). Similar experi-\n",
      "ments done on Quechua–Spanish and Inuktitut–\n",
      "Enlgish (Schwartz et al., 2020), comparing BPEs\n",
      "against Morfessor (Smit et al., 2014). Also (Or-\n",
      "tega et al., 2020a) improves the SOTA (state-of-\n",
      "the-art) for Quechua–Spanish MT using a mor-\n",
      "phological guided BPE algorithm.\n",
      "2.2 Architectures\n",
      "NMT models typically are sequence-to-sequence\n",
      "models. They encode a variable-length sequence\n",
      "into a vector or matrix representation, which they\n",
      "then decode back into a variable-length sequence\n",
      "(Cho et al., 2014). The two most frequent archi-\n",
      "tectures are: i) recurrent neural networks (RNN),\n",
      "such as LSTMs (Hochreiter and Schmidhuber,\n",
      "1997) or GRUs (Cho et al., 2014), and ii) trans-\n",
      "formers (Vaswani et al., 2017), which define the\n",
      "current state of the art in the high-resource setting.\n",
      "As for most neural network models, training an\n",
      "NMT system on a limited number of instances\n",
      "is challenging (Fernández-Delgado et al., 2014).\n",
      "There are common problems that arise from lim-\n",
      "ited data in the training set. One major advan-\n",
      "tage of neural models is their ability to learn rep-\n",
      "resentations from raw data, in contrast to manu-\n",
      "ally engineered features (Barron, 1993). However,\n",
      "problems arise when not enough data is provided\n",
      "to enable effective learning of features. Another\n",
      "strength of neural networks is their generalization\n",
      "capacity (Kawaguchi et al., 2017). However, train-\n",
      "ing a neural network on a small dataset easily leads\n",
      "to overfitting (Rolnick et al., 2017). Recent stud-\n",
      "ies, however, show empirically that this does not\n",
      "necessarily happen if the network is tuned cor-\n",
      "rectly (Olson et al., 2018).\n",
      "2.3 Evaluation\n",
      "Accurately judging translation quality is difficult\n",
      "and, thus, often still done manually: bilingual\n",
      "speakers assign scores according to provided crite-\n",
      "ria such as fluency and adequacy ( Does the output\n",
      "have the same meaning as the input? ). However,\n",
      "manual evaluation is expensive and slow. More-over, in the case of endangered languages, bilin-\n",
      "gual speakers can be hard or impossible to find.\n",
      "Automatic metrics provide an alternative.2\n",
      "These metrics assign a score to system output,\n",
      "given one or more ground truth reference transla-\n",
      "tions. The most widely used metric is BLEU (Pa-\n",
      "pineni et al., 2002), which relies on token-level n-\n",
      "gram matches between the translation to be rated\n",
      "and one or more gold-standard translations. For\n",
      "morphologically rich languages, character-level\n",
      "metrics, such as chrF (Popovi ´c, 2017), are often\n",
      "more suitable, as they allow for more flexibility.\n",
      "In the AmericasNLP ST (Mager et al., 2021) this\n",
      "metric was used over BLEU, as it fits better to the\n",
      "rich morphology of many ILA.\n",
      "To have a concrete example, lets have the fol-\n",
      "lowing Wixarika phrase with an English transla-\n",
      "tion:\n",
      "yu-huta-me ne-p+-we-’iwa\n",
      "an-two-ns 1sg:s-asi-2pl:o-brother\n",
      "I have two brothers\n",
      "As discussed in (Mager et al., 2018c) it is dif-\n",
      "ficult to translate back from Spanish (or other Fu-\n",
      "sional language) the morpheme p+as it has not\n",
      "equivalent in these languages. So if we would ig-\n",
      "nore these morpheme at all, BLEU would penal-\n",
      "ize the entire word nep+we’iwa . In contrast, chrF\n",
      "would give credit to the translation, even if the p+\n",
      "is missing.\n",
      "One shortcoming of these evaluation metrics is\n",
      "that the evaluation is very dependent on the sur-\n",
      "face forms and not on the ultimate goal of seman-\n",
      "tic similarity and fluency. Recent work uses pre-\n",
      "trained models to evaluate semantic similarity be-\n",
      "tween translations and the gold standard (Zhang\n",
      "et al., 2020d), but these methods are limited to lan-\n",
      "guages for which such models are available. This\n",
      "is not possible for the ILA, as the amount of mono-\n",
      "lingual data is not enough to train a reliable pre-\n",
      "trained language model3.\n",
      "3 Challenges and open questions\n",
      "In an overview of the datasets and recent studies\n",
      "of MT for the ILA, we found the following main\n",
      "issues to be handled.\n",
      "2For a detailed overview of automatic metrics for MT we\n",
      "refer the interested reader to specialized reviews (Han, 2016;\n",
      "Celikyilmaz et al., 2020; Chatzikoumi, 2020).\n",
      "3One exception to this is Quechua, that has a large enough\n",
      "monolingual dataset to train a BERT like model (Zevallos\n",
      "et al., 2022)Extreme low-resource parallel datasets Even\n",
      "with the recent advances, the resources available\n",
      "to train MT systems are extremely scarce, hav-\n",
      "ing training set between 4k and 20k sentences (see\n",
      "§4), with notable exceptions for Inuktitut, Guarani\n",
      "and Quechua (Joanis et al., 2020; Ortega et al.,\n",
      "2020a).\n",
      "Lack of monolingual data Most of these lan-\n",
      "guages are mostly used in spoken form. In re-\n",
      "cent years, with the advancement and democra-\n",
      "tization of mobile technologies, indigenous lan-\n",
      "guages have seen a slight increase in massaging\n",
      "systems and private spheres (Rosales et al.). How-\n",
      "ever, the usage of these languages on the internet\n",
      "is rather limited. Even Wikipedia has a limited\n",
      "amount of these languages (Mager et al., 2018b).\n",
      "Low domain diversity . As most parallel\n",
      "datasets are scarce, they are restricted to a small\n",
      "number of domains, making it challenging to\n",
      "adapt it, or try to aim for general translation mod-\n",
      "els. This has been recognized as a major problem\n",
      "during the AmericasNLP ST (Mager et al., 2021).\n",
      "Rich morphology An important number of\n",
      "these languages are morphological highly rich. In\n",
      "many cases, we find polysynthetic, with or highly\n",
      "agglutinative languages (Kann et al., 2018) or even\n",
      "fusional phenomenon (Mager et al., 2020).\n",
      "Distant paired language The most common\n",
      "languages that we find that ILA is translated into\n",
      "are Spanish, English, and Portuguese. However,\n",
      "these languages are distantly related to the ILA,\n",
      "and have completely different linguistically phe-\n",
      "nomenons (Campbell, 2000; Romero et al., 2016).\n",
      "Noisy text environments Monolingual texts, if\n",
      "exist, are found in social media that often use a\n",
      "non-canonical witting (Rosales et al.).\n",
      "Code-Swithing This phenomenon is strongly\n",
      "present in ILA, as all of these languages are mi-\n",
      "nority languages in their own countries. The\n",
      "bilingualism among their communities is strong\n",
      "(and CS is a common phenomenon in this setup\n",
      "(Çetino ˘glu, 2017)). The final result of this phe-\n",
      "nomenon is the inclusion of code-switching on a\n",
      "common base (Mager et al., 2019) in their lan-\n",
      "guage.\n",
      "Lack of orthographic normalization The us-\n",
      "age of ILA faces the problem of having a unifiedorthographic standard. This is not always possi-\n",
      "ble, as the suggestions of linguists and official en-\n",
      "tities do not always match the day-by-day writ-\n",
      "ing of the speakers. Moreover, in some cases,\n",
      "special symbols present in the orthographic stan-\n",
      "dards are not accessible in English or Spanish key-\n",
      "board and need to be replaced with other symbols.\n",
      "The winner of the AmericasNLP ST got important\n",
      "improvements using orthographic normalizers de-\n",
      "veloped specifically for each American language\n",
      "(Vázquez et al., 2021).\n",
      "Dialectal variety The indigenous languages\n",
      "have a strong dialectal variety, making it hard for\n",
      "native speakers to understand even speakers from\n",
      "neighboring villages. The linguistic richness of\n",
      "entire regions is so diverse that even a single state\n",
      "like the Mexican Oaxaca could correspond to the\n",
      "diversity in the whole Europe (McQuown, 1955).\n",
      "4 Available MT datasets for ILA\n",
      "The parallel datasets available for MT have been\n",
      "increasing during the last years. At this moment,\n",
      "we can show in two folds the development of these\n",
      "resources: as shown in table 2 work on specific\n",
      "language has emerged; but also broader datasets\n",
      "have started to cover the ILA (see table 1).\n",
      "Language-specific corpus collection work has\n",
      "been done for many languages, where parallel\n",
      "corpus has been the main component. In re-\n",
      "cent time we have seen Cherokee–English (OPUS)\n",
      "(Zhang et al., 2020c), Wixarika–Spanish (Mager\n",
      "et al., 2018a), Shipio–Konibo (Feldman and Coto-\n",
      "Solano, 2020), and others (see table 2). The most\n",
      "prominent of these datasets has been the Inuktitut–\n",
      "English parallel data. The last version of this\n",
      "dataset corpora (Joanis et al., 2020) is has medium\n",
      "size with 1,450,094 sentences. Previous versions\n",
      "of this corpus are (Martin et al., 2003). This data\n",
      "set was used for the WMT 2020 Shared Task on\n",
      "Unsupervised, and Low Resourced MT (Barrault\n",
      "et al., 2020).\n",
      "For wide-spoken languages like Guarani, it is\n",
      "even possible to collect a web crawled dataset,\n",
      "including news articles and social media parallel\n",
      "aligned data (Chiruzzo et al., 2020; Góngora et al.,\n",
      "2021) This dataset also includes monolingual data.\n",
      "This is possible as Guaraní is one of the most spo-\n",
      "ken indigenous languages of the continent.\n",
      "In contrast to the language-specific datasets,\n",
      "we find broader approaches (see table 1). The\n",
      "broadest multilingual dataset, which contains theDataset Paired-languages Authors\n",
      "AmericasNLI Aymara, Asháninka, Bribri, Guaraní,\n",
      "Nahuatl, Otomí, Quechua, Rarámuri,\n",
      "Shipibo-Konibo, Wixarika(Ebrahimi et al., 2022)\n",
      "CPML Ch’ol, Maya, Mazatec, Mixtec, Nahu-\n",
      "atl and Otomi(Sierra Martínez et al., 2020)\n",
      "OPUS * (Tiedemann, 2016)\n",
      "New testament Bible * (McCarthy et al., 2020)\n",
      "Table 1: Parallel dataset collections that contain one or more indigenous languages of the Americas\n",
      "Language Paried-language ISO Family Sentences Domain Authors\n",
      "Asháninka Spanish cni Arawak 3883 (Ortega et al., 2020b)\n",
      "Bribri Spanish bzd Chibchan 5923 (Feldman and Coto-\n",
      "Solano, 2020)\n",
      "Guarani Spanish gn Tupi-Guarani News,\n",
      "Blogs(Abdelali et al., 2006)\n",
      "Guarani Spanish gn Tupi-Guarani 14,531 News,\n",
      "Blogs(Chiruzzo et al., 2020)\n",
      "Guarani Spanish gn Tupi-Guarani 14,792 News, So-\n",
      "cial Media(Góngora et al., 2021)\n",
      "Guarani Spanish gn Tupi-Guarani 30855 8 Domains (Chiruzzo et al., 2022)\n",
      "Nahuatl Spanish nah Uto-Aztecan 16145 Diverse\n",
      "Books(Gutierrez-Vasques\n",
      "et al., 2016)\n",
      "Otomí Spanish oto Oto-Manguean 4889 Diverse\n",
      "Bookshttps://\n",
      "tsunkua.elotl.\n",
      "mx\n",
      "Rarámuri Spanish tar Uto-Aztecan 14721 Dictionary\n",
      "Examples(Mager et al., 2022)\n",
      "Shipibo-Konibo Spanish shp Panoan 14592 Educational,\n",
      "Religious(Galarreta et al., 2017)\n",
      "Wixarika Spanish hch Uto-Aztecan 8966 Literature (Mager et al., 2018a)\n",
      "Cherokee English chr Uto-Aztecan OPUS (Zhang et al., 2020c)\n",
      "Inuktitut English iku Eskimo–Aleut 1,450,094 Legislative (Joanis et al., 2020)\n",
      "Ayuuk Spanish mir Mixe–Zoque 7553 Diverse (Zacarías Márquez and\n",
      "Meza Ruiz, 2021)\n",
      "Mazatec Spanish Many Oto-Manguean 9799 Diverse (Tonja et al., 2023)\n",
      "Mixtec Spanish Many Oto-Manguean 13235 Diverse (Tonja et al., 2023)\n",
      "Table 2: Parallel datasets that have been released focusing on one indigenous language\n",
      "Bible’s New Testament, includes about 1600 lan-\n",
      "guages (Mayer and Cysouw, 2014; McCarthy\n",
      "et al., 2020) of the 2,508 that have been collected\n",
      "by the Summer Institute of Linguistic (SIL) (An-\n",
      "derson and Anderson, 2012). Another remarkable\n",
      "effort to obtain broad language coverage is the\n",
      "PanLex project (Kamholz et al., 2014), which has\n",
      "gathered lexical translation dictionaries for over\n",
      "5,700 languages. However, for most languages,\n",
      "PanLex contains only a few dozen words. Duan\n",
      "et al. (2020) show that such dictionaries can be\n",
      "used to create an NMT system, making bilingual\n",
      "dictionaries relevant for further studies.\n",
      "Recently community-driven research groups\n",
      "have started the creation of own parallel datasets,\n",
      "such as Masakhane (Orife et al., 2020; Nekoto\n",
      "et al., 2020) for African languages, and Americ-\n",
      "asNLP for indigenous languages of the Americas(Ebrahimi et al., 2021; Mager et al., 2021). The\n",
      "AmericasNLI dataset is an important effort to have\n",
      "a common evaluation benchmark for the 10 in-\n",
      "digenous languages of the Americas for the MT\n",
      "and NLI tasks.\n",
      "Given the constitutional rights of indigenous\n",
      "languages in many countries of the Americas, it is\n",
      "possible to access this data. Vázquez et al. (2021)\n",
      "made available this resource during their shared\n",
      "task system development.\n",
      "Finally, it is important to mention that many\n",
      "of the languages spoken in the Americas have\n",
      "Wikipedia’s set of articles available4.\n",
      "4The available languages in wikipedia can be consulted\n",
      "at:https://es.wikipedia.org/wiki/Portal:\n",
      "Lenguas_indÃ genas_de_AmÃl’rica . Until the\n",
      "publication of this article, there were only entries in Nahu-\n",
      "atl, Navajo, Guarani, Aymara, Klaalisut, Esquimal, Inukitut,\n",
      "Cherokee, and Cree.Collection of New Data A common way to cre-\n",
      "ate parallel data with the help of bilingual speakers\n",
      "is via elicitation (translating the foreign text into\n",
      "another language). It has the disadvantage of bias-\n",
      "ing the created text to forms and topics, culture,\n",
      "and even grammatical forms towards the source\n",
      "language (Lörscher, 2005). A method that avoids\n",
      "this problem is language documentation, which\n",
      "consists of storing and annotating commonly used\n",
      "speech or text (Himmelmann, 2008). However, it\n",
      "is costly and requires specialists. In this process,\n",
      "involving the community members that are bilin-\n",
      "gual speakers is important (Bird, 2020).\n",
      "5 Low-resource MT\n",
      "For the purpose of this paper we define LRLs\n",
      "as languages for which standard techniques are\n",
      "unable to create well performing systems, which\n",
      "makes it necessary to resort to other techniques\n",
      "(cf. Figure 1) such as transfer learning. For MT,\n",
      "the amount of available resources differs widely\n",
      "across language pairs: some have less than 10k\n",
      "parallel sentences, while other have more than\n",
      "500k, with some exceptions in the orders of sev-\n",
      "eral million.\n",
      "Emulating a low-resource scenario by down-\n",
      "sampling available data for high-resource lan-\n",
      "guages is common and helps understanding a\n",
      "model’s performance across different settings.\n",
      "However, further evaluating methods on a diverse\n",
      "set of low-resource languages is crucial, since\n",
      "many languages exhibit particular linguistic phe-\n",
      "nomena (Mager et al., 2020), that perturb the fi-\n",
      "nal results, especially since most large datasets\n",
      "are from the Indo-European language family, to\n",
      "which only 6.16% of the world’s languages belong\n",
      "(Lewis, 2009).\n",
      "Importantly, there is no strong correlation be-\n",
      "tween the number of resources available per lan-\n",
      "guage and the number of speakers: Javanese with\n",
      "95 million speakers and Kannada with 44 million\n",
      "are considered LRLs, while French, with only 64\n",
      "million native speakers, is among the most widely\n",
      "studied languages. Improving models to handle\n",
      "LRLs will extend access to information online as\n",
      "well as human language technology to all mono-\n",
      "lingual speakers of those languages. In the case\n",
      "of ILA, most languages are endangered at some\n",
      "degree, but most of them have the same issue:\n",
      "they are low resourced for parallel and monolin-\n",
      "gual data.Endangered Languages Krauss (1992) esti-\n",
      "mates that 50% of all languages are doomed or\n",
      "dying, and that in this century we will see either\n",
      "the death or the doom of 90% of all human lan-\n",
      "guages. The current proportion of languages that\n",
      "are already extinct or moribund ranges from 31%\n",
      "down to 8% depending on the region, with the\n",
      "most severe cases in the Americas and Australia\n",
      "(Simons and Lewis, 2013). To determine how en-\n",
      "dangered a language is, Lewis and Simons (2010)\n",
      "proposes a classification scale called EGIDS with\n",
      "13 levels. The higher the number on this scale,\n",
      "the greater the level of disruption of the language’s\n",
      "inter-generational transmission.5MT for endan-\n",
      "gered LRLs has the potential to help with doc-\n",
      "umentation, promotion and revitalization efforts\n",
      "(Galla, 2016; Mager et al., 2018b). However, as\n",
      "these languages are commonly spoken by small\n",
      "communities, or indigenous people, researchers\n",
      "should aim for a direct involvement of those com-\n",
      "munities (Bird, 2020).\n",
      "What is polysynthesis? A polysynthetic lan-\n",
      "guage is defined by the following linguistic fea-\n",
      "tures: the verb in a polysynthetic language must\n",
      "have an agreement with the subject, objects and\n",
      "indirect objects (Baker, 1996); nouns can be in-\n",
      "corporated into the complex verb morphology\n",
      "(Mithun, 1986); and, therefore, polysynthetic lan-\n",
      "guages have agreement morphemes, pronominal\n",
      "affixes and incorporated roots in the verb (Baker,\n",
      "1996), and also encode their relations and charac-\n",
      "terizations into that verb. The most common word\n",
      "orders present in these languages are SOV , VSO,\n",
      "SVO and free order. It is important to notice that\n",
      "a polysynthtic language can have a aggutinative6\n",
      "or can have also fusional characteristics, like To-\n",
      "tonaco or Tepehua (Mager et al., 2020).\n",
      "6 Low-resource MT paradigms\n",
      "Most languages of the Americas do not have high\n",
      "amount of data for MT. Therefore, we introduce\n",
      "the most important paradigms to improve low-\n",
      "resourced machine translation. Figure 1 shows a\n",
      "general overview of the methods and options to\n",
      "improve LRL MT. For a more detailed understand-\n",
      "ing of this techniques we refer the reader to spe-\n",
      "cialized low-resource MT surveys (Haddow et al.,\n",
      "5The complete EGIDS scale can be found at https://\n",
      "www.ethnologue.com/about/language-status\n",
      "6Agglutination refers to a concatenation of morphemes,\n",
      "with minimal changes to the surface form.DAT A\n",
      "No parallel data\n",
      " Low amount of\n",
      "parallel dataCollect new data\n",
      "Parallel data\n",
      "to 3th language\n",
      "Only monolingual\n",
      "data\n",
      "Additional\n",
      "Annotated data\n",
      "*Monolingual*Parallel data in\n",
      "other languages*Pre-trained\n",
      "models\n",
      "*  Any data\n",
      "(Monolingual,\n",
      "Multilingual, etc)*Annotators\n",
      "Noisy Parallel\n",
      "Data\n",
      "Speechrecordings\n",
      "Zero-shot\n",
      "Multi-task Data\n",
      "Augmentation\n",
      "Back\n",
      "T ranslationSentence\n",
      "ModificationMultilingual\n",
      "T ransfer L earningElicitation\n",
      "Annotating\n",
      "commonly\n",
      "used speech\n",
      "or text\n",
      "Noisy parallel\n",
      "documents\n",
      "to parallel\n",
      "sentencesPivotingUnsupervised\n",
      "MT\n",
      "Figure 1: What to do when we have low o no data to train our machine translation models? This diagram shows\n",
      "basic scenarios, solutions, and common requirements for each method, with the section describing the method.\n",
      "L1L2\n",
      "Ln...\n",
      "L1L2\n",
      "Ln...\n",
      "L1\n",
      "Ln...L1\n",
      "Ln...θ\n",
      "θ\n",
      "θ1) One to many\n",
      "2) Many to one\n",
      "3) Many to many\n",
      "Figure 2: An overview of different multilingual setups.\n",
      "2022; Wang et al., 2021; Ranathunga et al., 2021).\n",
      "6.1 Multilingual Supervised Training\n",
      "With a multilingual set of parallel data\n",
      "Dparallel between different language pairs\n",
      "{(L1, L2), . . . , (Lm, Ln)}we can train a model\n",
      "that is able to map a sentence from any source\n",
      "language Lxinto any target language Lythat is\n",
      "contained in Dparallel (see 2). These multilingual\n",
      "NMT models have seen a growth in popularity\n",
      "and efficiency in recent years. We will now\n",
      "cover the different training algorithms for these\n",
      "models: 1) many source languages and one target\n",
      "language ( many-to-one ), 2) one source and many\n",
      "target languages ( one-to-many ), and 3) many\n",
      "source languages and many target languages\n",
      "(many-to-many ). For a general overview of\n",
      "multilingual MT, we refer the reader to surveys\n",
      "dedicated to this topic (Tan et al., 2019; Dabre\n",
      "et al., 2019). Johnson et al. (2017) are the first\n",
      "to introduce a multilingual NMT model, trained\n",
      "on translating from a large number of languages\n",
      "to English as well as in the opposite direction.\n",
      "The authors show that these models improve over\n",
      "DL1D'L2L1 ⇾ L2\n",
      "D'L2DL1L2 ⇾ L1Decode\n",
      "TrainFigure 3: Backtranslation\n",
      "single-language pair models for LRLs.\n",
      "6.2 Multi-task Training\n",
      "Multi-task training (Caruana, 1997) aims to im-\n",
      "prove the performance of the main task – MT in\n",
      "our case – by adding one or more auxiliary tasks\n",
      "to the training. The easiest way is to share all pa-\n",
      "rameters of the network, using the ideas already\n",
      "explored in multilingual NMT (§6.1). This can be\n",
      "done with a special flag in the input that specifies\n",
      "the current task. It is also possible to share only the\n",
      "encoder and have two separate decoders for each\n",
      "task.\n",
      "Multilingual Modeling In order to handle mul-\n",
      "tilinguality it is also possible to adapt modify the\n",
      "NMT models. The main proposals to do so has\n",
      "been: sharing all parameter except the attention\n",
      "mechanism of a RNN NMT model (Blackwood\n",
      "et al., 2018); parameter sharing in the transformer\n",
      "architecture Sachan and Neubig (2018);\n",
      "6.3 Data Augmentation\n",
      "Back-Translation A straightforward way to\n",
      "leverage monolingual data for low-resource MT is\n",
      "to generate a meaningful signal with the help of\n",
      "an already initialized MT model (see Figure 3).This method is called back-translation (BT; Sen-\n",
      "nrich et al., 2016b): With monolingual data MLx\n",
      "in source language Lxand a trained model that is\n",
      "able to translate from Lxinto a target language Ly\n",
      "we can generate a translation M′Ly. This pseudo\n",
      "parallel data (MLx, M′Ly)is then used to train a\n",
      "new model in the opposite direction. This process\n",
      "can be applied iteratively to improve the transla-\n",
      "tion (Hoang et al., 2018).\n",
      "Sentence Modification Other methods to gen-\n",
      "erate more parallel sentences are based on lexi-\n",
      "cal substitution. Fadaee et al. (2017) explores re-\n",
      "placing frequent words with low-frequency ones\n",
      "in both source and target to improve the transla-\n",
      "tion of rare words. This is done using language\n",
      "models (LMs) and automatic alignment.\n",
      "Pivoting If no parallel corpus between lan-\n",
      "guages LxandLyis available, but both of them\n",
      "have parallel corpora with a third language Lp,\n",
      "pivoting is an option. The basic idea is to train\n",
      "two MT systems: one that translates Lx→Lp\n",
      "and another for Lp→Ly. Pivoting has first been\n",
      "introduced for SMT (Wu and Wang, 2007; Cohn\n",
      "and Lapata, 2007; Utiyama and Isahara, 2007).\n",
      "6.4 Semi-supervised and Unsupervised MT\n",
      "Transfer Learning via Pretraining Transfer\n",
      "learning refers to using knowledge learned from\n",
      "one task to improve performance on a related task\n",
      "(Weiss et al., 2016). In recent years this approach\n",
      "has gained popularity with big multilingual mod-\n",
      "els such as Conneau and Lample (2019) that pro-\n",
      "poses training the encoder and the decoder sep-\n",
      "arately in order to get cross-language represen-\n",
      "tations (XLM). This idea has further been ex-\n",
      "tended by Song et al. (2019, MASS) to masking\n",
      "asequence of tokens from the input (multilingual\n",
      "MASS (Siddhant et al., 2020)). Another approach\n",
      "is to train the entire transformer model as a denois-\n",
      "ing autoencoder (BART; Lewis et al., 2019) ( mul-\n",
      "tilingual BART (mBART) (Liu et al., 2020)). It is\n",
      "also possible to pretrain a transformer in a multi-\n",
      "task, text-to-text fashion, where one of the tasks is\n",
      "MT (T5; Raffel et al., 2020) (multilingual version\n",
      "(Xue et al., 2021)).\n",
      "Unsupervised MT UMT covers approaches that\n",
      "donotrequire any parallel text, relying only on\n",
      "monolingual data. This differs from zero-shot\n",
      "translation, which uses parallel data for other lan-\n",
      "guage pairs. Early approaches tackled the prob-lem with an auto-encoder with adversarial train-\n",
      "ing (Lample et al., 2017) or with auto-encoders\n",
      "with a shared encoding space as well as separate\n",
      "decoders for each target language (Artetxe et al.,\n",
      "2018). The main problem for these approches is\n",
      "the need of a big monolingual dataset, that is not\n",
      "available for most ILA.\n",
      "7 Advances in MT for the indigenous\n",
      "languages of the Americas\n",
      "In recent years the interest in MT for indigenous\n",
      "languages of the Americas has increased. The\n",
      "task is not easy. The first usage of NMT systems\n",
      "has not been successful (Mager and Meza, 2021).\n",
      "However, with the use of LRL MT methods, we\n",
      "have witnessed great improvements.\n",
      "The Cherokee–English (Zhang et al., 2020c)\n",
      "language pair has been explored using a pre-\n",
      "trained BERT (Devlin et al., 2019) for the En-\n",
      "glish side. A system demonstration of this ap-\n",
      "proach is also accessible (Zhang et al., 2021). The\n",
      "back translation strategy for Bribri–Spanish NMT\n",
      "transformers has also been explored (Feldman and\n",
      "Coto-Solano, 2020) and by (Oncevay, 2021) (for\n",
      "four Peruvian languages to Spanish) with good\n",
      "results. The scarce indigenous language mono-\n",
      "lingual text can be replaced to some extent with\n",
      "Spanish text or extracted from PDFs, and other\n",
      "sources (Bustamante et al., 2020).\n",
      "One of the main challenges for the complex\n",
      "morphological languages in the area has been the\n",
      "prepossessing step. Schwartz et al. (2020) show\n",
      "that even if morphological segmentation has less\n",
      "perplexity a the language modeling time, it is\n",
      "still under-performing or equivalent against BPEs\n",
      "for MT (for Inuktitut-–English, Yupik—English\n",
      "Data, Guaraní—Spanish Data). A more compre-\n",
      "hensive (on the segmentation modeling side) was\n",
      "done by (Mager et al., 2022) exploring a wide\n",
      "array of segmentation models.The latter study\n",
      "showed that supervised morphological segmenta-\n",
      "tion under-perform unsupervised. However, unsu-\n",
      "pervised morphological segmentation like LMVR\n",
      "(Ataman et al., 2017) and FlatCat (Grönroos et al.,\n",
      "2014) perform better than BPEs. (Ngoc Le and\n",
      "Sadat, 2020) studied how better to perform word\n",
      "segmentation for the Inuktitut–English pair. They\n",
      "found that for this language pair, a morphological\n",
      "segmentation, or a combination of BPEs and mor-\n",
      "phological segmentation, works better than just\n",
      "applying vanilla BPEs. Also, training word em-beddings for Guarani–Spanish translation is an\n",
      "excellent opportunity to increase the MT perfor-\n",
      "mance of these languages (Góngora et al., 2022).\n",
      "The usage of transfer learning from multilin-\n",
      "gual systems has been tried, with limited re-\n",
      "sults (Nagoudi et al., 2021) (training an own T5\n",
      "model for indigenous languages) and (Zheng et al.,\n",
      "2021). However, pertaining a Spanish–English\n",
      "model together with ILA, and then fine-tuning it\n",
      "(together with a careful prepossessing and filter-\n",
      "ing step) has been the most successful strategy\n",
      "(Vázquez et al., 2021).\n",
      "The quality of MT systems of ILA has been a\n",
      "constant debate. However, Ebrahimi et al. (2021)\n",
      "shows that the quality of MT for these languages\n",
      "is enough to improve other tasks like natural lan-\n",
      "guage inference (NLI).\n",
      "Inuktitut–Enlgish ST The WMT 2020 news\n",
      "translation task included Inuktitut–English trans-\n",
      "lation (Barrault et al., 2020). The participating\n",
      "systems explored the difficulties of working with\n",
      "a polysynthetic language in a medium resource\n",
      "scenario. Participating teams in this competi-\n",
      "tion were: (Kocmi, 2020; Hernandez and Nguyen,\n",
      "2020; Scherrer et al., 2020; Roest et al., 2020; Lo,\n",
      "2020; Knowles et al., 2020; Zhang et al., 2020e;\n",
      "Krubi ´nski et al., 2020).\n",
      "AmericasNLP 2021 and 2023 ST In 2021, the\n",
      "AmericasNLP community organized a workshop\n",
      "on Machine Translation for 10 indigenous lan-\n",
      "guages of the Americas in 2021 (Mager et al.,\n",
      "2021) and 2023 (Ebrahimi et al., 2023) with an\n",
      "additional indigenous language (Chatino). The\n",
      "AmericasNLP shared task winner was (Vázquez\n",
      "et al., 2021) in 2021, and a more mixed result\n",
      "in 20237. Other participants in this shared task\n",
      "are (Nagoudi et al., 2021; Bollmann et al., 2021;\n",
      "Zheng et al., 2021; Knowles et al., 2021; Parida\n",
      "et al., 2021; Nagoudi et al., 2021). It is impor-\n",
      "tant to point at the importance of clean datata. For\n",
      "Quechua, (Moreno, 2021) got the best results gen-\n",
      "erating an additional amount of clean data.\n",
      "AmericasNLP 2022 Competition is a com-\n",
      "petition on Speech-to-Text translation is or-\n",
      "ganized and is targeting the following lan-\n",
      "guage pairs: Bribri–Spanish, Guaraní–Spanish,\n",
      "Kotiria–Portuguese, Wa’ikhana–Portuguese, and\n",
      "7Up to this moment, no official desciption papers for the\n",
      "2023 are published.Quechua–Spanish (Ebrahim et al., 2023)8.\n",
      "8 Ethical aspects\n",
      "When working with ILAs are also interacting with\n",
      "communities and nations that speak these lan-\n",
      "guages. In most cases, these speakers have been\n",
      "exposed to a colonial past, or to a local oppres-\n",
      "sion, by the majority language and culture. It is\n",
      "important to point to best practices and recom-\n",
      "mendations when performing our research. Bird\n",
      "(2020) and Liu et al. (2022) advocate to include\n",
      "community members as co-authors (Liu et al.,\n",
      "2022) as well as considering data and technology\n",
      "sovereignty. This is also aligned with the com-\n",
      "munity building aimed at by Zhang et al. (2022).\n",
      "Mager et al. (2023) summarizes the main aspects\n",
      "that should be considered as follows: i) Consul-\n",
      "tation, Negotiation and Mutual Understanding . It\n",
      "is important to inform the community about the\n",
      "planned research, negotiating a possible outcome,\n",
      "and reaching a mutual agreement on the direc-\n",
      "tions and details of the project should happen in\n",
      "all cases. ii) Respect of the local culture and in-\n",
      "volvement . As each community has its own cul-\n",
      "ture and view of the world, researchers should be\n",
      "familiar with the history and traditions of the com-\n",
      "munity. Also, it should be recommended that lo-\n",
      "cal researchers, speakers, or internal governments\n",
      "should be involved in the project. iii) Sharing and\n",
      "distribution of data and research . The product\n",
      "of the research should be available for use by the\n",
      "community, so they can take advantage of the gen-\n",
      "erated materials, like papers, books, or data.\n",
      "9 Conclusion\n",
      "Machine translation for ILA has gained interest in\n",
      "the NLP community over the last few years. Here,\n",
      "we provide an exhaustive overview of the basic\n",
      "MT concepts and the particular challenges for MT\n",
      "for ILA (in the context of low-resource scenarios\n",
      "and its relation to endangered languages). We ad-\n",
      "ditionally survey the current advances of MT for\n",
      "these languages.\n",
      "Limitations\n",
      "This paper’s aim is to give an introduction to re-\n",
      "searchers, students, of interested community in-\n",
      "digenous community members to the topic of Ma-\n",
      "chine Translation for Indigenous languages of the\n",
      "8http://turing.iimas.unam.mx/\n",
      "americasnlp/st.htmlAmericas. Therefore, this paper is not an in-depth\n",
      "survey of the literature on indigenous languages\n",
      "nor a more technical survey of low-resource ma-\n",
      "chine translation. We would point the reader to\n",
      "more specific surveys on these aspects (Haddow\n",
      "et al., 2022; Mager et al., 2018b).\n",
      "Ethical statement\n",
      "We could not find any specific Ethical issue for\n",
      "this paper or potential danger. Nevertheless, we\n",
      "want to point to the reader that working with in-\n",
      "digenous languages (in this case, MT) implies a\n",
      "set of ethical questions that are important to han-\n",
      "dle. For a deeper understanding of the matter, we\n",
      "suggest specialized literature to the reader (Mager\n",
      "et al., 2023; Bird, 2020; Schwartz, 2022).\n",
      "References\n",
      "Ahmed Abdelali, James Cowie, Steve Helmreich,\n",
      "Wanying Jin, Maria Pilar Milagros, Bill Ogden,\n",
      "Mansouri Rad, and Ron Zacharski. 2006. Guarani:\n",
      "A case study in resource development for quick\n",
      "ramp-up MT. In Proceedings of the 7th Confer-\n",
      "ence of the Association for Machine Translation in\n",
      "the Americas: Technical Papers , pages 1–9, Cam-\n",
      "bridge, Massachusetts, USA. Association for Ma-\n",
      "chine Translation in the Americas.\n",
      "Idris Abdulmumin, Bashir Shehu Galadanci, and Aliyu\n",
      "Garba. 2019. Tag-less back-translation. arXiv\n",
      "preprint arXiv:1912.10514 .\n",
      "Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019.\n",
      "Massively multilingual neural machine translation.\n",
      "InProceedings of the 2019 Conference of the North\n",
      "American Chapter of the Association for Computa-\n",
      "tional Linguistics: Human Language Technologies,\n",
      "Volume 1 (Long and Short Papers) , pages 3874–\n",
      "3884.\n",
      "Benyamin Ahmadnia and Bonnie J Dorr. 2019. Aug-\n",
      "menting neural machine translation through round-\n",
      "trip training approach. Open Computer Science ,\n",
      "9(1):268–278.\n",
      "Antonios Anastasopoulos and David Chiang. 2018.\n",
      "Tied multitask learning for neural speech translation.\n",
      "InProceedings of the 2018 Conference of the North\n",
      "American Chapter of the Association for Computa-\n",
      "tional Linguistics: Human Language Technologies,\n",
      "Volume 1 (Long Papers) , pages 82–91, New Orleans,\n",
      "Louisiana. Association for Computational Linguis-\n",
      "tics.\n",
      "Stephen R Anderson and Stephen Anderson. 2012.\n",
      "Languages: A very short introduction , volume 320.\n",
      "Oxford University Press.Naveen Arivazhagan, Ankur Bapna, Orhan Firat,\n",
      "Roee Aharoni, Melvin Johnson, and Wolfgang\n",
      "Macherey. 2019a. The missing ingredient in zero-\n",
      "shot neural machine translation. arXiv preprint\n",
      "arXiv:1903.07091 .\n",
      "Naveen Arivazhagan, Ankur Bapna, Orhan Firat,\n",
      "Dmitry Lepikhin, Melvin Johnson, Maxim Krikun,\n",
      "Mia Xu Chen, Yuan Cao, George Foster, Colin\n",
      "Cherry, et al. 2019b. Massively multilingual neural\n",
      "machine translation in the wild: Findings and chal-\n",
      "lenges. arXiv preprint arXiv:1907.05019 .\n",
      "Mikel Artetxe, Gorka Labaka, Eneko Agirre, and\n",
      "Kyunghyun Cho. 2018. Unsupervised neural ma-\n",
      "chine translation. In 6th International Conference\n",
      "on Learning Representations, ICLR 2018 .\n",
      "Mikel Artetxe and Holger Schwenk. 2019. Mas-\n",
      "sively multilingual sentence embeddings for zero-\n",
      "shot cross-lingual transfer and beyond. Transac-\n",
      "tions of the Association for Computational Linguis-\n",
      "tics, 7:597–610.\n",
      "Duygu Ataman and Marcello Federico. 2018. Compo-\n",
      "sitional representation of morphologically-rich input\n",
      "for neural machine translation. In Proceedings of\n",
      "the 56th Annual Meeting of the Association for Com-\n",
      "putational Linguistics (Volume 2: Short Papers) ,\n",
      "pages 305–311.\n",
      "Duygu Ataman, Matteo Negri, Marco Turchi, and Mar-\n",
      "cello Federico. 2017. Linguistically motivated vo-\n",
      "cabulary reduction for neural machine translation\n",
      "from turkish to english.\n",
      "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\n",
      "ton. 2016. Layer normalization. stat, 1050:21.\n",
      "Mark C Baker. 1996. The polysynthesis parameter .\n",
      "Oxford University Press.\n",
      "Mona Baker. 2018. In other words: A coursebook on\n",
      "translation . Routledge.\n",
      "Loïc Barrault, Magdalena Biesialska, Ond ˇrej Bojar,\n",
      "Marta R. Costa-jussà, Christian Federmann, Yvette\n",
      "Graham, Roman Grundkiewicz, Barry Haddow,\n",
      "Matthias Huck, Eric Joanis, Tom Kocmi, Philipp\n",
      "Koehn, Chi-kiu Lo, Nikola Ljubeši ´c, Christof\n",
      "Monz, Makoto Morishita, Masaaki Nagata, Toshi-\n",
      "aki Nakazawa, Santanu Pal, Matt Post, and Marcos\n",
      "Zampieri. 2020. Findings of the 2020 conference on\n",
      "machine translation (WMT20). In Proceedings of\n",
      "the Fifth Conference on Machine Translation , pages\n",
      "1–55, Online. Association for Computational Lin-\n",
      "guistics.\n",
      "Andrew R Barron. 1993. Universal approximation\n",
      "bounds for superpositions of a sigmoidal func-\n",
      "tion. IEEE Transactions on Information theory ,\n",
      "39(3):930–945.\n",
      "Christos Baziotis, Barry Haddow, and Alexandra\n",
      "Birch. 2020. Language model prior for low-\n",
      "resource neural machine translation. arXiv preprint\n",
      "arXiv:2004.14928 .Yonatan Belinkov and Yonatan Bisk. 2018. Synthetic\n",
      "and natural noise both break neural machine transla-\n",
      "tion. In International Conference on Learning Rep-\n",
      "resentations .\n",
      "Steven Bird. 2020. Decolonising speech and lan-\n",
      "guage technology. In Proceedings of the 28th Inter-\n",
      "national Conference on Computational Linguistics ,\n",
      "pages 3504–3519, Barcelona, Spain (Online). Inter-\n",
      "national Committee on Computational Linguistics.\n",
      "Graeme Blackwood, Miguel Ballesteros, and Todd\n",
      "Ward. 2018. Multilingual neural machine transla-\n",
      "tion with task-specific attention. In Proceedings of\n",
      "the 27th International Conference on Computational\n",
      "Linguistics , pages 3112–3122.\n",
      "Piotr Bojanowski, Edouard Grave, Armand Joulin, and\n",
      "Tomas Mikolov. 2017. Enriching word vectors with\n",
      "subword information. Transactions of the Associa-\n",
      "tion for Computational Linguistics , 5:135–146.\n",
      "Marcel Bollmann, Rahul Aralikatte, Héctor Murri-\n",
      "eta Bello, Daniel Hershcovich, Miryam de Lhoneux,\n",
      "and Anders Søgaard. 2021. Moses and the\n",
      "character-based random babbling baseline:\n",
      "CoAStaL at AmericasNLP 2021 shared task.\n",
      "InProceedings of the First Workshop on Natural\n",
      "Language Processing for Indigenous Languages of\n",
      "the Americas , pages 248–254, Online. Association\n",
      "for Computational Linguistics.\n",
      "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\n",
      "Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\n",
      "Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n",
      "Askell, Sandhini Agarwal, Ariel Herbert-V oss,\n",
      "Gretchen Krueger, Tom Henighan, Rewon Child,\n",
      "Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\n",
      "Clemens Winter, Christopher Hesse, Mark Chen,\n",
      "Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\n",
      "Chess, Jack Clark, Christopher Berner, Sam Mc-\n",
      "Candlish, Alec Radford, Ilya Sutskever, and Dario\n",
      "Amodei. 2020. Language models are few-shot\n",
      "learners.\n",
      "Gina Bustamante, Arturo Oncevay, and Roberto\n",
      "Zariquiey. 2020. No data to crawl? monolingual\n",
      "corpus creation from PDF files of truly low-resource\n",
      "languages in Peru. In Proceedings of the 12th Lan-\n",
      "guage Resources and Evaluation Conference , pages\n",
      "2914–2923, Marseille, France. European Language\n",
      "Resources Association.\n",
      "Lyle Campbell. 2000. American Indian languages: the\n",
      "historical linguistics of Native America , volume 4.\n",
      "Oxford University Press on Demand.\n",
      "Rich Caruana. 1997. Multitask learning. Machine\n",
      "learning , 28(1):41–75.\n",
      "Isaac Caswell, Ciprian Chelba, and David Grangier.\n",
      "2019. Tagged back-translation. In Proceedings of\n",
      "the Fourth Conference on Machine Translation (Vol-\n",
      "ume 1: Research Papers) , pages 53–63.Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao.\n",
      "2020. Evaluation of text generation: A survey.\n",
      "arXiv preprint arXiv:2006.14799 .\n",
      "Özlem Çetino ˘glu. 2017. A code-switching corpus of\n",
      "Turkish-German conversations. In Proceedings of\n",
      "the 11th Linguistic Annotation Workshop , pages 34–\n",
      "40, Valencia, Spain. Association for Computational\n",
      "Linguistics.\n",
      "Bharathi Raja Chakravarthi, Ruba Priyadharshini,\n",
      "Shubhanker Banerjee, Richard Saldanha, John P.\n",
      "McCrae, Anand Kumar M, Parameswari Krishna-\n",
      "murthy, and Melvin Johnson. 2021. Findings of the\n",
      "shared task on machine translation in Dravidian lan-\n",
      "guages. In Proceedings of the First Workshop on\n",
      "Speech and Language Technologies for Dravidian\n",
      "Languages , pages 119–125, Kyiv. Association for\n",
      "Computational Linguistics.\n",
      "Eirini Chatzikoumi. 2020. How to evaluate machine\n",
      "translation: A review of automated and human met-\n",
      "rics. Natural Language Engineering , 26(2):137–\n",
      "161.\n",
      "Guanhua Chen, Shuming Ma, Yun Chen, Li Dong,\n",
      "Dongdong Zhang, Jia Pan, Wenping Wang, and Furu\n",
      "Wei. 2021. Zero-shot cross-lingual transfer of neu-\n",
      "ral machine translation with multilingual pretrained\n",
      "encoders. In Proceedings of the 2021 Conference on\n",
      "Empirical Methods in Natural Language Process-\n",
      "ing, pages 15–26, Online and Punta Cana, Domini-\n",
      "can Republic. Association for Computational Lin-\n",
      "guistics.\n",
      "Yong Cheng. 2019. Joint training for pivot-based neu-\n",
      "ral machine translation. In Joint Training for Neural\n",
      "Machine Translation , pages 41–54. Springer.\n",
      "Yong Cheng, Lu Jiang, and Wolfgang Macherey. 2019.\n",
      "Robust neural machine translation with doubly ad-\n",
      "versarial inputs. In Proceedings of the 57th Annual\n",
      "Meeting of the Association for Computational Lin-\n",
      "guistics , pages 4324–4333.\n",
      "Yong Cheng, Lu Jiang, Wolfgang Macherey, and Ja-\n",
      "cob Eisenstein. 2020. AdvAug: Robust adversar-\n",
      "ial augmentation for neural machine translation. In\n",
      "Proceedings of the 58th Annual Meeting of the Asso-\n",
      "ciation for Computational Linguistics , pages 5961–\n",
      "5970, Online. Association for Computational Lin-\n",
      "guistics.\n",
      "Yong Cheng, Zhaopeng Tu, Fandong Meng, Junjie\n",
      "Zhai, and Yang Liu. 2018. Towards robust neural\n",
      "machine translation. In Proceedings of the 56th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics (Volume 1: Long Papers) , pages 1756–\n",
      "1766.\n",
      "Luis Chiruzzo, Pedro Amarilla, Adolfo Ríos, and Gus-\n",
      "tavo Giménez Lugo. 2020. Development of a\n",
      "Guarani - Spanish parallel corpus. In Proceedings of\n",
      "the 12th Language Resources and Evaluation Con-\n",
      "ference , pages 2629–2633, Marseille, France. Euro-\n",
      "pean Language Resources Association.Luis Chiruzzo, Santiago Góngora, Aldo Alvarez, Gus-\n",
      "tavo Giménez-Lugo, Marvin Agüero-Torales, and\n",
      "Yliana Rodríguez. 2022. Jojajovai: A parallel\n",
      "guarani-spanish corpus for mt benchmarking. In\n",
      "Proceedings of the Thirteenth Language Resources\n",
      "and Evaluation Conference , pages 2098–2107.\n",
      "Kyunghyun Cho, Bart van Merriënboer, Caglar Gul-\n",
      "cehre, Dzmitry Bahdanau, Fethi Bougares, Holger\n",
      "Schwenk, and Yoshua Bengio. 2014. Learning\n",
      "phrase representations using rnn encoder–decoder\n",
      "for statistical machine translation. In Proceedings of\n",
      "the 2014 Conference on Empirical Methods in Nat-\n",
      "ural Language Processing (EMNLP) , pages 1724–\n",
      "1734.\n",
      "Trevor Cohn and Mirella Lapata. 2007. Machine trans-\n",
      "lation by triangulation: Making effective use of\n",
      "multi-parallel corpora. In Proceedings of the 45th\n",
      "Annual Meeting of the Association of Computational\n",
      "Linguistics , pages 728–735.\n",
      "Alexis Conneau and Guillaume Lample. 2019. Cross-\n",
      "lingual language model pretraining. In Advances\n",
      "in Neural Information Processing Systems , pages\n",
      "7057–7067.\n",
      "Alexis Conneau, Guillaume Lample, Marc’Aurelio\n",
      "Ranzato, Ludovic Denoyer, and Hervé Jégou. 2017.\n",
      "Word translation without parallel data. arXiv\n",
      "preprint arXiv:1710.04087 .\n",
      "Raj Dabre, Chenhui Chu, and Anoop Kunchukuttan.\n",
      "2019. A survey of multilingual neural machine\n",
      "translation. arXiv preprint arXiv:1905.05395 .\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\n",
      "Kristina Toutanova. 2019. Bert: Pre-training of\n",
      "deep bidirectional transformers for language under-\n",
      "standing. In Proceedings of the 2019 Conference of\n",
      "the North American Chapter of the Association for\n",
      "Computational Linguistics: Human Language Tech-\n",
      "nologies, Volume 1 (Long and Short Papers) , pages\n",
      "4171–4186.\n",
      "Xiangyu Duan, Baijun Ji, Hao Jia, Min Tan, Min\n",
      "Zhang, Boxing Chen, Weihua Luo, and Yue Zhang.\n",
      "2020. Bilingual dictionary based neural machine\n",
      "translation without using parallel sentences. In Pro-\n",
      "ceedings of the 58th Annual Meeting of the Asso-\n",
      "ciation for Computational Linguistics , pages 1570–\n",
      "1579.\n",
      "Abteen Ebrahim, Manuel Mager, Pavel Oncevay Ar-\n",
      "turo Danni Liu Koneru Sai Ugan Enes Yavuz\n",
      "Wiemerslage, Adam Denisov, Zhaolin Li, Jan\n",
      "Niehues, Monica Romero, Ivan G Torre, Tanel\n",
      "Alumäe, Jiaming Kong, Sergey Polezhaev, Yury\n",
      "Belousov, Wei-Rui Chen, Peter Sullivan, Ife\n",
      "Adebara, Bashar Talafha, Inciarte Alcides Al-\n",
      "coba, Muhammad Abdul-Mageed, Luis Chiruzzo,\n",
      "Rolando Coto-Solano, Hilaria Cruz, Sofía Flores-\n",
      "Solórzano, Aldo Andrés Alvarez López, Ivan Meza-\n",
      "Ruiz, John E. Ortega, Alexis Palmer, Rodolfo Joel\n",
      "Zevallos Salazar, Kristine, Thang Vu Stenzel, andKatharina Kann. 2023. Findings of the second amer-\n",
      "icasnlp competition on speech-to-text translation.\n",
      "preprint .\n",
      "Abteen Ebrahimi, Manuel Mager, Arturo Once-\n",
      "vay, Vishrav Chaudhary, Luis Chiruzzo, Angela\n",
      "Fan, John Ortega, Ricardo Ramos, Annette Rios,\n",
      "Ivan Vladimir Meza Ruiz, Gustavo Giménez-Lugo,\n",
      "Elisabeth Mager, Graham Neubig, Alexis Palmer,\n",
      "Rolando Coto-Solano, Thang Vu, and Katharina\n",
      "Kann. 2022. AmericasNLI: Evaluating zero-shot\n",
      "natural language understanding of pretrained multi-\n",
      "lingual models in truly low-resource languages. In\n",
      "Proceedings of the 60th Annual Meeting of the As-\n",
      "sociation for Computational Linguistics (Volume 1:\n",
      "Long Papers) , pages 6279–6299, Dublin, Ireland.\n",
      "Association for Computational Linguistics.\n",
      "Abteen Ebrahimi, Manuel Mager, Arturo Oncevay,\n",
      "Vishrav Chaudhary, Luis Chiruzzo, Angela Fan,\n",
      "John Ortega, Ricardo Ramos, Annette Rios, Ivan\n",
      "Vladimir, Gustavo A. Giménez-Lugo, Elisabeth\n",
      "Mager, Graham Neubig, Alexis Palmer, Rolando\n",
      "A. Coto Solano, Ngoc Thang Vu, and Katharina\n",
      "Kann. 2021. Americasnli: Evaluating zero-shot nat-\n",
      "ural language understanding of pretrained multilin-\n",
      "gual models in truly low-resource languages.\n",
      "Abteen Ebrahimi, Manuel Mager, Arturo Oncevay,\n",
      "Enora Rice, Cynthia Montaño, John Ortega, Shruti\n",
      "Rijhwani, Alexis Palmer, Rolando Coto-Solano, Hi-\n",
      "laria Cruz, and Katharina Kann. 2023. Findings\n",
      "of the AmericasNLP 2023 shared task on machine\n",
      "translation into indigenous languages. In Proceed-\n",
      "ings of the Third Workshop on Natural Language\n",
      "Processing for Indigenous Languages of the Amer-\n",
      "icas. Association for Computational Linguistics.\n",
      "Javid Ebrahimi, Daniel Lowd, and Dejing Dou. 2018.\n",
      "On adversarial examples for character-level neural\n",
      "machine translation. In Proceedings of the 27th In-\n",
      "ternational Conference on Computational Linguis-\n",
      "tics, pages 653–663, Santa Fe, New Mexico, USA.\n",
      "Association for Computational Linguistics.\n",
      "Sergey Edunov, Alexei Baevski, and Michael Auli.\n",
      "2019. Pre-trained language model representations\n",
      "for language generation. In Proceedings of the 2019\n",
      "Conference of the North American Chapter of the\n",
      "Association for Computational Linguistics: Human\n",
      "Language Technologies, Volume 1 (Long and Short\n",
      "Papers) , pages 4052–4059.\n",
      "Sergey Edunov, Myle Ott, Michael Auli, and David\n",
      "Grangier. 2018. Understanding back-translation at\n",
      "scale. In Proceedings of the 2018 Conference on\n",
      "Empirical Methods in Natural Language Process-\n",
      "ing, pages 489–500.\n",
      "Marzieh Fadaee, Arianna Bisazza, and Christof Monz.\n",
      "2017. Data augmentation for low-resource neural\n",
      "machine translation. In Proceedings of the 55th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics (Volume 2: Short Papers) , pages 567–\n",
      "573.Isaac Feldman and Rolando Coto-Solano. 2020. Neu-\n",
      "ral machine translation models with back-translation\n",
      "for the extremely low-resource indigenous language\n",
      "Bribri. In Proceedings of the 28th International\n",
      "Conference on Computational Linguistics , pages\n",
      "3965–3976, Barcelona, Spain (Online). Interna-\n",
      "tional Committee on Computational Linguistics.\n",
      "Manuel Fernández-Delgado, Eva Cernadas, Senén\n",
      "Barro, and Dinani Amorim. 2014. Do we need hun-\n",
      "dreds of classifiers to solve real world classification\n",
      "problems? The journal of machine learning re-\n",
      "search , 15(1):3133–3181.\n",
      "Alexander Fraser. 2020. Findings of the WMT 2020\n",
      "shared tasks in unsupervised MT and very low re-\n",
      "source supervised MT. In Proceedings of the Fifth\n",
      "Conference on Machine Translation , pages 765–\n",
      "771, Online. Association for Computational Lin-\n",
      "guistics.\n",
      "Ana-Paula Galarreta, Andrés Melgar, and Arturo On-\n",
      "cevay. 2017. Corpus creation and initial SMT ex-\n",
      "periments between Spanish and Shipibo-konibo. In\n",
      "Proceedings of the International Conference Recent\n",
      "Advances in Natural Language Processing, RANLP\n",
      "2017 , pages 238–244, Varna, Bulgaria. INCOMA\n",
      "Ltd.\n",
      "Candace Kaleimamoowahinekapu Galla. 2016. Indige-\n",
      "nous language revitalization, promotion, and educa-\n",
      "tion: Function of digital technology. Computer As-\n",
      "sisted Language Learning , 29(7):1137–1151.\n",
      "Xavier Garcia, Pierre Foret, Thibault Sellam, and\n",
      "Ankur P Parikh. 2020. A multilingual view of\n",
      "unsupervised machine translation. arXiv preprint\n",
      "arXiv:2002.02955 .\n",
      "Mozhdeh Gheini, Xiang Ren, and Jonathan May. 2021.\n",
      "Cross-attention is all you need: Adapting pretrained\n",
      "Transformers for machine translation. In Proceed-\n",
      "ings of the 2021 Conference on Empirical Methods\n",
      "in Natural Language Processing , pages 1754–1765,\n",
      "Online and Punta Cana, Dominican Republic. Asso-\n",
      "ciation for Computational Linguistics.\n",
      "Santiago Góngora, Nicolás Giossa, and Luis Chiruzzo.\n",
      "2021. Experiments on a Guarani corpus of news\n",
      "and social media. In Proceedings of the First Work-\n",
      "shop on Natural Language Processing for Indige-\n",
      "nous Languages of the Americas , pages 153–158,\n",
      "Online. Association for Computational Linguistics.\n",
      "Santiago Góngora, Nicolás Giossa, and Luis Chiruzzo.\n",
      "2022. Can we use word embeddings for enhancing\n",
      "Guarani-Spanish machine translation? In Proceed-\n",
      "ings of the Fifth Workshop on the Use of Compu-\n",
      "tational Methods in the Study of Endangered Lan-\n",
      "guages , pages 127–132, Dublin, Ireland. Associa-\n",
      "tion for Computational Linguistics.\n",
      "Yvette Graham, Barry Haddow, and Philipp Koehn.\n",
      "2020. Statistical power and translationese in ma-\n",
      "chine translation evaluation. In Proceedings of the2020 Conference on Empirical Methods in Natural\n",
      "Language Processing (EMNLP) , pages 72–81, On-\n",
      "line. Association for Computational Linguistics.\n",
      "Stig-Arne Grönroos, Sami Virpioja, Peter Smit, and\n",
      "Mikko Kurimo. 2014. Morfessor flatcat: An hmm-\n",
      "based method for unsupervised and semi-supervised\n",
      "learning of morphology. In Proceedings of COLING\n",
      "2014, the 25th International Conference on Compu-\n",
      "tational Linguistics: Technical Papers , pages 1177–\n",
      "1185.\n",
      "Jiatao Gu, Yong Wang, Kyunghyun Cho, and Vic-\n",
      "tor OK Li. 2019. Improved zero-shot neural ma-\n",
      "chine translation via ignoring spurious correlations.\n",
      "InProceedings of the 57th Annual Meeting of the\n",
      "Association for Computational Linguistics , pages\n",
      "1258–1268.\n",
      "Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun\n",
      "Cho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,\n",
      "Holger Schwenk, and Yoshua Bengio. 2015. On us-\n",
      "ing monolingual corpora in neural machine transla-\n",
      "tion. arXiv preprint arXiv:1503.03535 .\n",
      "Ximena Gutierrez-Vasques, Gerardo Sierra, and\n",
      "Isaac Hernandez Pompa. 2016. Axolotl: a web\n",
      "accessible parallel corpus for Spanish-Nahuatl. In\n",
      "Proceedings of the Tenth International Conference\n",
      "on Language Resources and Evaluation (LREC’16) ,\n",
      "pages 4210–4214, Portorož, Slovenia. European\n",
      "Language Resources Association (ELRA).\n",
      "Barry Haddow, Rachel Bawden, Antonio Valerio\n",
      "Miceli Barone, Jind ˇrich Helcl, and Alexandra Birch.\n",
      "2022. Survey of low-resource machine translation.\n",
      "Computational Linguistics , pages 1–67.\n",
      "Lifeng Han. 2016. Machine translation evaluation re-\n",
      "sources and methods: A survey. arXiv preprint\n",
      "arXiv:1605.04515 .\n",
      "François Hernandez and Vincent Nguyen. 2020. The\n",
      "ubiqus English-Inuktitut system for WMT20. In\n",
      "Proceedings of the Fifth Conference on Machine\n",
      "Translation , pages 213–217, Online. Association for\n",
      "Computational Linguistics.\n",
      "Nikolaus P Himmelmann. 2008. Language documen-\n",
      "tation: What is it and what is it good for? In Es-\n",
      "sentials of language documentation , pages 1–30. De\n",
      "Gruyter Mouton.\n",
      "Vu Cong Duy Hoang, Philipp Koehn, Gholamreza\n",
      "Haffari, and Trevor Cohn. 2018. Iterative back-\n",
      "translation for neural machine translation. In Pro-\n",
      "ceedings of the 2nd Workshop on Neural Machine\n",
      "Translation and Generation , pages 18–24.\n",
      "Sepp Hochreiter and Jürgen Schmidhuber. 1997.\n",
      "Long short-term memory. Neural computation ,\n",
      "9(8):1735–1780.\n",
      "J Edward Hu, Huda Khayrallah, Ryan Culkin, Patrick\n",
      "Xia, Tongfei Chen, Matt Post, and Benjamin\n",
      "Van Durme. 2019. Improved lexically constraineddecoding for translation and monolingual rewriting.\n",
      "InProceedings of the 2019 Conference of the North\n",
      "American Chapter of the Association for Computa-\n",
      "tional Linguistics: Human Language Technologies,\n",
      "Volume 1 (Long and Short Papers) , pages 839–850.\n",
      "W John Hutchins. 2004. The georgetown-ibm experi-\n",
      "ment demonstrated in january 1954. In Conference\n",
      "of the Association for Machine Translation in the\n",
      "Americas , pages 102–114. Springer.\n",
      "Sébastien Jean, Kyunghyun Cho, Roland Memisevic,\n",
      "and Yoshua Bengio. 2015. On using very large\n",
      "target vocabulary for neural machine translation.\n",
      "InProceedings of the 53rd Annual Meeting of the\n",
      "Association for Computational Linguistics and the\n",
      "7th International Joint Conference on Natural Lan-\n",
      "guage Processing (Volume 1: Long Papers) , pages\n",
      "1–10, Beijing, China. Association for Computa-\n",
      "tional Linguistics.\n",
      "Eric Joanis, Rebecca Knowles, Roland Kuhn, Samuel\n",
      "Larkin, Patrick Littell, Chi-kiu Lo, Darlene Stewart,\n",
      "and Jeffrey Micher. 2020. The Nunavut Hansard\n",
      "Inuktitut–English parallel corpus 3.0 with prelimi-\n",
      "nary machine translation results. In Proceedings of\n",
      "the 12th Language Resources and Evaluation Con-\n",
      "ference , pages 2562–2572, Marseille, France. Euro-\n",
      "pean Language Resources Association.\n",
      "Melvin Johnson, Mike Schuster, Quoc V Le, Maxim\n",
      "Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\n",
      "Fernanda Viégas, Martin Wattenberg, Greg Corrado,\n",
      "et al. 2017. Google’s multilingual neural machine\n",
      "translation system: Enabling zero-shot translation.\n",
      "Transactions of the Association for Computational\n",
      "Linguistics , 5:339–351.\n",
      "David Kamholz, Jonathan Pool, and Susan M Colow-\n",
      "ick. 2014. Panlex: Building a resource for panlin-\n",
      "gual lexical translation. In LREC , pages 3145–3150.\n",
      "Katharina Kann, Jesus Manuel Mager Hois,\n",
      "Ivan Vladimir Meza-Ruiz, and Hinrich Schütze.\n",
      "2018. Fortification of neural morphological\n",
      "segmentation models for polysynthetic minimal-\n",
      "resource languages. In Proceedings of the 2018\n",
      "Conference of the North American Chapter of the\n",
      "Association for Computational Linguistics: Human\n",
      "Language Technologies, Volume 1 (Long Papers) ,\n",
      "pages 47–57, New Orleans, Louisiana. Association\n",
      "for Computational Linguistics.\n",
      "Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua\n",
      "Bengio. 2017. Generalization in deep learning.\n",
      "arXiv preprint arXiv:1710.05468 .\n",
      "Huda Khayrallah, Brian Thompson, Matt Post, and\n",
      "Philipp Koehn. 2020. Simulated multiple reference\n",
      "training improves low-resource machine translation.\n",
      "arXiv preprint arXiv:2004.14524 .\n",
      "Rebecca Knowles, Darlene Stewart, Samuel Larkin,\n",
      "and Patrick Littell. 2020. NRC systems for the 2020\n",
      "Inuktitut-English news translation task. In Proceed-\n",
      "ings of the Fifth Conference on Machine Translation ,pages 156–170, Online. Association for Computa-\n",
      "tional Linguistics.\n",
      "Rebecca Knowles, Darlene Stewart, Samuel Larkin,\n",
      "and Patrick Littell. 2021. NRC-CNRC machine\n",
      "translation systems for the 2021 AmericasNLP\n",
      "shared task. In Proceedings of the First Workshop on\n",
      "Natural Language Processing for Indigenous Lan-\n",
      "guages of the Americas , pages 224–233, Online. As-\n",
      "sociation for Computational Linguistics.\n",
      "Sosuke Kobayashi. 2018. Contextual augmentation:\n",
      "Data augmentation by words with paradigmatic re-\n",
      "lations. In Proceedings of the 2018 Conference of\n",
      "the North American Chapter of the Association for\n",
      "Computational Linguistics: Human Language Tech-\n",
      "nologies, Volume 2 (Short Papers) , pages 452–457.\n",
      "Tom Kocmi. 2020. CUNI submission for the Inuk-\n",
      "titut language in WMT news 2020. In Proceed-\n",
      "ings of the Fifth Conference on Machine Translation ,\n",
      "pages 171–174, Online. Association for Computa-\n",
      "tional Linguistics.\n",
      "Michael Krauss. 1992. The world’s languages in crisis.\n",
      "Language , 68(1):4–10.\n",
      "Mateusz Krubi ´nski, Marcin Chochowski, Bartłomiej\n",
      "Boczek, Mikołaj Koszowski, Adam Dobrowolski,\n",
      "Marcin Szyma ´nski, and Paweł Przybysz. 2020.\n",
      "Samsung R&D institute Poland submission to\n",
      "WMT20 news translation task. In Proceedings of\n",
      "the Fifth Conference on Machine Translation , pages\n",
      "181–190, Online. Association for Computational\n",
      "Linguistics.\n",
      "Surafel M Lakew, Quintino F Lotito, Matteo Negri,\n",
      "Marco Turchi, and Marcello Federico. 2018. Im-\n",
      "proving zero-shot translation of low-resource lan-\n",
      "guages. In Proceedings of the 14h IWSLT , pages\n",
      "113–119.\n",
      "Guillaume Lample, Alexis Conneau, Ludovic Denoyer,\n",
      "and Marc’Aurelio Ranzato. 2017. Unsupervised\n",
      "machine translation using monolingual corpora only.\n",
      "arXiv preprint arXiv:1711.00043 .\n",
      "Sahinur Rahman Laskar, Abdullah Faiz Ur Rah-\n",
      "man Khilji, Partha Pakray, and Sivaji Bandyopad-\n",
      "hyay. 2020. Zero-shot neural machine translation:\n",
      "Russian-Hindi @LoResMT 2020. In Proceedings\n",
      "of the 3rd Workshop on Technologies for MT of Low\n",
      "Resource Languages , pages 38–42, Suzhou, China.\n",
      "Association for Computational Linguistics.\n",
      "Yichong Leng, Xu Tan, Tao Qin, Xiang-Yang Li, and\n",
      "Tie-Yan Liu. 2019. Unsupervised pivot translation\n",
      "for distant languages. In Proceedings of the 57th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics , pages 175–183.\n",
      "M Paul Lewis. 2009. Ethnologue: Languages of the\n",
      "world . SIL international.\n",
      "M Paul Lewis and Gary F Simons. 2010. Assessing\n",
      "endangerment: expanding fishman’s gids. Revue\n",
      "roumaine de linguistique , 55(2):103–120.Mike Lewis, Yinhan Liu, Naman Goyal, Mar-\n",
      "jan Ghazvininejad, Abdelrahman Mohamed, Omer\n",
      "Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\n",
      "Bart: Denoising sequence-to-sequence pre-training\n",
      "for natural language generation, translation, and\n",
      "comprehension. arXiv preprint arXiv:1910.13461 .\n",
      "Zuchao Li, Rui Wang, Kehai Chen, Masso Utiyama,\n",
      "Eiichiro Sumita, Zhuosheng Zhang, and Hai Zhao.\n",
      "2020. Data-dependent gaussian prior objective for\n",
      "language generation. In International Conference\n",
      "on Learning Representations .\n",
      "Zehui Lin, Xiao Pan, Mingxuan Wang, Xipeng Qiu,\n",
      "Jiangtao Feng, Hao Zhou, and Lei Li. 2020. Pre-\n",
      "training multilingual neural machine translation by\n",
      "leveraging alignment information. In Proceed-\n",
      "ings of the 2020 Conference on Empirical Methods\n",
      "in Natural Language Processing (EMNLP) , pages\n",
      "2649–2663, Online. Association for Computational\n",
      "Linguistics.\n",
      "Wang Ling, Isabel Trancoso, Chris Dyer, and Alan W\n",
      "Black. 2015. Character-based neural machine trans-\n",
      "lation. arXiv preprint arXiv:1511.04586 .\n",
      "Patrick Littell, Anna Kazantseva, Roland Kuhn, Aidan\n",
      "Pine, Antti Arppe, Christopher Cox, and Marie-\n",
      "Odile Junker. 2018. Indigenous language technolo-\n",
      "gies in Canada: Assessment, challenges, and suc-\n",
      "cesses. In Proceedings of the 27th International\n",
      "Conference on Computational Linguistics , pages\n",
      "2620–2632, Santa Fe, New Mexico, USA. Associ-\n",
      "ation for Computational Linguistics.\n",
      "Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\n",
      "Edunov, Marjan Ghazvininejad, Mike Lewis, and\n",
      "Luke Zettlemoyer. 2020. Multilingual denoising\n",
      "pre-training for neural machine translation. arXiv\n",
      "preprint arXiv:2001.08210 .\n",
      "Zihan Liu, Yan Xu, Genta Indra Winata, and Pascale\n",
      "Fung. 2019. Incorporating word and subword units\n",
      "in unsupervised machine translation using language\n",
      "model rescoring. In Proceedings of the Fourth Con-\n",
      "ference on Machine Translation (Volume 2: Shared\n",
      "Task Papers, Day 1) , pages 275–282.\n",
      "Zoey Liu, Crystal Richardson, Richard Hatcher Jr, and\n",
      "Emily Prud’hommeaux. 2022. Not always about\n",
      "you: Prioritizing community needs when develop-\n",
      "ing endangered language technology. arXiv preprint\n",
      "arXiv:2204.05541 .\n",
      "Chi-kiu Lo. 2020. Extended study on using pretrained\n",
      "language models and YiSi-1 for machine transla-\n",
      "tion evaluation. In Proceedings of the Fifth Confer-\n",
      "ence on Machine Translation , pages 895–902, On-\n",
      "line. Association for Computational Linguistics.\n",
      "Wolfgang Lörscher. 2005. The translation process:\n",
      "Methods and problems of its investigation. Meta:\n",
      "journal des traducteurs/Meta: Translators’ Journal ,\n",
      "50(2):597–608.Bruce T Lowerre. 1976. The harpy speech recognition\n",
      "system. Technical report, CARNEGIE-MELLON\n",
      "UNIV PITTSBURGH PA DEPT OF COMPUTER\n",
      "SCIENCE.\n",
      "Yichao Lu, Phillip Keung, Faisal Ladhak, Vikas Bhard-\n",
      "waj, Shaonan Zhang, and Jason Sun. 2018. A neu-\n",
      "ral interlingua for multilingual machine translation.\n",
      "InProceedings of the Third Conference on Machine\n",
      "Translation: Research Papers , pages 84–92.\n",
      "Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals,\n",
      "and Wojciech Zaremba. 2015. Addressing the rare\n",
      "word problem in neural machine translation. In Pro-\n",
      "ceedings of the 53rd Annual Meeting of the Associ-\n",
      "ation for Computational Linguistics and the 7th In-\n",
      "ternational Joint Conference on Natural Language\n",
      "Processing (Volume 1: Long Papers) , pages 11–19,\n",
      "Beijing, China. Association for Computational Lin-\n",
      "guistics.\n",
      "Manuel Mager, Diónico Carrillo, and Ivan Meza.\n",
      "2018a. Probabilistic finite-state morphological seg-\n",
      "menter for wixarika (huichol) language. Journal of\n",
      "Intelligent & Fuzzy Systems , 34(5):3081–3087.\n",
      "Manuel Mager, Özlem Çetino ˘glu, and Katharina Kann.\n",
      "2019. Subword-level language identification for\n",
      "intra-word code-switching. In Proceedings of the\n",
      "2019 Conference of the North American Chapter of\n",
      "the Association for Computational Linguistics: Hu-\n",
      "man Language Technologies, Volume 1 (Long and\n",
      "Short Papers) , pages 2005–2011, Minneapolis, Min-\n",
      "nesota. Association for Computational Linguistics.\n",
      "Manuel Mager, Özlem Çetino ˘glu, and Katharina\n",
      "Kann. 2020. Tackling the low-resource chal-\n",
      "lenge for canonical segmentation. arXiv preprint\n",
      "arXiv:2010.02804 .\n",
      "Manuel Mager, Ximena Gutierrez-Vasques, Gerardo\n",
      "Sierra, and Ivan Meza-Ruiz. 2018b. Challenges of\n",
      "language technologies for the indigenous languages\n",
      "of the Americas. In Proceedings of the 27th Inter-\n",
      "national Conference on Computational Linguistics ,\n",
      "pages 55–69, Santa Fe, New Mexico, USA. Associ-\n",
      "ation for Computational Linguistics.\n",
      "Manuel Mager, Elisabeth Mager, Katharina Kann,\n",
      "and Ngoc Thang Vu. 2023. Ethical considerations\n",
      "for machine translation of indigenous languages:\n",
      "Giving a voice to the speakers. arXiv preprint\n",
      "arXiv:2305.19474 .\n",
      "Manuel Mager, Elisabeth Mager, Alfonso Medina-\n",
      "Urrea, Ivan Vladimir Meza Ruiz, and Katharina\n",
      "Kann. 2018c. Lost in translation: Analysis of in-\n",
      "formation loss during machine translation between\n",
      "polysynthetic and fusional languages. In Proceed-\n",
      "ings of the Workshop on Computational Modeling\n",
      "of Polysynthetic Languages , pages 73–83, Santa Fe,\n",
      "New Mexico, USA. Association for Computational\n",
      "Linguistics.Manuel Mager and Ivan Meza. 2021. Retos en con-\n",
      "strucción de traductores automáticos para lenguas\n",
      "indígenas de México. Digital Scholarship in the Hu-\n",
      "manities , 36.\n",
      "Manuel Mager, Arturo Oncevay, Abteen Ebrahimi,\n",
      "John Ortega, Annette Rios, Angela Fan, Xi-\n",
      "mena Gutierrez-Vasques, Luis Chiruzzo, Gustavo\n",
      "Giménez-Lugo, Ricardo Ramos, Anna Currey,\n",
      "Vishrav Chaudhary, Ivan Vladimir Meza Ruiz,\n",
      "Rolando Coto-Solano, Alexis Palmer, Elisabeth\n",
      "Mager, Ngoc Thang Vu, Graham Neubig, and\n",
      "Katharina Kann. 2021. Findings of the Americas-\n",
      "NLP 2021 Shared Task on Open Machine Transla-\n",
      "tion for Indigenous Languages of the Americas. In\n",
      "Proceedings of theThe First Workshop on NLP for\n",
      "Indigenous Languages of the Americas , Online. As-\n",
      "sociation for Computational Linguistics.\n",
      "Manuel Mager, Arturo Oncevay, Elisabeth Mager,\n",
      "Katharina Kann, and Thang Vu. 2022. BPE vs. mor-\n",
      "phological segmentation: A case study on machine\n",
      "translation of four polysynthetic languages. In Find-\n",
      "ings of the Association for Computational Linguis-\n",
      "tics: ACL 2022 , pages 961–971, Dublin, Ireland.\n",
      "Association for Computational Linguistics.\n",
      "Chaitanya Malaviya, Graham Neubig, and Patrick Lit-\n",
      "tell. 2017. Learning language representations for ty-\n",
      "pology prediction. In Proceedings of the 2017 Con-\n",
      "ference on Empirical Methods in Natural Language\n",
      "Processing , pages 2529–2535.\n",
      "Benjamin Marie, Raphael Rubino, and Atsushi Fujita.\n",
      "2020. Tagged back-translation revisited: Why does\n",
      "it really work? In Proceedings of the 58th Annual\n",
      "Meeting of the Association for Computational Lin-\n",
      "guistics , pages 5990–5997, Online. Association for\n",
      "Computational Linguistics.\n",
      "Joel Martin, Howard Johnson, Benoit Farley, and Anna\n",
      "Maclachlan. 2003. Aligning and using an english-\n",
      "inuktitut parallel corpus. In Proceedings of the HLT-\n",
      "NAACL 2003 Workshop on Building and using par-\n",
      "allel texts: data driven machine translation and\n",
      "beyond-Volume 3 , pages 115–118. Association for\n",
      "Computational Linguistics.\n",
      "Thomas Mayer and Michael Cysouw. 2014. Creat-\n",
      "ing a massively parallel bible corpus. Oceania ,\n",
      "135(273):40.\n",
      "Arya D. McCarthy, Rachel Wicks, Dylan Lewis,\n",
      "Aaron Mueller, Winston Wu, Oliver Adams, Gar-\n",
      "rett Nicolai, Matt Post, and David Yarowsky. 2020.\n",
      "The johns hopkins university bible corpus: 1600+\n",
      "tongues for typological exploration. In Proceedings\n",
      "of The 12th Language Resources and Evaluation\n",
      "Conference , pages 2884–2892, Marseille, France.\n",
      "European Language Resources Association.\n",
      "Norman A McQuown. 1955. The indigenous lan-\n",
      "guages of latin america. American Anthropologist ,\n",
      "57(3):501–570.Antonio Valerio Miceli-Barone, Jind ˇrich Helcl, Rico\n",
      "Sennrich, Barry Haddow, and Alexandra Birch.\n",
      "2017. Deep architectures for neural machine trans-\n",
      "lation. In Proceedings of the Second Conference on\n",
      "Machine Translation , pages 99–107.\n",
      "Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-\n",
      "frey Dean. 2013. Efficient estimation of word\n",
      "representations in vector space. arXiv preprint\n",
      "arXiv:1301.3781 .\n",
      "Marianne Mithun. 1986. On the nature of noun incor-\n",
      "poration. Language , 62(1):32–37.\n",
      "Oscar Moreno. 2021. The REPU CS’ Spanish–\n",
      "Quechua submission to the AmericasNLP 2021\n",
      "shared task on open machine translation. In Pro-\n",
      "ceedings of the First Workshop on Natural Language\n",
      "Processing for Indigenous Languages of the Ameri-\n",
      "cas, pages 241–247, Online. Association for Com-\n",
      "putational Linguistics.\n",
      "El Moatez Billah Nagoudi, Wei-Rui Chen, Muham-\n",
      "mad Abdul-Mageed, and Hasan Cavusoglu. 2021.\n",
      "IndT5: A text-to-text transformer for 10 indigenous\n",
      "languages. In Proceedings of the First Workshop on\n",
      "Natural Language Processing for Indigenous Lan-\n",
      "guages of the Americas , pages 265–271, Online. As-\n",
      "sociation for Computational Linguistics.\n",
      "Wilhelmina Nekoto, Vukosi Marivate, Tshinondiwa\n",
      "Matsila, Timi Fasubaa, Taiwo Fagbohungbe,\n",
      "Solomon Oluwole Akinola, Shamsuddeen Muham-\n",
      "mad, Salomon Kabongo Kabenamualu, Salomey\n",
      "Osei, Freshia Sackey, Rubungo Andre Niyongabo,\n",
      "Ricky Macharm, Perez Ogayo, Orevaoghene Ahia,\n",
      "Musie Meressa Berhe, Mofetoluwa Adeyemi,\n",
      "Masabata Mokgesi-Selinga, Lawrence Okegbemi,\n",
      "Laura Martinus, Kolawole Tajudeen, Kevin Degila,\n",
      "Kelechi Ogueji, Kathleen Siminyu, Julia Kreutzer,\n",
      "Jason Webster, Jamiil Toure Ali, Jade Abbott,\n",
      "Iroro Orife, Ignatius Ezeani, Idris Abdulkadir\n",
      "Dangana, Herman Kamper, Hady Elsahar, Good-\n",
      "ness Duru, Ghollah Kioko, Murhabazi Espoir,\n",
      "Elan van Biljon, Daniel Whitenack, Christopher\n",
      "Onyefuluchi, Chris Chinenye Emezue, Bonaventure\n",
      "F. P. Dossou, Blessing Sibanda, Blessing Bassey,\n",
      "Ayodele Olabiyi, Arshath Ramkilowan, Alp Öktem,\n",
      "Adewale Akinfaderin, and Abdallah Bashir. 2020.\n",
      "Participatory research for low-resourced machine\n",
      "translation: A case study in African languages.\n",
      "InFindings of the Association for Computational\n",
      "Linguistics: EMNLP 2020 , pages 2144–2160,\n",
      "Online. Association for Computational Linguistics.\n",
      "Graham Neubig. 2017. Neural machine translation\n",
      "and sequence-to-sequence models: A tutorial. arXiv\n",
      "preprint arXiv:1703.01619 .\n",
      "Graham Neubig and Junjie Hu. 2018. Rapid adapta-\n",
      "tion of neural machine translation to new languages.\n",
      "InProceedings of the 2018 Conference on Empiri-\n",
      "cal Methods in Natural Language Processing , pages\n",
      "875–880.Tan Ngoc Le and Fatiha Sadat. 2020. Revitalization\n",
      "of indigenous languages through pre-processing and\n",
      "neural machine translation: The case of Inuktitut.\n",
      "InProceedings of the 28th International Conference\n",
      "on Computational Linguistics , pages 4661–4666,\n",
      "Barcelona, Spain (Online). International Committee\n",
      "on Computational Linguistics.\n",
      "Dat Quoc Nguyen, Kairit Sirts, and Mark Johnson.\n",
      "2015. Improving topic coherence with latent fea-\n",
      "ture word representations in MAP estimation for\n",
      "topic modeling. In Proceedings of the Australasian\n",
      "Language Technology Association Workshop 2015 ,\n",
      "pages 116–121, Parramatta, Australia.\n",
      "Toan Q Nguyen and David Chiang. 2017. Trans-\n",
      "fer learning across low-resource, related languages\n",
      "for neural machine translation. In Proceedings of\n",
      "the Eighth International Joint Conference on Natu-\n",
      "ral Language Processing (Volume 2: Short Papers) ,\n",
      "pages 296–301.\n",
      "Eugene Nida. 1945. Linguistics and ethnology in\n",
      "translation-problems. Word , 1(2):194–208.\n",
      "Jan Niehues and Eunah Cho. 2017. Exploiting linguis-\n",
      "tic resources for neural machine translation using\n",
      "multi-task learning. In Proceedings of the Second\n",
      "Conference on Machine Translation , pages 80–89,\n",
      "Copenhagen, Denmark. Association for Computa-\n",
      "tional Linguistics.\n",
      "Jan Niehues, Eunah Cho, Thanh-Le Ha, and Alex\n",
      "Waibel. 2016. Pre-translation for neural machine\n",
      "translation. In Proceedings of COLING 2016, the\n",
      "26th International Conference on Computational\n",
      "Linguistics: Technical Papers , pages 1828–1836,\n",
      "Osaka, Japan. The COLING 2016 Organizing Com-\n",
      "mittee.\n",
      "Farhad Nooralahzadeh, Giannis Bekoulis, Johannes\n",
      "Bjerva, and Isabelle Augenstein. 2020. Zero-shot\n",
      "cross-lingual transfer with meta learning. arXiv\n",
      "preprint arXiv:2003.02739 .\n",
      "Atul Kr. Ojha, Valentin Malykh, Alina Karakanta, and\n",
      "Chao-Hong Liu. 2020. Findings of the LoResMT\n",
      "2020 shared task on zero-shot for low-resource lan-\n",
      "guages. In Proceedings of the 3rd Workshop on\n",
      "Technologies for MT of Low Resource Languages ,\n",
      "pages 33–37, Suzhou, China. Association for Com-\n",
      "putational Linguistics.\n",
      "Matthew Olson, Abraham Wyner, and Richard Berk.\n",
      "2018. Modern neural networks generalize on small\n",
      "data sets. In Advances in Neural Information Pro-\n",
      "cessing Systems , pages 3619–3628.\n",
      "Arturo Oncevay. 2021. Peru is multilingual, its ma-\n",
      "chine translation should be too? In Proceedings\n",
      "of the First Workshop on Natural Language Pro-\n",
      "cessing for Indigenous Languages of the Americas ,\n",
      "pages 194–201, Online. Association for Computa-\n",
      "tional Linguistics.Iroro Orife, Julia Kreutzer, Blessing Sibanda, Daniel\n",
      "Whitenack, Kathleen Siminyu, Laura Martinus,\n",
      "Jamiil Toure Ali, Jade Abbott, Vukosi Marivate,\n",
      "Salomon Kabongo, et al. 2020. Masakhane–\n",
      "machine translation for africa. arXiv preprint\n",
      "arXiv:2003.11529 .\n",
      "John E Ortega, Richard Castro Mamani, and\n",
      "Kyunghyun Cho. 2020a. Neural machine translation\n",
      "with a polysynthetic low resource language. Ma-\n",
      "chine Translation , 34(4):325–346.\n",
      "John E Ortega, Richard Alexander Castro-Mamani, and\n",
      "Jaime Rafael Montoya Samame. 2020b. Overcom-\n",
      "ing resistance: The normalization of an Amazonian\n",
      "tribal language. In Proceedings of the 3rd Work-\n",
      "shop on Technologies for MT of Low Resource Lan-\n",
      "guages , pages 1–13, Suzhou, China. Association for\n",
      "Computational Linguistics.\n",
      "Yirong Pan, Xiao Li, Yating Yang, and Rui Dong. 2020.\n",
      "Multi-task neural model for agglutinative language\n",
      "translation. In Proceedings of the 58th Annual Meet-\n",
      "ing of the Association for Computational Linguis-\n",
      "tics: Student Research Workshop , pages 103–110,\n",
      "Online. Association for Computational Linguistics.\n",
      "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-\n",
      "Jing Zhu. 2002. Bleu: a method for automatic eval-\n",
      "uation of machine translation. In Proceedings of the\n",
      "40th Annual Meeting of the Association for Com-\n",
      "putational Linguistics , pages 311–318, Philadelphia,\n",
      "Pennsylvania, USA. Association for Computational\n",
      "Linguistics.\n",
      "Shantipriya Parida, Subhadarshi Panda, Amulya Dash,\n",
      "Esau Villatoro-Tello, A. Seza Do ˘gruöz, Rosa M.\n",
      "Ortega-Mendoza, Amadeo Hernández, Yashvardhan\n",
      "Sharma, and Petr Motlicek. 2021. Open machine\n",
      "translation for low resource South American lan-\n",
      "guages (AmericasNLP 2021 shared task contribu-\n",
      "tion). In Proceedings of the First Workshop on Natu-\n",
      "ral Language Processing for Indigenous Languages\n",
      "of the Americas , pages 218–223, Online. Associa-\n",
      "tion for Computational Linguistics.\n",
      "Jeffrey Pennington, Richard Socher, and Christopher\n",
      "Manning. 2014. GloVe: Global vectors for word\n",
      "representation. In Proceedings of the 2014 Con-\n",
      "ference on Empirical Methods in Natural Language\n",
      "Processing (EMNLP) , pages 1532–1543, Doha,\n",
      "Qatar. Association for Computational Linguistics.\n",
      "Asya Pereltsvaig. 2020. Languages of the World .\n",
      "Cambridge University Press.\n",
      "Matthew Peters, Mark Neumann, Mohit Iyyer, Matt\n",
      "Gardner, Christopher Clark, Kenton Lee, and Luke\n",
      "Zettlemoyer. 2018. Deep contextualized word rep-\n",
      "resentations. In Proceedings of the 2018 Confer-\n",
      "ence of the North American Chapter of the Associ-\n",
      "ation for Computational Linguistics: Human Lan-\n",
      "guage Technologies, Volume 1 (Long Papers) , pages\n",
      "2227–2237.Alberto Poncelas, Maja Popovi ´c, Dimitar Shterionov,\n",
      "Gideon Maillette de Buy Wenniger, and Andy Way.\n",
      "2019. Combining pbsmt and nmt back-translated\n",
      "data for efficient nmt. In Proceedings of the Inter-\n",
      "national Conference on Recent Advances in Natural\n",
      "Language Processing (RANLP 2019) , pages 922–\n",
      "931.\n",
      "Maja Popovi ´c. 2017. chrf++: words helping character\n",
      "n-grams. In Proceedings of the second conference\n",
      "on machine translation , pages 612–618.\n",
      "Nima Pourdamghani and Kevin Knight. 2019. Neigh-\n",
      "bors helping the poor: improving low-resource ma-\n",
      "chine translation using related languages. Machine\n",
      "Translation , 33(3):239–258.\n",
      "Ofir Press and Lior Wolf. 2017. Using the output em-\n",
      "bedding to improve language models. In Proceed-\n",
      "ings of the 15th Conference of the European Chap-\n",
      "ter of the Association for Computational Linguistics:\n",
      "Volume 2, Short Papers , pages 157–163.\n",
      "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\n",
      "Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\n",
      "Wei Li, and Peter J Liu. 2020. Exploring the limits\n",
      "of transfer learning with a unified text-to-text trans-\n",
      "former. Journal of Machine Learning Research ,\n",
      "21:1–67.\n",
      "Alessandro Raganato, Raúl Vázquez, Mathias Creutz,\n",
      "and Jörg Tiedemann. 2021. An empirical investi-\n",
      "gation of word alignment supervision for zero-shot\n",
      "multilingual neural machine translation. In Pro-\n",
      "ceedings of the 2021 Conference on Empirical Meth-\n",
      "ods in Natural Language Processing , pages 8449–\n",
      "8456, Online and Punta Cana, Dominican Republic.\n",
      "Association for Computational Linguistics.\n",
      "Surangika Ranathunga, En-Shiun Annie Lee, Mar-\n",
      "jana Prifti Skenduli, Ravi Shekhar, Mehreen Alam,\n",
      "and Rishemjit Kaur. 2021. Neural machine trans-\n",
      "lation for low-resource languages: A survey. arXiv\n",
      "preprint arXiv:2106.15115 .\n",
      "Shuo Ren, Yu Wu, Shujie Liu, Ming Zhou, and\n",
      "Shuai Ma. 2020. A retrieve-and-rewrite initializa-\n",
      "tion method for unsupervised machine translation.\n",
      "InProceedings of the 58th Annual Meeting of the\n",
      "Association for Computational Linguistics , pages\n",
      "3498–3504, Online. Association for Computational\n",
      "Linguistics.\n",
      "Parker Riley, Isaac Caswell, Markus Freitag, and David\n",
      "Grangier. 2020. Translationese as a language in\n",
      "“multilingual” NMT. In Proceedings of the 58th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics , pages 7737–7746, Online. Association\n",
      "for Computational Linguistics.\n",
      "Christian Roest, Lukas Edman, Gosse Minnema, Kevin\n",
      "Kelly, Jennifer Spenader, and Antonio Toral. 2020.\n",
      "Machine translation for English–Inuktitut with seg-\n",
      "mentation, data acquisition and pre-training. In\n",
      "Proceedings of the Fifth Conference on MachineTranslation , pages 274–281, Online. Association for\n",
      "Computational Linguistics.\n",
      "David Rolnick, Andreas Veit, Serge Belongie, and Nir\n",
      "Shavit. 2017. Deep learning is robust to massive la-\n",
      "bel noise. arXiv preprint arXiv:1705.10694 .\n",
      "Carlos Barron Romero, Jesús Manuel Mager Hois, and\n",
      "Fernando Reyes Avilés. 2016. Richard feynman, los\n",
      "alfabetos y los lenguajes. Relingüística aplicada ,\n",
      "(19):2.\n",
      "Mónica Jasso Rosales, Manuel Mager, and Ivan\n",
      "Vladimir Meza Ruız. Towards a twitter corpus of\n",
      "the indigenous languages of the americas.\n",
      "Devendra Singh Sachan and Graham Neubig. 2018.\n",
      "Parameter sharing methods for multilingual self-\n",
      "attentional translation models. arXiv preprint\n",
      "arXiv:1809.00252 .\n",
      "Elizabeth Salesky, David Etter, and Matt Post. 2021.\n",
      "Robust open-vocabulary translation from visual text\n",
      "representations. arXiv preprint arXiv:2104.08211 .\n",
      "Jonne Saleva and Constantine Lignos. 2021. The effec-\n",
      "tiveness of morphology-aware segmentation in low-\n",
      "resource neural machine translation. In Proceedings\n",
      "of the 16th Conference of the European Chapter of\n",
      "the Association for Computational Linguistics: Stu-\n",
      "dent Research Workshop , pages 164–174, Online.\n",
      "Association for Computational Linguistics.\n",
      "Motoki Sano, Jun Suzuki, and Shun Kiyono. 2019. Ef-\n",
      "fective adversarial regularization for neural machine\n",
      "translation. In Proceedings of the 57th Annual Meet-\n",
      "ing of the Association for Computational Linguis-\n",
      "tics, pages 204–210.\n",
      "Yves Scherrer, Stig-Arne Grönroos, and Sami Virpi-\n",
      "oja. 2020. The University of Helsinki and aalto\n",
      "university submissions to the WMT 2020 news and\n",
      "low-resource translation tasks. In Proceedings of\n",
      "the Fifth Conference on Machine Translation , pages\n",
      "1129–1138, Online. Association for Computational\n",
      "Linguistics.\n",
      "Lane Schwartz. 2022. Primum non nocere: Before\n",
      "working with indigenous data, the acl must confront\n",
      "ongoing colonialism. In Proceedings of the 60th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics (Volume 2: Short Papers) , pages 724–\n",
      "731.\n",
      "Lane Schwartz, Francis Tyers, Lori Levin, Christo\n",
      "Kirov, Patrick Littell, Chi-kiu Lo, Emily\n",
      "Prud’hommeaux, Hyunji Hayley Park, Ken-\n",
      "neth Steimel, Rebecca Knowles, et al. 2020. Neural\n",
      "polysynthetic language modelling. arXiv preprint\n",
      "arXiv:2005.05477 .\n",
      "Lee Sechrest, Todd L Fay, and SM Hafeez Zaidi. 1972.\n",
      "Problems of translation in cross-cultural research.\n",
      "Journal of cross-cultural psychology , 3(1):41–56.Rico Sennrich, Barry Haddow, and Alexandra Birch.\n",
      "2016a. Improving neural machine translation mod-\n",
      "els with monolingual data. In Proceedings of the\n",
      "54th Annual Meeting of the Association for Compu-\n",
      "tational Linguistics (Volume 1: Long Papers) , pages\n",
      "86–96, Berlin, Germany. Association for Computa-\n",
      "tional Linguistics.\n",
      "Rico Sennrich, Barry Haddow, and Alexandra Birch.\n",
      "2016b. Improving neural machine translation mod-\n",
      "els with monolingual data. In Proceedings of the\n",
      "54th Annual Meeting of the Association for Compu-\n",
      "tational Linguistics (Volume 1: Long Papers) , pages\n",
      "86–96.\n",
      "Rico Sennrich, Barry Haddow, and Alexandra Birch.\n",
      "2016c. Neural machine translation of rare words\n",
      "with subword units. In Proceedings of the 54th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics (Volume 1: Long Papers) , pages 1715–\n",
      "1725, Berlin, Germany. Association for Computa-\n",
      "tional Linguistics.\n",
      "Joan Serra, Didac Suris, Marius Miron, and Alexan-\n",
      "dros Karatzoglou. 2018. Overcoming catastrophic\n",
      "forgetting with hard attention to the task. In In-\n",
      "ternational Conference on Machine Learning , pages\n",
      "4548–4557. PMLR.\n",
      "Aditya Siddhant, Ankur Bapna, Yuan Cao, Orhan Fi-\n",
      "rat, Mia Chen, Sneha Kudugunta, Naveen Arivazha-\n",
      "gan, and Yonghui Wu. 2020. Leveraging mono-\n",
      "lingual data with self-supervision for multilingual\n",
      "neural machine translation. In Proceedings of the\n",
      "58th Annual Meeting of the Association for Compu-\n",
      "tational Linguistics , pages 2827–2835, Online. As-\n",
      "sociation for Computational Linguistics.\n",
      "Gerardo Sierra Martínez, Cynthia Montaño, Gemma\n",
      "Bel-Enguix, Diego Córdova, and Margarita\n",
      "Mota Montoya. 2020. CPLM, a parallel corpus for\n",
      "Mexican languages: Development and interface.\n",
      "InProceedings of the 12th Language Resources\n",
      "and Evaluation Conference , pages 2947–2952,\n",
      "Marseille, France. European Language Resources\n",
      "Association.\n",
      "Gary F Simons and M Paul Lewis. 2013. The world’s\n",
      "languages in crisis. Responses to language endan-\n",
      "germent: In honor of Mickey Noonan. New direc-\n",
      "tions in language documentation and language revi-\n",
      "talization , 3:20.\n",
      "Peter Smit, Sami Virpioja, Stig-Arne Grönroos, Mikko\n",
      "Kurimo, et al. 2014. Morfessor 2.0: Toolkit for sta-\n",
      "tistical morphological segmentation. In The 14th\n",
      "Conference of the European Chapter of the Associa-\n",
      "tion for Computational Linguistics (EACL), Gothen-\n",
      "burg, Sweden, April 26-30, 2014 . Aalto University.\n",
      "Anders Søgaard, Sebastian Ruder, and Ivan Vuli ´c.\n",
      "2018. On the limitations of unsupervised bilingual\n",
      "dictionary induction. In Proceedings of the 56th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics (Volume 1: Long Papers) , pages 778–\n",
      "788.Haiyue Song, Raj Dabre, Zhuoyuan Mao, Fei Cheng,\n",
      "Sadao Kurohashi, and Eiichiro Sumita. 2020. Pre-\n",
      "training via leveraging assisting languages and data\n",
      "selection for neural machine translation. arXiv\n",
      "preprint arXiv:2001.08353 .\n",
      "Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\n",
      "Yan Liu. 2019. Mass: Masked sequence to se-\n",
      "quence pre-training for language generation. In In-\n",
      "ternational Conference on Machine Learning , pages\n",
      "5926–5936.\n",
      "Xabier Soto, Dimitar Shterionov, Alberto Poncelas,\n",
      "and Andy Way. 2020. Selecting backtranslated data\n",
      "from multiple sources for improved neural machine\n",
      "translation. In Proceedings of the 58th Annual\n",
      "Meeting of the Association for Computational Lin-\n",
      "guistics , pages 3898–3908, Online. Association for\n",
      "Computational Linguistics.\n",
      "Tejas Srinivasan, Ramon Sanabria, and Florian Metze.\n",
      "2019. Multitask learning for different subword seg-\n",
      "mentations in neural machine translation. arXiv\n",
      "preprint arXiv:1910.12368 .\n",
      "Dario Stojanovski, Viktor Hangya, Matthias Huck, and\n",
      "Alexander Fraser. 2019. The lmu munich unsuper-\n",
      "vised machine translation system for wmt19. In\n",
      "Proceedings of the Fourth Conference on Machine\n",
      "Translation (Volume 2: Shared Task Papers, Day 1) ,\n",
      "pages 393–399.\n",
      "Haipeng Sun, Rui Wang, Kehai Chen, Masao Utiyama,\n",
      "Eiichiro Sumita, and Tiejun Zhao. 2020. Robust un-\n",
      "supervised neural machine translation with adversar-\n",
      "ial training. arXiv preprint arXiv:2002.12549 .\n",
      "Xu Tan, Yichong Leng, Jiale Chen, Yi Ren, Tao\n",
      "Qin, and Tie-Yan Liu. 2019. A study of multi-\n",
      "lingual neural machine translation. arXiv preprint\n",
      "arXiv:1912.11625 .\n",
      "Sarah G Thomason. 2015. Endangered languages .\n",
      "Cambridge University Press.\n",
      "Jörg Tiedemann. 2016. Opus–parallel corpora for ev-\n",
      "eryone. Baltic Journal of Modern Computing , page\n",
      "384.\n",
      "Jörg Tiedemann. 2018. Emerging language spaces\n",
      "learned from massively multilingual corpora. arXiv\n",
      "preprint arXiv:1802.00273 .\n",
      "Atnafu Lambebo Tonja, Christian Maldonado-\n",
      "Sifuentes, David Alejandro Mendoza Castillo, Olga\n",
      "Kolesnikova, Noé Castro-Sánchez, Grigori Sidorov,\n",
      "and Alexander Gelbukh. 2023. Parallel corpus\n",
      "for indigenous language translation: Spanish-\n",
      "mazatec and spanish-mixtec. arXiv preprint\n",
      "arXiv:2305.17404 .\n",
      "Antonio Toral, Sheila Castilho, Ke Hu, and Andy\n",
      "Way. 2018. Attaining the unattainable? reassess-\n",
      "ing claims of human parity in neural machine trans-\n",
      "lation. In Proceedings of the Third Conference on\n",
      "Machine Translation: Research Papers , pages 113–\n",
      "123.Hai-Long Trieu, Duc-Vu Tran, Ashwin Ittoo, and Le-\n",
      "Minh Nguyen. 2019. Leveraging additional re-\n",
      "sources for improving statistical machine translation\n",
      "on asian low-resource languages. ACM Trans. Asian\n",
      "Low-Resour. Lang. Inf. Process. , 18(3).\n",
      "Masao Utiyama and Hitoshi Isahara. 2007. A com-\n",
      "parison of pivot methods for phrase-based statistical\n",
      "machine translation. In Human Language Technolo-\n",
      "gies 2007: The Conference of the North American\n",
      "Chapter of the Association for Computational Lin-\n",
      "guistics; Proceedings of the Main Conference , pages\n",
      "484–491.\n",
      "Clara Vania and Adam Lopez. 2017. From characters\n",
      "to words to in between: Do we capture morphol-\n",
      "ogy? In Proceedings of the 55th Annual Meeting of\n",
      "the Association for Computational Linguistics (Vol-\n",
      "ume 1: Long Papers) , pages 2016–2027, Vancouver,\n",
      "Canada. Association for Computational Linguistics.\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\n",
      "Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\n",
      "Kaiser, and Illia Polosukhin. 2017. Attention is all\n",
      "you need. In Advances in neural information pro-\n",
      "cessing systems , pages 5998–6008.\n",
      "Raúl Vázquez, Yves Scherrer, Sami Virpioja, and Jörg\n",
      "Tiedemann. 2021. The Helsinki submission to the\n",
      "AmericasNLP shared task. In Proceedings of the\n",
      "First Workshop on Natural Language Processing for\n",
      "Indigenous Languages of the Americas , pages 255–\n",
      "264, Online. Association for Computational Lin-\n",
      "guistics.\n",
      "Ivan Vuli ´c, Goran Glavaš, Roi Reichart, and Anna Ko-\n",
      "rhonen. 2019. Do we really need fully unsuper-\n",
      "vised cross-lingual embeddings? In Proceedings of\n",
      "the 2019 Conference on Empirical Methods in Nat-\n",
      "ural Language Processing and the 9th International\n",
      "Joint Conference on Natural Language Processing\n",
      "(EMNLP-IJCNLP) , pages 4407–4418, Hong Kong,\n",
      "China. Association for Computational Linguistics.\n",
      "Ivan Vuli ´c, Sebastian Ruder, and Anders Søgaard.\n",
      "2020. Are all good word vector spaces isomorphic?\n",
      "arXiv preprint arXiv:2004.04070 .\n",
      "Liang Wang, Wei Zhao, Ruoyu Jia, Sujian Li, and\n",
      "Jingming Liu. 2019a. Denoising based sequence-\n",
      "to-sequence pre-training for text generation. In Pro-\n",
      "ceedings of the 2019 Conference on Empirical Meth-\n",
      "ods in Natural Language Processing and the 9th In-\n",
      "ternational Joint Conference on Natural Language\n",
      "Processing (EMNLP-IJCNLP) , pages 3994–4006.\n",
      "Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu,\n",
      "Changliang Li, Derek F. Wong, and Lidia S. Chao.\n",
      "2019b. Learning deep transformer models for ma-\n",
      "chine translation. In Proceedings of the 57th Annual\n",
      "Meeting of the Association for Computational Lin-\n",
      "guistics , pages 1810–1822, Florence, Italy. Associa-\n",
      "tion for Computational Linguistics.Rui Wang, Xu Tan, Renqian Luo, Tao Qin, and Tie-\n",
      "Yan Liu. 2021. A survey on low-resource neural\n",
      "machine translation. In Proceedings of the Thirtieth\n",
      "International Joint Conference on Artificial Intel-\n",
      "ligence, IJCAI-21 , pages 4636–4643. International\n",
      "Joint Conferences on Artificial Intelligence Organi-\n",
      "zation. Survey Track.\n",
      "Xinyi Wang, Hieu Pham, Philip Arthur, and Gra-\n",
      "ham Neubig. 2019c. Multilingual neural machine\n",
      "translation with soft decoupled encoding. In Inter-\n",
      "national Conference on Learning Representations\n",
      "(ICLR) , New Orleans, LA, USA.\n",
      "Xinyi Wang, Yulia Tsvetkov, and Graham Neubig.\n",
      "2020. Balancing training for multilingual neural\n",
      "machine translation. In Proceedings of the 58th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics , pages 8526–8537, Online. Association\n",
      "for Computational Linguistics.\n",
      "Karl Weiss, Taghi M Khoshgoftaar, and DingDing\n",
      "Wang. 2016. A survey of transfer learning. Jour-\n",
      "nal of Big Data , 3(1):9.\n",
      "John Wieting, Taylor Berg-Kirkpatrick, Kevin Gimpel,\n",
      "and Graham Neubig. 2019. Beyond bleu: Train-\n",
      "ing neural machine translation with semantic sim-\n",
      "ilarity. In Proceedings of the 57th Annual Meet-\n",
      "ing of the Association for Computational Linguis-\n",
      "tics, pages 4344–4355.\n",
      "Hua Wu and Haifeng Wang. 2007. Pivot language ap-\n",
      "proach for phrase-based statistical machine transla-\n",
      "tion. Machine Translation , 21(3):165–181.\n",
      "Xing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han,\n",
      "and Songlin Hu. 2019. Conditional bert contextual\n",
      "augmentation. In International Conference on Com-\n",
      "putational Science , pages 84–95. Springer.\n",
      "Haoran Xu, Benjamin Van Durme, and Kenton Murray.\n",
      "2021. BERT, mBERT, or BiBERT? a study on con-\n",
      "textualized embeddings for neural machine transla-\n",
      "tion. In Proceedings of the 2021 Conference on Em-\n",
      "pirical Methods in Natural Language Processing ,\n",
      "pages 6663–6675, Online and Punta Cana, Domini-\n",
      "can Republic. Association for Computational Lin-\n",
      "guistics.\n",
      "Linting Xue, Noah Constant, Adam Roberts, Mi-\n",
      "hir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\n",
      "Barua, and Colin Raffel. 2021. mt5: A massively\n",
      "multilingual pre-trained text-to-text transformer. In\n",
      "Proceedings of the 2021 Conference of the North\n",
      "American Chapter of the Association for Computa-\n",
      "tional Linguistics: Human Language Technologies ,\n",
      "pages 483–498.\n",
      "Delfino Zacarías Márquez and Ivan Vladimir\n",
      "Meza Ruiz. 2021. Ayuuk-Spanish neural ma-\n",
      "chine translator. In Proceedings of the First\n",
      "Workshop on Natural Language Processing for\n",
      "Indigenous Languages of the Americas , pages\n",
      "168–172, Online. Association for Computational\n",
      "Linguistics.Lenka Zajícová. 2017. Lenguas indígenas en\n",
      "la legislación de los países hispanoamericanos.\n",
      "Onomázein , (NE III):171–203.\n",
      "Poorya Zaremoodi, Wray Buntine, and Gholamreza\n",
      "Haffari. 2018. Adaptive knowledge sharing in\n",
      "multi-task learning: Improving low-resource neural\n",
      "machine translation. In Proceedings of the 56th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics (Volume 2: Short Papers) , pages 656–\n",
      "661.\n",
      "Rodolfo Zevallos, John Ortega, William Chen, Richard\n",
      "Castro, Núria Bel, Cesar Toshio, Renzo Venturas,\n",
      "Aradiel, and Hilario Nelsi Melgarejo. 2022. Intro-\n",
      "ducing QuBERT: A large monolingual corpus and\n",
      "BERT model for Southern Quechua. In Proceed-\n",
      "ings of the Third Workshop on Deep Learning for\n",
      "Low-Resource Natural Language Processing , pages\n",
      "1–13, Hybrid. Association for Computational Lin-\n",
      "guistics.\n",
      "Biao Zhang, Philip Williams, Ivan Titov, and Rico\n",
      "Sennrich. 2020a. Improving massively multilingual\n",
      "neural machine translation and zero-shot translation.\n",
      "arXiv preprint arXiv:2004.11867 .\n",
      "Biao Zhang, Philip Williams, Ivan Titov, and Rico\n",
      "Sennrich. 2020b. Improving massively multilingual\n",
      "neural machine translation and zero-shot translation.\n",
      "InProceedings of the 58th Annual Meeting of the\n",
      "Association for Computational Linguistics , pages\n",
      "1628–1639, Online. Association for Computational\n",
      "Linguistics.\n",
      "Shiyue Zhang, Ben Frey, and Mohit Bansal. 2022.\n",
      "How can nlp help revitalize endangered languages?\n",
      "a case study and roadmap for the cherokee language.\n",
      "arXiv preprint arXiv:2204.11909 .\n",
      "Shiyue Zhang, Benjamin Frey, and Mohit Bansal.\n",
      "2020c. ChrEn: Cherokee-English machine transla-\n",
      "tion for endangered language revitalization. In Pro-\n",
      "ceedings of the 2020 Conference on Empirical Meth-\n",
      "ods in Natural Language Processing (EMNLP) ,\n",
      "pages 577–595, Online. Association for Computa-\n",
      "tional Linguistics.\n",
      "Shiyue Zhang, Benjamin Frey, and Mohit Bansal.\n",
      "2021. ChrEnTranslate: Cherokee-English machine\n",
      "translation demo with quality estimation and correc-\n",
      "tive feedback. In Proceedings of the 59th Annual\n",
      "Meeting of the Association for Computational Lin-\n",
      "guistics and the 11th International Joint Conference\n",
      "on Natural Language Processing: System Demon-\n",
      "strations , pages 272–279, Online. Association for\n",
      "Computational Linguistics.\n",
      "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\n",
      "Weinberger, and Yoav Artzi. 2020d. Bertscore:\n",
      "Evaluating text generation with bert. In ICLR .\n",
      "Yuhao Zhang, Ziyang Wang, Runzhe Cao, Binghao\n",
      "Wei, Weiqiao Shan, Shuhan Zhou, Abudurexiti Re-\n",
      "heman, Tao Zhou, Xin Zeng, Laohu Wang, YongyuMu, Jingnan Zhang, Xiaoqian Liu, Xuanjun Zhou,\n",
      "Yinqiao Li, Bei Li, Tong Xiao, and Jingbo Zhu.\n",
      "2020e. The NiuTrans machine translation systems\n",
      "for WMT20. In Proceedings of the Fifth Confer-\n",
      "ence on Machine Translation , pages 338–345, On-\n",
      "line. Association for Computational Linguistics.\n",
      "Zhuosheng Zhang, Yafang Huang, and Hai Zhao. 2019.\n",
      "Open vocabulary learning for neural chinese pinyin\n",
      "ime. In Proceedings of the 57th Annual Meeting\n",
      "of the Association for Computational Linguistics ,\n",
      "pages 1584–1594.\n",
      "Francis Zheng, Machel Reid, Edison Marrese-Taylor,\n",
      "and Yutaka Matsuo. 2021. Low-resource machine\n",
      "translation using cross-lingual language model pre-\n",
      "training. In Proceedings of the First Workshop on\n",
      "Natural Language Processing for Indigenous Lan-\n",
      "guages of the Americas , pages 234–240, Online. As-\n",
      "sociation for Computational Linguistics.\n",
      "Hao Zheng, Yong Cheng, and Yang Liu. 2017.\n",
      "Maximum expected likelihood estimation for zero-\n",
      "resource neural machine translation. In IJCAI ,\n",
      "pages 4251–4257.\n",
      "Shuyan Zhou, Xiangkai Zeng, Yingqi Zhou, Antonios\n",
      "Anastasopoulos, and Graham Neubig. 2019. Im-\n",
      "proving robustness of neural machine translation\n",
      "with multi-task learning. In Proceedings of the\n",
      "Fourth Conference on Machine Translation (Volume\n",
      "2: Shared Task Papers, Day 1) , pages 565–571, Flo-\n",
      "rence, Italy. Association for Computational Linguis-\n",
      "tics.\n",
      "Changfeng Zhu, Heng Yu, Shanbo Cheng, and Weihua\n",
      "Luo. 2020a. Language-aware interlingua for multi-\n",
      "lingual neural machine translation. In Proceedings\n",
      "of the 58th Annual Meeting of the Association for\n",
      "Computational Linguistics , pages 1650–1655, On-\n",
      "line. Association for Computational Linguistics.\n",
      "Jinhua Zhu, Fei Gao, Lijun Wu, Yingce Xia, Tao\n",
      "Qin, Wengang Zhou, Xueqi Cheng, and Tie-Yan\n",
      "Liu. 2019. Soft contextual data augmentation\n",
      "for neural machine translation. arXiv preprint\n",
      "arXiv:1905.10523 .\n",
      "Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,\n",
      "Wengang Zhou, Houqiang Li, and Tie-Yan Liu.\n",
      "2020b. Incorporating bert into neural machine trans-\n",
      "lation. arXiv preprint arXiv:2002.06823 .\n",
      "Barret Zoph, Deniz Yuret, Jonathan May, and\n",
      "Kevin Knight. 2016. Transfer learning for low-\n",
      "resource neural machine translation. arXiv preprint\n",
      "arXiv:1604.02201 .A Appendix\n",
      "In this appendix we expand the information re-\n",
      "garding current work on MT for LRL.\n",
      "A.1 Expanded LR work on Multilingual\n",
      "supervised training\n",
      "Arivazhagan et al. (2019a) introduce a represen-\n",
      "tational invariance training objective across lan-\n",
      "guages that achieves comparable results with piv-\n",
      "oting methods. Promising results of multilingual\n",
      "models have encouraged experiments with models\n",
      "trained on a massive amount of language pairs, re-\n",
      "sulting in large multilingual models: Aharoni et al.\n",
      "(2019) train a single model on 102 languages to\n",
      "and from English in contrast to the 58 languages\n",
      "used by Neubig and Hu (2018).\n",
      "The negative aspect of this approach is the size\n",
      "of the network. Arivazhagan et al. (2019b) per-\n",
      "form an extensive study on 102 language pairs\n",
      "to explore different settings and training setups\n",
      "and achieve good results for LRLs, while main-\n",
      "taining good performance for high-resource lan-\n",
      "guages. Related massively multilingual NMT\n",
      "systems have been trained for analytic proposes\n",
      "(Tiedemann, 2018; Malaviya et al., 2017) and\n",
      "general zero-shot transfer learning (Artetxe and\n",
      "Schwenk, 2019). mRASP (Lin et al., 2020) use\n",
      "for pretraining of the multilingual model and add\n",
      "a randomly aligned substitution loss that aims to\n",
      "bring words and phrases closer in the cross-lingual\n",
      "space.\n",
      "Zhang et al. (2020a) explores the main problems\n",
      "that arise for such models: multilingual NMT usu-\n",
      "ally underperforms bilingual models (Arivazha-\n",
      "gan et al., 2019b), the larger the number of lan-\n",
      "guages gets the more the performance drops (Aha-\n",
      "roni et al., 2019), languages in datasets used for\n",
      "multilingual training are unbalanced in size, and\n",
      "poor zero-shot performance compared to pivot\n",
      "models (cf. §6.3). Zhang et al. (2020a) ad-\n",
      "dresses these problems with a language-aware in-\n",
      "put layer, a deep transformer architecture (Wang\n",
      "et al., 2019b), and an online back-translation\n",
      "approach. These modifications boost zero-shot\n",
      "translation performance for multilingual models.\n",
      "To improve the problem of imbalanced and lin-\n",
      "guistically diverse training data, mostly heuristic\n",
      "methods have been proposed: Arivazhagan et al.\n",
      "(2019b) samples training data from different lan-\n",
      "guages based on a data size scaled by temperature\n",
      "term. These heuristics have an impact on perfor-mance, and ignore other factors that are not size.\n",
      "Oversampling of data is used by Johnson et al.\n",
      "(2017); Neubig and Hu (2018); Conneau and Lam-\n",
      "ple (2019). Wang et al. (2020) proposes a differ-\n",
      "entiable data selection method that automatically\n",
      "learns to weight training data, optimizing transla-\n",
      "tion on all languages.\n",
      "Multilingual modeling Sharing all parameters\n",
      "except for the attention mechanism shows im-\n",
      "provements compared with sharing everything in\n",
      "an RNN NMT model (Blackwood et al., 2018).\n",
      "Sachan and Neubig (2018) explores parameter\n",
      "sharing in the transformer architecture for the de-\n",
      "coder in the one-to-many translation setting and\n",
      "shows that transformers are more suitable than\n",
      "RNNs for this task. Also, parameter sharing in\n",
      "the decoder and embedding layer further improves\n",
      "performance. Lu et al. (2018) proposes a shared\n",
      "layer intended to capture the interlingua knowl-\n",
      "edge and an extension to the typical RNN network\n",
      "with multiple blocks along with a trainable routing\n",
      "network. The routing network enables adaptive\n",
      "collaboration by dynamic sharing of blocks condi-\n",
      "tioned on the task at hand, input, and model state\n",
      "(Zaremoodi et al., 2018). Zhang et al. (2020a) pro-\n",
      "poses a language-aware layer to improve such ar-\n",
      "chitectures further. With a similar idea, Zhu et al.\n",
      "(2020a) incorporates two special language embed-\n",
      "dings into the self-attention mechanism. The first\n",
      "encodes the unique characteristics of each lan-\n",
      "guage, while the second captures common seman-\n",
      "tics across languages.\n",
      "One problem in multilingual NMT systems is\n",
      "the translation into the wrong language. To ad-\n",
      "dress this problem, Zhang et al. (2020b) add\n",
      "a language-aware layer normalization and a lin-\n",
      "ear transformation that is inserted between the\n",
      "encoder and the decoder to induce a language-\n",
      "specific translation. Raganato et al. (2021) explore\n",
      "to weight the target language label with jointly\n",
      "training one cross attention head with word align-\n",
      "ments.\n",
      "Other modifications of NMT model archi-\n",
      "tectures to improve their performance on low-\n",
      "resource languages include: deep RNNs (Miceli-\n",
      "Barone et al., 2017), normalization layers (Ba\n",
      "et al., 2016), direct lexical connections (Nguyen\n",
      "et al., 2015), word embedding layers conducive to\n",
      "lexical sharing (Wang et al., 2019c).A.2 Extended Multi-task training\n",
      "Zhou et al. (2019) uses this approach, but extends\n",
      "it with a cascade architecture: the first decoder\n",
      "reads the encoder, and the second decoder reads\n",
      "the encoder and the first decoder (Niehues et al.,\n",
      "2016; Anastasopoulos and Chiang, 2018). The\n",
      "auxiliary task (first decoder) is a denoising de-\n",
      "coder. With RNN NMT architectures, one can\n",
      "further decide if the attention mechanism should\n",
      "be shared among tasks (Niehues and Cho, 2017).\n",
      "The authors compare all architectures and find that\n",
      "they perform similarly, with only sharing the en-\n",
      "coder being slightly better.\n",
      "Using linguistic information as an auxiliary task\n",
      "has not yet been explored exhaustively. Niehues\n",
      "and Cho (2017) studies the usage of part-of-speech\n",
      "(POS) and named entity (NE) tags, finding that\n",
      "training on named entity recognition (NER), POS\n",
      "tagging and MT together improves performance\n",
      "the most. For agglutinative languages, morpho-\n",
      "logical auxiliary tasks can be beneficial: Pan et al.\n",
      "(2020) uses stemming with fully shared parame-\n",
      "ters.\n",
      "As an alternative to linguistically informed aux-\n",
      "iliary tasks Srinivasan et al. (2019) uses multiple\n",
      "BPE vocabulary sizes to generate different seg-\n",
      "mentations. Each segmentation is treated as an in-\n",
      "dividual task.\n",
      "A.3 Data augmentation\n",
      "Back-translation Caswell et al. (2019) shows\n",
      "that adding a special tag to the synthetic data im-\n",
      "proves performance. A technique that exploits this\n",
      "idea is training an initial translation model with\n",
      "synthetic data generated via BT and then finetune\n",
      "it with gold data (Abdulmumin et al., 2019). This\n",
      "simple yet effective training algorithm improves\n",
      "NMT for LRLs; however, it can also degrade per-\n",
      "formance on HRLs if trained without a tagging\n",
      "strategy (Marie et al., 2020).\n",
      "Multiple improvements of BT have been pro-\n",
      "posed. Edunov et al. (2018) shows that sampling\n",
      "or noisy beam search can generate more effective\n",
      "pseudo-parallel data. However, for LRLs an op-\n",
      "timal beam search and greedy decoding are bet-\n",
      "ter. A factor that influences BT’s effectiveness\n",
      "is the quality of the initial MT systems (Hoang\n",
      "et al., 2018). Using back-translated data from mul-\n",
      "tiple sources (Poncelas et al., 2019) or optimizing\n",
      "the ranking of back-translated data yields further\n",
      "gains (Soto et al., 2020).BT results in gains when the parallel corpora are\n",
      "naturally occurring text and not translationese, as\n",
      "the latter would only improve automatic n metrics\n",
      "(Toral et al., 2018; Graham et al., 2020). ?shows\n",
      "that BT produces more fluent text and is preferred\n",
      "by humans. Additionally, translationese and origi-\n",
      "nal data can be modeled as separate languages in a\n",
      "multilingual model (Riley et al., 2020). BT is also\n",
      "a central part of unsupervised MT (UMT; cf. §6.4)\n",
      "and zero-shot MT (Gu et al., 2019).\n",
      "Sentence modification Zhu et al. (2019) pro-\n",
      "poses to replace a randomly chosen word in a sen-\n",
      "tence with a soft-word . That means that, instead\n",
      "of sampling a word from the lexical distribution\n",
      "of a LM like Kobayashi (2018), the authors use\n",
      "the hidden state vector of the LM directly. Wu\n",
      "et al. (2019) substitutes the RNN LMs from pre-\n",
      "vious work and use BERT (Devlin et al., 2019)\n",
      "– a transformer trained with a masked language\n",
      "modeling objective – instead. The authors finetune\n",
      "BERT with a conditional masked language mod-\n",
      "eling objective that tries to avoid the prediction of\n",
      "words that do not correspond to the original sen-\n",
      "tence meaning.\n",
      "Another way to augmented MT data is by para-\n",
      "phrasing. If a good paraphrase system exists, this\n",
      "can increase the number of training instances (Hu\n",
      "et al., 2019). Paraphrasing can also be used at\n",
      "training time by sampling paraphrases of the refer-\n",
      "ence sentence from a paraphraser and training the\n",
      "MT model to predict the distribution of the para-\n",
      "phraser (Khayrallah et al., 2020). This helps the\n",
      "model to generalize. Wieting et al. (2019) propose\n",
      "a similar approach, using minimum risk training to\n",
      "optimize BLEU. To avoid BLEU’s constraints to a\n",
      "specific reference, they use paraphrasing to diver-\n",
      "sify the given reference.\n",
      "Finally, existing data can be augmented by\n",
      "adding noise. This noise can be continuous or dis-\n",
      "crete. In the case of applying continuous noise,\n",
      "noise vectors are added to the word embeddings\n",
      "(Cheng et al., 2018; Sano et al., 2019). Discrete\n",
      "noise is realized by inserting, deleting, or replac-\n",
      "ing words, BPE tokens, or characters to expand\n",
      "the training set in an adversarial fashion (Belinkov\n",
      "and Bisk, 2018; Ebrahimi et al., 2018; ?; Cheng\n",
      "et al., 2019, 2020).\n",
      "Pivoting While it is simple to implement and\n",
      "effective, pivot-based approaches suffer from er-\n",
      "ror propagation. To overcome that for NMT, jointtraining Zheng et al. (2017); Cheng (2019) and\n",
      "round-trip training (Ahmadnia and Dorr, 2019)\n",
      "have been proposed.\n",
      "Pivoting with NMT systems has been used\n",
      "for translating Japanese, Indonesian, and Malay\n",
      "into Vietnamese (Trieu et al., 2019), translation\n",
      "of related languages (Pourdamghani and Knight,\n",
      "2019), multilingual zero-shot MT (Lakew et al.,\n",
      "2018), and UMT (cf. §6.4) between distant lan-\n",
      "guage pairs (Leng et al., 2019).\n",
      "A.4 Recent low-resource Shared Tasks\n",
      "First, the LoResMT 2020 shared task (Ojha\n",
      "et al., 2020) explores the case of language pairs\n",
      "which have no parallel data between them (Hindi–\n",
      "Bhojpuri, Hindi–Magahi, and Russian–Hindi).\n",
      "The winning system (Laskar et al., 2020) uses\n",
      "a MASS model in a zero-shot fashion with ad-\n",
      "ditional monolingual data (see §6.4). Second,\n",
      "the WMT 2020 shared tasks on UMT and very\n",
      "low-resource supervised MT (Fraser, 2020) pro-\n",
      "vide text and 60k aligned phrases for German–\n",
      "Upper Sorbian., The most important technique in\n",
      "all tracks is transfer learning, achieving surpris-\n",
      "ingly good results. For the AmericasNLP 2021\n",
      "shared task on open MT (Mager et al., 2021), 10\n",
      "indigenous language languages were paired with\n",
      "Spanish, resulting in an extreme low-resource set-\n",
      "ting (4k to 125k paired sentences), with challenges\n",
      "out as domain, dialectical, and orthographic mis-\n",
      "matches between splits and datasets. The best\n",
      "systems shows that data cleaning and collection\n",
      "(§??) as well as multilingual approaches (§6.1)\n",
      "result in the best performance in this conditions.\n",
      "Finally the shared task on MT in Dravidian lan-\n",
      "guages (Chakravarthi et al., 2021) features 3 lan-\n",
      "guages paired with English as well as Tamil–\n",
      "Telugu. Again, the winning system uses a mul-\n",
      "tilingual approach. The best performing systems\n",
      "use BT (§6.3) and BPE word segmentation (§2.1).\n",
      "The results from these challenges indicate that\n",
      "the optimal selection and combination of meth-\n",
      "ods differs between cases (i.e., amount of mono-\n",
      "lingual, parallel data, cleanness of data, domain\n",
      "mismatch, linguistic closeness of languages). This\n",
      "implies that data analysis and linguistic knowl-\n",
      "edge are needed to improve a final system’s per-\n",
      "formance.\n",
      "A.5 Transfer learning\n",
      "This helps low-resource tasks as a lower amount\n",
      "of data can be used for training. One applicationof transfer learning to MT is the usage of a pre-\n",
      "trained RNN LM (Gulcehre et al., 2015) as the de-\n",
      "coder in an NMT system. Zoph et al. (2016) is the\n",
      "first work that uses pretrained models to improve\n",
      "NMT systems. The authors perform two experi-\n",
      "ments with an RNN encoder–decoder architecture\n",
      "with an attention mechanism: the model is first\n",
      "pretrained on a high-resource language pair This\n",
      "works even better if related languages are used\n",
      "during pretraining (Nguyen and Chiang, 2017).\n",
      "Using pretrained LMs at decoding time and as pri-\n",
      "ors at training time also improves vanilla models\n",
      "(Baziotis et al., 2020).\n",
      "To avoid overfitting, models can be finetuned on\n",
      "both a HRLs pair and a LRLs pair in a multi-task\n",
      "fashion (Neubig and Hu, 2018).\n",
      "However, how can we represent best the vocab-\n",
      "ulary? Zoph et al. (2016) use separate embeddings\n",
      "for the source and the target language. However,\n",
      "using tied embeddings has been shown to yield\n",
      "better results (Press and Wolf, 2017). Edunov et al.\n",
      "(2019) employs ELMO (Peters et al., 2018) repre-\n",
      "sentations as pretrained features in the encoder of\n",
      "a transformer model. Song et al. (2020) shows that\n",
      "it is possible to improve performance by combin-\n",
      "ing monolingual texts from linguistically related\n",
      "languages, performing a script mapping. It is also\n",
      "possible to extract features from a BERT model\n",
      "in the source language and combining these with\n",
      "an NMT system (Zhu et al., 2020b), but using a\n",
      "BERT model pretrained with a mixed sentences\n",
      "from source and target languages lead to even bet-\n",
      "ter results (Xu et al., 2021).\n",
      "Encoder-decoder pretrained models have\n",
      "gained popularity in the last years for low-\n",
      "resource MT. Conneau and Lample (2019)\n",
      "proposes training the encoder and the decoder\n",
      "separately in order to get cross-language rep-\n",
      "resentations (XLM). This idea has further been\n",
      "extended by Song et al. (2019, MASS) to\n",
      "masking a sequence of tokens from the input.\n",
      "Training MASS in a multilingual fashion and\n",
      "using monolingual data for pretraining helps to\n",
      "improve NMT for low-resource languages and\n",
      "zero-shot translation (Siddhant et al., 2020).\n",
      "Another approach is to train the entire transformer\n",
      "model as a denoising autoencoder (BART; Lewis\n",
      "et al., 2019). The multilingual version of BART\n",
      "(mBART) is more suitable for NMT tasks and\n",
      "yields important gains (Liu et al., 2020). It is also\n",
      "possible to pretrain a transformer in a multi-task,text-to-text fashion, where one of the tasks is\n",
      "MT (T5; Raffel et al., 2020). All four models\n",
      "can be finetuned for MT or used in an unsuper-\n",
      "vised fashion. Improvements to BART can be\n",
      "obtained by augmenting the maximum likelihood\n",
      "objective with an additional objective, which is\n",
      "a data-dependent Gaussian prior distribution (Li\n",
      "et al., 2020). Huge LMs can improve zero-shot\n",
      "and few-shot learning even further (Brown et al.,\n",
      "2020), but at a high computational cost. Pursuing\n",
      "another direction, Wang et al. (2019a) develops a\n",
      "hybrid architecture between a transformer and a\n",
      "pointer-generator network. At training time, the\n",
      "authors jointly train the encoder and the decoder\n",
      "in a denoising auto-encoding fashion.\n",
      "One crucial problem for transfer-learning is\n",
      "minimizing catastrophic forgetting (Serra et al.,\n",
      "2018). Chen et al. (2021) show that it is possible\n",
      "to combine a pre-trained multilingual model, with\n",
      "fine-tuining it with one single language pair, to im-\n",
      "prove zero-shot machine translation. Another way\n",
      "to handle this problem is reducing the number of\n",
      "parameter to be updated. Gheini et al. (2021) pro-\n",
      "pose to only update the cross attention parameters.\n",
      "A.6 Unsupervised MT\n",
      "The addition of other components such as masked\n",
      "LMs and denoising auto-encoding has also been\n",
      "tried (Stojanovski et al., 2019). Unsupervised\n",
      "methods are vulnerable to adversarial attacks of\n",
      "word substitution and order change in the input.\n",
      "Adversarial training can improve performance in\n",
      "such situations (Sun et al., 2020). Since the ini-\n",
      "tialization step is crucial for UMT, Ren et al.\n",
      "(2020) aligns semantically similar sentences from\n",
      "two monolingual corpora with the help of cross-\n",
      "lingual embeddings. With these, an SMT system\n",
      "is trained to warm up an NMT system. How-\n",
      "ever, UMT still has to overcome a set of chal-\n",
      "lenges. Søgaard et al. (2018) shows that perfor-\n",
      "mance decays dramatically for languages with dif-\n",
      "ferent typological features, since, in such situa-\n",
      "tions, bilingual word embeddings (Conneau et al.,\n",
      "2017) are far from isomorphic. Vuli ´c et al. (2020)\n",
      "finds that isomorphism is also less likely if small\n",
      "amounts of monolingual data are used for training\n",
      "bilingual word embeddings. Nooralahzadeh et al.\n",
      "(2020) discovers that performance quickly deteri-\n",
      "orates for a mismatch of source and target domain\n",
      "and that the initialization of word embeddings can\n",
      "affect MT performance. All of this makes UMTfor LRLs or endangered languages challenging.\n",
      "Some of the described issues have been ad-\n",
      "dressed: Liu et al. (2019) proposes to combine\n",
      "word-level and subword-level embeddings to ac-\n",
      "count for morphological complexity. For the prob-\n",
      "lem of distant language pairs, Leng et al. (2019)\n",
      "proposes pivoting (cf. §6.3). Isomorphism of\n",
      "bilingual word-embeddings can be improved with\n",
      "semi-supervised methods (Vuli ´c et al., 2019).\n",
      "Garcia et al. (2020) introduces multilingual\n",
      "UMT systems. The main idea consists of general-\n",
      "izing UMT by using a multi-way back-translation\n",
      "objective. Recently, pretrained multilingual trans-\n",
      "former networks are used to improve UMT even\n",
      "further (cf. §6.4).\n",
      "B Ethical Considerations\n",
      "Ethical concerns when working on MT for endan-\n",
      "gered languages include a lack of community in-\n",
      "volvement during language documentation, data\n",
      "creation, and development and setup of MT sys-\n",
      "tems. For more information, we refer interested\n",
      "readers to Bird (2020). Finally, we want to men-\n",
      "tion that publicly employing low-quality MT sys-\n",
      "tems for LRLs bears a risk of translating incor-\n",
      "rectly or in biased (e.g., sexist or racist) ways.\n"
     ]
    }
   ],
   "source": [
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Machine Translation for the Indigenous Languages of the\n",
      "Americas: An Introduction.\n",
      "Manuel MagerRajat BhatnagarGraham Neubig\n",
      "Ngoc Thang VuKatharina Kann\n",
      "AWS AI LabsCarnegie Mellon University\n",
      "University of Colorado BoulderUniversity of Stuttgart\n",
      "Abstract\n",
      "Neural models have drastically advanced state\n",
      "of the art for machine translation (MT) be-\n",
      "tween high-resource languages. Traditionally,\n",
      "these models rely on large amounts of train-\n",
      "ing data, but many language pairs lack these\n",
      "resources. However, an important part of the\n",
      "languages in the world do not have this amount\n",
      "of data. Most languages from the Americas are\n",
      "among them, having a limited amount of par-\n",
      "allel and monolingual data, if any. Here, we\n",
      "present an introduction to the interested reader\n",
      "to the basic challenges, concepts, and tech-\n",
      "niques that involve the creation of MT systems\n",
      "for these languages. Finally, we discuss the\n",
      "recent advances and findings and open ques-\n",
      "tions, product of an increased interest of the\n",
      "NLP community in these languages.\n",
      "1 Introduction\n",
      "More than 7 billion people on Earth communicate\n",
      "in nearly 7000 different languages (Pereltsvaig,\n",
      "2020). Of these, approximately 900 languages\n",
      "are native of the American continent (Campbell,\n",
      "2000). Most of these indigenous languages of the\n",
      "Americas (ILA) are endangered at some degree\n",
      "(Thomason, 2015). This huge variety in languages\n",
      "is simultaneously a rich treasure for humanity and\n",
      "also a barrier to communication among people\n",
      "from different backgrounds. Human translators\n",
      "have been important in overcoming language bar-\n",
      "riers. However, trained translators are not acces-\n",
      "sible to everyone on Earth and even scarcer for\n",
      "endangered and minority languages. The need\n",
      "for translations is even written in the constitutions\n",
      "of several countries like Mexico, Peru, Paraguay,\n",
      "Venezuela, and Bolivia (Zajcov, 2017) to allow\n",
      "native speakers to have equal language rights re-\n",
      "garding law.\n",
      "This is why developing MT is crucial: it helps\n",
      "humanity overcome language barriers while si-\n",
      "multaneously allowing people to continue using\n",
      "Work done while at the University of Stuttgart.their native tongue. However, the challenges to\n",
      "achieving these problems are not trivial. It is not\n",
      "only the amount of available data (a common the-\n",
      "sis among the NLP community) but also a set\n",
      "of challenging issues (dialectical and orthographic\n",
      "variations, noisy texts, complex morphology, etc.)\n",
      "that must be addressed.\n",
      "MT has always been an important task within\n",
      "the larger area of natural language processing\n",
      "(NLP). In 1954, the GeorgetownIBM experiment\n",
      "(Hutchins, 2004) was the first that showed at least\n",
      "some effectiveness of MT. Further research re-\n",
      "sulted in rule-based systems and statistical models.\n",
      "In 2023, neural models define state of the art for\n",
      "MT if training data is plentiful  i.e., for so-called\n",
      "high-resource languages (HRLs)  and have also\n",
      "achieved impressive results for low-resource lan-\n",
      "guages (LRLs). MT is also the most studied NLP\n",
      "task for the ILA (Mager et al., 2018b; Littell et al.,\n",
      "2018). The common issue among these languages\n",
      "is the extreme low-resource conditions they are\n",
      "confronted with. The research interest for these\n",
      "languages has increased in the last years, including\n",
      "the recent AmericasNLP 2021 shared task (Mager\n",
      "et al., 2021) on 10 indigenous languages to Span-\n",
      "ish, and the WMT (Conference on Machine Trans-\n",
      "lation) shared task for InuktitutEnglish (Barrault\n",
      "et al., 2020).\n",
      "In this work we aim to provide a comprehensive\n",
      "introduction to the challenges that involve creat-\n",
      "ing MT systems for ILA, and the current status\n",
      "of the existing work. We organize this work as\n",
      "follows: We start by introducing state-of-the-art\n",
      "NMT models (2). Then, we discuss the current\n",
      "challenges for these languages (3); and we in-\n",
      "troduce the key concepts related to low-resource\n",
      "NMT and the implications for endangered lan-\n",
      "guages of the Americas(3). This is followed by\n",
      "a discussion of available data (4). Afterwards,\n",
      "we introduce the important concepts for LRL and\n",
      "endangered languages (5); then we introduce thearXiv:2306.06804v1  [cs.CL]  11 Jun 2023main strategies aimed at improving NMT with\n",
      "limited training data (6); and finally we give an\n",
      "overview of the work done for ILA on MT (7).\n",
      "In doing so, we provide insights into the follow-\n",
      "ing questions: Which systems define the state of\n",
      "the art on low-resource NMT applied to the ILA?\n",
      "What is the route that ahead to improve the trans-\n",
      "lations of the ILA?\n",
      "2 Background and Definitions\n",
      "Formally, the task of MT consists of converting\n",
      "textXin a source language Lxinto text Yin a\n",
      "target language Lythat conveys the same mean-\n",
      "ing.1Translating text XLxintoYLycan be\n",
      "described as a function (Neubig, 2017):\n",
      "Y=MT(X). (1)\n",
      "XandYcan be of variable length, such as\n",
      "phrases, sentences, or even documents.\n",
      "If other languages are used during the transla-\n",
      "tion process, e.g., as pivots, we denote them as\n",
      "L1, . . . , L n. We refer to a corpus of monolingual\n",
      "sentences in language LiasMLi=S1, ..., S n.\n",
      "Probabilistic Modeling and Data When us-\n",
      "ing probabilistic MT models, the goal is to find\n",
      "YLywith the highest conditional probability,\n",
      "given XLx. Under the supervised machine\n",
      "learning paradigm, a parallel corpus Cparallel =\n",
      "(X1, Y1), ...,(Xn, Yn)is used to learn a set of pa-\n",
      "rameters , which define a probability distribution\n",
      "over possible translations. Given Cparallel , the\n",
      "training objective of an NMT model is generally\n",
      "to maximize the log-likelihood Lwith respect to\n",
      ":\n",
      "L=X\n",
      "(Xi,Yi)Cparallellogp(Yi|Xi;).(2)\n",
      "Within this overall framework, there are a num-\n",
      "ber of design decisions one has to make, such as\n",
      "which model architecture to use, how to generate\n",
      "translations, and how to evaluate.\n",
      "Decoding Decoding refers to the generation of\n",
      "output Y, given the parameters and an input X.\n",
      "Often, decoding is done by approximately solving\n",
      "the following maximization problem:\n",
      "argmax Yp(Y|X;) (3)\n",
      "1This is an approximation, since it is in general not possi-\n",
      "ble to map the meaning of text exactly into another language\n",
      "(Nida, 1945; Sechrest et al., 1972; Baker, 2018).Most NMT systems factorize the probability of\n",
      "Y= y1, ...,yTin a left-to-right fashion:\n",
      "p(Y) =TY\n",
      "t=1p(yt|y<t, X,  ) (4)\n",
      "Thus, the probability of token ytat time step tis\n",
      "computed using the previously generated tokens\n",
      "y<t, the source sentence Xand the model param-\n",
      "eters . Common algorithms for finding a high-\n",
      "probability translation are greedy decoding, i.e.,\n",
      "picking the token with the highest probability at\n",
      "each time step, and beam search (Lowerre, 1976).\n",
      "2.1 Input Representations\n",
      "The texts XandYare input into an NMT sys-\n",
      "tem as sequences of continuous vectors. How-\n",
      "ever, defining which units should be represented\n",
      "as such vectors is non-trivial. The classic way\n",
      "is to represent each word within XandYas\n",
      "a vector (or embedding). However, in a low-\n",
      "resource setting, often not all vocabulary items ap-\n",
      "pear in the training data (Jean et al., 2015; Lu-\n",
      "ong et al., 2015). This issue especially effects lan-\n",
      "guages with a rich inflectional morphology (Sen-\n",
      "nrich et al., 2016c): as many word forms can\n",
      "represent the same lemma, the vocabulary cover-\n",
      "age decreases drastically. Furthermore, for many\n",
      "LRLs, boundaries between words or morphemes\n",
      "are not easy to obtain or not well defined in the\n",
      "case of languages without a standard orthography.\n",
      "Alternative input units have been explored, such as\n",
      "characters (Ling et al., 2015), byte pair encoding\n",
      "(BPE; Sennrich et al., 2016a), morphological rep-\n",
      "resentations (Vania and Lopez, 2017; Ataman and\n",
      "Federico, 2018), syllables (Zhang et al., 2019), or,\n",
      "recently, a visual representation of rendered text\n",
      "(Salesky et al., 2021). No clear advantage has been\n",
      "discovered for using morphological segmentations\n",
      "over BPEs when testing them on LRLs (Saleva and\n",
      "Lignos, 2021).\n",
      "Input representations can be pretrained. The\n",
      "two most common options are: i) word em-\n",
      "beddings, where each type is represented by a\n",
      "vector, e.g., Word2Vec (Mikolov et al., 2013),\n",
      "Glove (Pennington et al., 2014), or Fasttext (Bo-\n",
      "janowski et al., 2017)) embeddings, and ii) contex-\n",
      "tualized word representations, where entire sen-\n",
      "tences are being encoded at a time, e.g., ELMo\n",
      "(Peters et al., 2018) or BERT (Devlin et al.,\n",
      "2019). However, training of these methods re-\n",
      "quires large monolingual training corpora, whichmay not be readily available for LRLs. As most\n",
      "ILA have rich morphology, this topic has gath-\n",
      "ered special interest. The discussion about the us-\n",
      "age of morpholigical segmented input for NMT\n",
      "models is recurrent. (Mager et al., 2022) show\n",
      "that the unsupervised morphologically inspired\n",
      "models outperform BPE pre-processing (experi-\n",
      "mented on 4 language pares). Similar experi-\n",
      "ments done on QuechuaSpanish and Inuktitut\n",
      "Enlgish (Schwartz et al., 2020), comparing BPEs\n",
      "against Morfessor (Smit et al., 2014). Also (Or-\n",
      "tega et al., 2020a) improves the SOTA (state-of-\n",
      "the-art) for QuechuaSpanish MT using a mor-\n",
      "phological guided BPE algorithm.\n",
      "2.2 Architectures\n",
      "NMT models typically are sequence-to-sequence\n",
      "models. They encode a variable-length sequence\n",
      "into a vector or matrix representation, which they\n",
      "then decode back into a variable-length sequence\n",
      "(Cho et al., 2014). The two most frequent archi-\n",
      "tectures are: i) recurrent neural networks (RNN),\n",
      "such as LSTMs (Hochreiter and Schmidhuber,\n",
      "1997) or GRUs (Cho et al., 2014), and ii) trans-\n",
      "formers (Vaswani et al., 2017), which define the\n",
      "current state of the art in the high-resource setting.\n",
      "As for most neural network models, training an\n",
      "NMT system on a limited number of instances\n",
      "is challenging (Fernndez-Delgado et al., 2014).\n",
      "There are common problems that arise from lim-\n",
      "ited data in the training set. One major advan-\n",
      "tage of neural models is their ability to learn rep-\n",
      "resentations from raw data, in contrast to manu-\n",
      "ally engineered features (Barron, 1993). However,\n",
      "problems arise when not enough data is provided\n",
      "to enable effective learning of features. Another\n",
      "strength of neural networks is their generalization\n",
      "capacity (Kawaguchi et al., 2017). However, train-\n",
      "ing a neural network on a small dataset easily leads\n",
      "to overfitting (Rolnick et al., 2017). Recent stud-\n",
      "ies, however, show empirically that this does not\n",
      "necessarily happen if the network is tuned cor-\n",
      "rectly (Olson et al., 2018).\n",
      "2.3 Evaluation\n",
      "Accurately judging translation quality is difficult\n",
      "and, thus, often still done manually: bilingual\n",
      "speakers assign scores according to provided crite-\n",
      "ria such as fluency and adequacy ( Does the output\n",
      "have the same meaning as the input? ). However,\n",
      "manual evaluation is expensive and slow. More-over, in the case of endangered languages, bilin-\n",
      "gual speakers can be hard or impossible to find.\n",
      "Automatic metrics provide an alternative.2\n",
      "These metrics assign a score to system output,\n",
      "given one or more ground truth reference transla-\n",
      "tions. The most widely used metric is BLEU (Pa-\n",
      "pineni et al., 2002), which relies on token-level n-\n",
      "gram matches between the translation to be rated\n",
      "and one or more gold-standard translations. For\n",
      "morphologically rich languages, character-level\n",
      "metrics, such as chrF (Popovi c, 2017), are often\n",
      "more suitable, as they allow for more flexibility.\n",
      "In the AmericasNLP ST (Mager et al., 2021) this\n",
      "metric was used over BLEU, as it fits better to the\n",
      "rich morphology of many ILA.\n",
      "To have a concrete example, lets have the fol-\n",
      "lowing Wixarika phrase with an English transla-\n",
      "tion:\n",
      "yu-huta-me ne-p+-we-iwa\n",
      "an-two-ns 1sg:s-asi-2pl:o-brother\n",
      "I have two brothers\n",
      "As discussed in (Mager et al., 2018c) it is dif-\n",
      "ficult to translate back from Spanish (or other Fu-\n",
      "sional language) the morpheme p+as it has not\n",
      "equivalent in these languages. So if we would ig-\n",
      "nore these morpheme at all, BLEU would penal-\n",
      "ize the entire word nep+weiwa . In contrast, chrF\n",
      "would give credit to the translation, even if the p+\n",
      "is missing.\n",
      "One shortcoming of these evaluation metrics is\n",
      "that the evaluation is very dependent on the sur-\n",
      "face forms and not on the ultimate goal of seman-\n",
      "tic similarity and fluency. Recent work uses pre-\n",
      "trained models to evaluate semantic similarity be-\n",
      "tween translations and the gold standard (Zhang\n",
      "et al., 2020d), but these methods are limited to lan-\n",
      "guages for which such models are available. This\n",
      "is not possible for the ILA, as the amount of mono-\n",
      "lingual data is not enough to train a reliable pre-\n",
      "trained language model3.\n",
      "3 Challenges and open questions\n",
      "In an overview of the datasets and recent studies\n",
      "of MT for the ILA, we found the following main\n",
      "issues to be handled.\n",
      "2For a detailed overview of automatic metrics for MT we\n",
      "refer the interested reader to specialized reviews (Han, 2016;\n",
      "Celikyilmaz et al., 2020; Chatzikoumi, 2020).\n",
      "3One exception to this is Quechua, that has a large enough\n",
      "monolingual dataset to train a BERT like model (Zevallos\n",
      "et al., 2022)Extreme low-resource parallel datasets Even\n",
      "with the recent advances, the resources available\n",
      "to train MT systems are extremely scarce, hav-\n",
      "ing training set between 4k and 20k sentences (see\n",
      "4), with notable exceptions for Inuktitut, Guarani\n",
      "and Quechua (Joanis et al., 2020; Ortega et al.,\n",
      "2020a).\n",
      "Lack of monolingual data Most of these lan-\n",
      "guages are mostly used in spoken form. In re-\n",
      "cent years, with the advancement and democra-\n",
      "tization of mobile technologies, indigenous lan-\n",
      "guages have seen a slight increase in massaging\n",
      "systems and private spheres (Rosales et al.). How-\n",
      "ever, the usage of these languages on the internet\n",
      "is rather limited. Even Wikipedia has a limited\n",
      "amount of these languages (Mager et al., 2018b).\n",
      "Low domain diversity . As most parallel\n",
      "datasets are scarce, they are restricted to a small\n",
      "number of domains, making it challenging to\n",
      "adapt it, or try to aim for general translation mod-\n",
      "els. This has been recognized as a major problem\n",
      "during the AmericasNLP ST (Mager et al., 2021).\n",
      "Rich morphology An important number of\n",
      "these languages are morphological highly rich. In\n",
      "many cases, we find polysynthetic, with or highly\n",
      "agglutinative languages (Kann et al., 2018) or even\n",
      "fusional phenomenon (Mager et al., 2020).\n",
      "Distant paired language The most common\n",
      "languages that we find that ILA is translated into\n",
      "are Spanish, English, and Portuguese. However,\n",
      "these languages are distantly related to the ILA,\n",
      "and have completely different linguistically phe-\n",
      "nomenons (Campbell, 2000; Romero et al., 2016).\n",
      "Noisy text environments Monolingual texts, if\n",
      "exist, are found in social media that often use a\n",
      "non-canonical witting (Rosales et al.).\n",
      "Code-Swithing This phenomenon is strongly\n",
      "present in ILA, as all of these languages are mi-\n",
      "nority languages in their own countries. The\n",
      "bilingualism among their communities is strong\n",
      "(and CS is a common phenomenon in this setup\n",
      "(etino glu, 2017)). The final result of this phe-\n",
      "nomenon is the inclusion of code-switching on a\n",
      "common base (Mager et al., 2019) in their lan-\n",
      "guage.\n",
      "Lack of orthographic normalization The us-\n",
      "age of ILA faces the problem of having a unifiedorthographic standard. This is not always possi-\n",
      "ble, as the suggestions of linguists and official en-\n",
      "tities do not always match the day-by-day writ-\n",
      "ing of the speakers. Moreover, in some cases,\n",
      "special symbols present in the orthographic stan-\n",
      "dards are not accessible in English or Spanish key-\n",
      "board and need to be replaced with other symbols.\n",
      "The winner of the AmericasNLP ST got important\n",
      "improvements using orthographic normalizers de-\n",
      "veloped specifically for each American language\n",
      "(Vzquez et al., 2021).\n",
      "Dialectal variety The indigenous languages\n",
      "have a strong dialectal variety, making it hard for\n",
      "native speakers to understand even speakers from\n",
      "neighboring villages. The linguistic richness of\n",
      "entire regions is so diverse that even a single state\n",
      "like the Mexican Oaxaca could correspond to the\n",
      "diversity in the whole Europe (McQuown, 1955).\n",
      "4 Available MT datasets for ILA\n",
      "The parallel datasets available for MT have been\n",
      "increasing during the last years. At this moment,\n",
      "we can show in two folds the development of these\n",
      "resources: as shown in table 2 work on specific\n",
      "language has emerged; but also broader datasets\n",
      "have started to cover the ILA (see table 1).\n",
      "Language-specific corpus collection work has\n",
      "been done for many languages, where parallel\n",
      "corpus has been the main component. In re-\n",
      "cent time we have seen CherokeeEnglish (OPUS)\n",
      "(Zhang et al., 2020c), WixarikaSpanish (Mager\n",
      "et al., 2018a), ShipioKonibo (Feldman and Coto-\n",
      "Solano, 2020), and others (see table 2). The most\n",
      "prominent of these datasets has been the Inuktitut\n",
      "English parallel data. The last version of this\n",
      "dataset corpora (Joanis et al., 2020) is has medium\n",
      "size with 1,450,094 sentences. Previous versions\n",
      "of this corpus are (Martin et al., 2003). This data\n",
      "set was used for the WMT 2020 Shared Task on\n",
      "Unsupervised, and Low Resourced MT (Barrault\n",
      "et al., 2020).\n",
      "For wide-spoken languages like Guarani, it is\n",
      "even possible to collect a web crawled dataset,\n",
      "including news articles and social media parallel\n",
      "aligned data (Chiruzzo et al., 2020; Gngora et al.,\n",
      "2021) This dataset also includes monolingual data.\n",
      "This is possible as Guaran is one of the most spo-\n",
      "ken indigenous languages of the continent.\n",
      "In contrast to the language-specific datasets,\n",
      "we find broader approaches (see table 1). The\n",
      "broadest multilingual dataset, which contains theDataset Paired-languages Authors\n",
      "AmericasNLI Aymara, Ashninka, Bribri, Guaran,\n",
      "Nahuatl, Otom, Quechua, Rarmuri,\n",
      "Shipibo-Konibo, Wixarika(Ebrahimi et al., 2022)\n",
      "CPML Chol, Maya, Mazatec, Mixtec, Nahu-\n",
      "atl and Otomi(Sierra Martnez et al., 2020)\n",
      "OPUS * (Tiedemann, 2016)\n",
      "New testament Bible * (McCarthy et al., 2020)\n",
      "Table 1: Parallel dataset collections that contain one or more indigenous languages of the Americas\n",
      "Language Paried-language ISO Family Sentences Domain Authors\n",
      "Ashninka Spanish cni Arawak 3883 (Ortega et al., 2020b)\n",
      "Bribri Spanish bzd Chibchan 5923 (Feldman and Coto-\n",
      "Solano, 2020)\n",
      "Guarani Spanish gn Tupi-Guarani News,\n",
      "Blogs(Abdelali et al., 2006)\n",
      "Guarani Spanish gn Tupi-Guarani 14,531 News,\n",
      "Blogs(Chiruzzo et al., 2020)\n",
      "Guarani Spanish gn Tupi-Guarani 14,792 News, So-\n",
      "cial Media(Gngora et al., 2021)\n",
      "Guarani Spanish gn Tupi-Guarani 30855 8 Domains (Chiruzzo et al., 2022)\n",
      "Nahuatl Spanish nah Uto-Aztecan 16145 Diverse\n",
      "Books(Gutierrez-Vasques\n",
      "et al., 2016)\n",
      "Otom Spanish oto Oto-Manguean 4889 Diverse\n",
      "Bookshttps://\n",
      "tsunkua.elotl.\n",
      "mx\n",
      "Rarmuri Spanish tar Uto-Aztecan 14721 Dictionary\n",
      "Examples(Mager et al., 2022)\n",
      "Shipibo-Konibo Spanish shp Panoan 14592 Educational,\n",
      "Religious(Galarreta et al., 2017)\n",
      "Wixarika Spanish hch Uto-Aztecan 8966 Literature (Mager et al., 2018a)\n",
      "Cherokee English chr Uto-Aztecan OPUS (Zhang et al., 2020c)\n",
      "Inuktitut English iku EskimoAleut 1,450,094 Legislative (Joanis et al., 2020)\n",
      "Ayuuk Spanish mir MixeZoque 7553 Diverse (Zacaras Mrquez and\n",
      "Meza Ruiz, 2021)\n",
      "Mazatec Spanish Many Oto-Manguean 9799 Diverse (Tonja et al., 2023)\n",
      "Mixtec Spanish Many Oto-Manguean 13235 Diverse (Tonja et al., 2023)\n",
      "Table 2: Parallel datasets that have been released focusing on one indigenous language\n",
      "Bibles New Testament, includes about 1600 lan-\n",
      "guages (Mayer and Cysouw, 2014; McCarthy\n",
      "et al., 2020) of the 2,508 that have been collected\n",
      "by the Summer Institute of Linguistic (SIL) (An-\n",
      "derson and Anderson, 2012). Another remarkable\n",
      "effort to obtain broad language coverage is the\n",
      "PanLex project (Kamholz et al., 2014), which has\n",
      "gathered lexical translation dictionaries for over\n",
      "5,700 languages. However, for most languages,\n",
      "PanLex contains only a few dozen words. Duan\n",
      "et al. (2020) show that such dictionaries can be\n",
      "used to create an NMT system, making bilingual\n",
      "dictionaries relevant for further studies.\n",
      "Recently community-driven research groups\n",
      "have started the creation of own parallel datasets,\n",
      "such as Masakhane (Orife et al., 2020; Nekoto\n",
      "et al., 2020) for African languages, and Americ-\n",
      "asNLP for indigenous languages of the Americas(Ebrahimi et al., 2021; Mager et al., 2021). The\n",
      "AmericasNLI dataset is an important effort to have\n",
      "a common evaluation benchmark for the 10 in-\n",
      "digenous languages of the Americas for the MT\n",
      "and NLI tasks.\n",
      "Given the constitutional rights of indigenous\n",
      "languages in many countries of the Americas, it is\n",
      "possible to access this data. Vzquez et al. (2021)\n",
      "made available this resource during their shared\n",
      "task system development.\n",
      "Finally, it is important to mention that many\n",
      "of the languages spoken in the Americas have\n",
      "Wikipedias set of articles available4.\n",
      "4The available languages in wikipedia can be consulted\n",
      "at:https://es.wikipedia.org/wiki/Portal:\n",
      "Lenguas_ind genas_de_Amlrica . Until the\n",
      "publication of this article, there were only entries in Nahu-\n",
      "atl, Navajo, Guarani, Aymara, Klaalisut, Esquimal, Inukitut,\n",
      "Cherokee, and Cree.Collection of New Data A common way to cre-\n",
      "ate parallel data with the help of bilingual speakers\n",
      "is via elicitation (translating the foreign text into\n",
      "another language). It has the disadvantage of bias-\n",
      "ing the created text to forms and topics, culture,\n",
      "and even grammatical forms towards the source\n",
      "language (Lrscher, 2005). A method that avoids\n",
      "this problem is language documentation, which\n",
      "consists of storing and annotating commonly used\n",
      "speech or text (Himmelmann, 2008). However, it\n",
      "is costly and requires specialists. In this process,\n",
      "involving the community members that are bilin-\n",
      "gual speakers is important (Bird, 2020).\n",
      "5 Low-resource MT\n",
      "For the purpose of this paper we define LRLs\n",
      "as languages for which standard techniques are\n",
      "unable to create well performing systems, which\n",
      "makes it necessary to resort to other techniques\n",
      "(cf. Figure 1) such as transfer learning. For MT,\n",
      "the amount of available resources differs widely\n",
      "across language pairs: some have less than 10k\n",
      "parallel sentences, while other have more than\n",
      "500k, with some exceptions in the orders of sev-\n",
      "eral million.\n",
      "Emulating a low-resource scenario by down-\n",
      "sampling available data for high-resource lan-\n",
      "guages is common and helps understanding a\n",
      "models performance across different settings.\n",
      "However, further evaluating methods on a diverse\n",
      "set of low-resource languages is crucial, since\n",
      "many languages exhibit particular linguistic phe-\n",
      "nomena (Mager et al., 2020), that perturb the fi-\n",
      "nal results, especially since most large datasets\n",
      "are from the Indo-European language family, to\n",
      "which only 6.16% of the worlds languages belong\n",
      "(Lewis, 2009).\n",
      "Importantly, there is no strong correlation be-\n",
      "tween the number of resources available per lan-\n",
      "guage and the number of speakers: Javanese with\n",
      "95 million speakers and Kannada with 44 million\n",
      "are considered LRLs, while French, with only 64\n",
      "million native speakers, is among the most widely\n",
      "studied languages. Improving models to handle\n",
      "LRLs will extend access to information online as\n",
      "well as human language technology to all mono-\n",
      "lingual speakers of those languages. In the case\n",
      "of ILA, most languages are endangered at some\n",
      "degree, but most of them have the same issue:\n",
      "they are low resourced for parallel and monolin-\n",
      "gual data.Endangered Languages Krauss (1992) esti-\n",
      "mates that 50% of all languages are doomed or\n",
      "dying, and that in this century we will see either\n",
      "the death or the doom of 90% of all human lan-\n",
      "guages. The current proportion of languages that\n",
      "are already extinct or moribund ranges from 31%\n",
      "down to 8% depending on the region, with the\n",
      "most severe cases in the Americas and Australia\n",
      "(Simons and Lewis, 2013). To determine how en-\n",
      "dangered a language is, Lewis and Simons (2010)\n",
      "proposes a classification scale called EGIDS with\n",
      "13 levels. The higher the number on this scale,\n",
      "the greater the level of disruption of the languages\n",
      "inter-generational transmission.5MT for endan-\n",
      "gered LRLs has the potential to help with doc-\n",
      "umentation, promotion and revitalization efforts\n",
      "(Galla, 2016; Mager et al., 2018b). However, as\n",
      "these languages are commonly spoken by small\n",
      "communities, or indigenous people, researchers\n",
      "should aim for a direct involvement of those com-\n",
      "munities (Bird, 2020).\n",
      "What is polysynthesis? A polysynthetic lan-\n",
      "guage is defined by the following linguistic fea-\n",
      "tures: the verb in a polysynthetic language must\n",
      "have an agreement with the subject, objects and\n",
      "indirect objects (Baker, 1996); nouns can be in-\n",
      "corporated into the complex verb morphology\n",
      "(Mithun, 1986); and, therefore, polysynthetic lan-\n",
      "guages have agreement morphemes, pronominal\n",
      "affixes and incorporated roots in the verb (Baker,\n",
      "1996), and also encode their relations and charac-\n",
      "terizations into that verb. The most common word\n",
      "orders present in these languages are SOV , VSO,\n",
      "SVO and free order. It is important to notice that\n",
      "a polysynthtic language can have a aggutinative6\n",
      "or can have also fusional characteristics, like To-\n",
      "tonaco or Tepehua (Mager et al., 2020).\n",
      "6 Low-resource MT paradigms\n",
      "Most languages of the Americas do not have high\n",
      "amount of data for MT. Therefore, we introduce\n",
      "the most important paradigms to improve low-\n",
      "resourced machine translation. Figure 1 shows a\n",
      "general overview of the methods and options to\n",
      "improve LRL MT. For a more detailed understand-\n",
      "ing of this techniques we refer the reader to spe-\n",
      "cialized low-resource MT surveys (Haddow et al.,\n",
      "5The complete EGIDS scale can be found at https://\n",
      "www.ethnologue.com/about/language-status\n",
      "6Agglutination refers to a concatenation of morphemes,\n",
      "with minimal changes to the surface form.DAT A\n",
      "No parallel data\n",
      " Low amount of\n",
      "parallel dataCollect new data\n",
      "Parallel data\n",
      "to 3th language\n",
      "Only monolingual\n",
      "data\n",
      "Additional\n",
      "Annotated data\n",
      "*Monolingual*Parallel data in\n",
      "other languages*Pre-trained\n",
      "models\n",
      "*  Any data\n",
      "(Monolingual,\n",
      "Multilingual, etc)*Annotators\n",
      "Noisy Parallel\n",
      "Data\n",
      "Speechrecordings\n",
      "Zero-shot\n",
      "Multi-task Data\n",
      "Augmentation\n",
      "Back\n",
      "T ranslationSentence\n",
      "ModificationMultilingual\n",
      "T ransfer L earningElicitation\n",
      "Annotating\n",
      "commonly\n",
      "used speech\n",
      "or text\n",
      "Noisy parallel\n",
      "documents\n",
      "to parallel\n",
      "sentencesPivotingUnsupervised\n",
      "MT\n",
      "Figure 1: What to do when we have low o no data to train our machine translation models? This diagram shows\n",
      "basic scenarios, solutions, and common requirements for each method, with the section describing the method.\n",
      "L1L2\n",
      "Ln...\n",
      "L1L2\n",
      "Ln...\n",
      "L1\n",
      "Ln...L1\n",
      "Ln...\n",
      "\n",
      "1) One to many\n",
      "2) Many to one\n",
      "3) Many to many\n",
      "Figure 2: An overview of different multilingual setups.\n",
      "2022; Wang et al., 2021; Ranathunga et al., 2021).\n",
      "6.1 Multilingual Supervised Training\n",
      "With a multilingual set of parallel data\n",
      "Dparallel between different language pairs\n",
      "{(L1, L2), . . . , (Lm, Ln)}we can train a model\n",
      "that is able to map a sentence from any source\n",
      "language Lxinto any target language Lythat is\n",
      "contained in Dparallel (see 2). These multilingual\n",
      "NMT models have seen a growth in popularity\n",
      "and efficiency in recent years. We will now\n",
      "cover the different training algorithms for these\n",
      "models: 1) many source languages and one target\n",
      "language ( many-to-one ), 2) one source and many\n",
      "target languages ( one-to-many ), and 3) many\n",
      "source languages and many target languages\n",
      "(many-to-many ). For a general overview of\n",
      "multilingual MT, we refer the reader to surveys\n",
      "dedicated to this topic (Tan et al., 2019; Dabre\n",
      "et al., 2019). Johnson et al. (2017) are the first\n",
      "to introduce a multilingual NMT model, trained\n",
      "on translating from a large number of languages\n",
      "to English as well as in the opposite direction.\n",
      "The authors show that these models improve over\n",
      "DL1D'L2L1  L2\n",
      "D'L2DL1L2  L1Decode\n",
      "TrainFigure 3: Backtranslation\n",
      "single-language pair models for LRLs.\n",
      "6.2 Multi-task Training\n",
      "Multi-task training (Caruana, 1997) aims to im-\n",
      "prove the performance of the main task  MT in\n",
      "our case  by adding one or more auxiliary tasks\n",
      "to the training. The easiest way is to share all pa-\n",
      "rameters of the network, using the ideas already\n",
      "explored in multilingual NMT (6.1). This can be\n",
      "done with a special flag in the input that specifies\n",
      "the current task. It is also possible to share only the\n",
      "encoder and have two separate decoders for each\n",
      "task.\n",
      "Multilingual Modeling In order to handle mul-\n",
      "tilinguality it is also possible to adapt modify the\n",
      "NMT models. The main proposals to do so has\n",
      "been: sharing all parameter except the attention\n",
      "mechanism of a RNN NMT model (Blackwood\n",
      "et al., 2018); parameter sharing in the transformer\n",
      "architecture Sachan and Neubig (2018);\n",
      "6.3 Data Augmentation\n",
      "Back-Translation A straightforward way to\n",
      "leverage monolingual data for low-resource MT is\n",
      "to generate a meaningful signal with the help of\n",
      "an already initialized MT model (see Figure 3).This method is called back-translation (BT; Sen-\n",
      "nrich et al., 2016b): With monolingual data MLx\n",
      "in source language Lxand a trained model that is\n",
      "able to translate from Lxinto a target language Ly\n",
      "we can generate a translation MLy. This pseudo\n",
      "parallel data (MLx, MLy)is then used to train a\n",
      "new model in the opposite direction. This process\n",
      "can be applied iteratively to improve the transla-\n",
      "tion (Hoang et al., 2018).\n",
      "Sentence Modification Other methods to gen-\n",
      "erate more parallel sentences are based on lexi-\n",
      "cal substitution. Fadaee et al. (2017) explores re-\n",
      "placing frequent words with low-frequency ones\n",
      "in both source and target to improve the transla-\n",
      "tion of rare words. This is done using language\n",
      "models (LMs) and automatic alignment.\n",
      "Pivoting If no parallel corpus between lan-\n",
      "guages LxandLyis available, but both of them\n",
      "have parallel corpora with a third language Lp,\n",
      "pivoting is an option. The basic idea is to train\n",
      "two MT systems: one that translates LxLp\n",
      "and another for LpLy. Pivoting has first been\n",
      "introduced for SMT (Wu and Wang, 2007; Cohn\n",
      "and Lapata, 2007; Utiyama and Isahara, 2007).\n",
      "6.4 Semi-supervised and Unsupervised MT\n",
      "Transfer Learning via Pretraining Transfer\n",
      "learning refers to using knowledge learned from\n",
      "one task to improve performance on a related task\n",
      "(Weiss et al., 2016). In recent years this approach\n",
      "has gained popularity with big multilingual mod-\n",
      "els such as Conneau and Lample (2019) that pro-\n",
      "poses training the encoder and the decoder sep-\n",
      "arately in order to get cross-language represen-\n",
      "tations (XLM). This idea has further been ex-\n",
      "tended by Song et al. (2019, MASS) to masking\n",
      "asequence of tokens from the input (multilingual\n",
      "MASS (Siddhant et al., 2020)). Another approach\n",
      "is to train the entire transformer model as a denois-\n",
      "ing autoencoder (BART; Lewis et al., 2019) ( mul-\n",
      "tilingual BART (mBART) (Liu et al., 2020)). It is\n",
      "also possible to pretrain a transformer in a multi-\n",
      "task, text-to-text fashion, where one of the tasks is\n",
      "MT (T5; Raffel et al., 2020) (multilingual version\n",
      "(Xue et al., 2021)).\n",
      "Unsupervised MT UMT covers approaches that\n",
      "donotrequire any parallel text, relying only on\n",
      "monolingual data. This differs from zero-shot\n",
      "translation, which uses parallel data for other lan-\n",
      "guage pairs. Early approaches tackled the prob-lem with an auto-encoder with adversarial train-\n",
      "ing (Lample et al., 2017) or with auto-encoders\n",
      "with a shared encoding space as well as separate\n",
      "decoders for each target language (Artetxe et al.,\n",
      "2018). The main problem for these approches is\n",
      "the need of a big monolingual dataset, that is not\n",
      "available for most ILA.\n",
      "7 Advances in MT for the indigenous\n",
      "languages of the Americas\n",
      "In recent years the interest in MT for indigenous\n",
      "languages of the Americas has increased. The\n",
      "task is not easy. The first usage of NMT systems\n",
      "has not been successful (Mager and Meza, 2021).\n",
      "However, with the use of LRL MT methods, we\n",
      "have witnessed great improvements.\n",
      "The CherokeeEnglish (Zhang et al., 2020c)\n",
      "language pair has been explored using a pre-\n",
      "trained BERT (Devlin et al., 2019) for the En-\n",
      "glish side. A system demonstration of this ap-\n",
      "proach is also accessible (Zhang et al., 2021). The\n",
      "back translation strategy for BribriSpanish NMT\n",
      "transformers has also been explored (Feldman and\n",
      "Coto-Solano, 2020) and by (Oncevay, 2021) (for\n",
      "four Peruvian languages to Spanish) with good\n",
      "results. The scarce indigenous language mono-\n",
      "lingual text can be replaced to some extent with\n",
      "Spanish text or extracted from PDFs, and other\n",
      "sources (Bustamante et al., 2020).\n",
      "One of the main challenges for the complex\n",
      "morphological languages in the area has been the\n",
      "prepossessing step. Schwartz et al. (2020) show\n",
      "that even if morphological segmentation has less\n",
      "perplexity a the language modeling time, it is\n",
      "still under-performing or equivalent against BPEs\n",
      "for MT (for Inuktitut-English, YupikEnglish\n",
      "Data, GuaranSpanish Data). A more compre-\n",
      "hensive (on the segmentation modeling side) was\n",
      "done by (Mager et al., 2022) exploring a wide\n",
      "array of segmentation models.The latter study\n",
      "showed that supervised morphological segmenta-\n",
      "tion under-perform unsupervised. However, unsu-\n",
      "pervised morphological segmentation like LMVR\n",
      "(Ataman et al., 2017) and FlatCat (Grnroos et al.,\n",
      "2014) perform better than BPEs. (Ngoc Le and\n",
      "Sadat, 2020) studied how better to perform word\n",
      "segmentation for the InuktitutEnglish pair. They\n",
      "found that for this language pair, a morphological\n",
      "segmentation, or a combination of BPEs and mor-\n",
      "phological segmentation, works better than just\n",
      "applying vanilla BPEs. Also, training word em-beddings for GuaraniSpanish translation is an\n",
      "excellent opportunity to increase the MT perfor-\n",
      "mance of these languages (Gngora et al., 2022).\n",
      "The usage of transfer learning from multilin-\n",
      "gual systems has been tried, with limited re-\n",
      "sults (Nagoudi et al., 2021) (training an own T5\n",
      "model for indigenous languages) and (Zheng et al.,\n",
      "2021). However, pertaining a SpanishEnglish\n",
      "model together with ILA, and then fine-tuning it\n",
      "(together with a careful prepossessing and filter-\n",
      "ing step) has been the most successful strategy\n",
      "(Vzquez et al., 2021).\n",
      "The quality of MT systems of ILA has been a\n",
      "constant debate. However, Ebrahimi et al. (2021)\n",
      "shows that the quality of MT for these languages\n",
      "is enough to improve other tasks like natural lan-\n",
      "guage inference (NLI).\n",
      "InuktitutEnlgish ST The WMT 2020 news\n",
      "translation task included InuktitutEnglish trans-\n",
      "lation (Barrault et al., 2020). The participating\n",
      "systems explored the difficulties of working with\n",
      "a polysynthetic language in a medium resource\n",
      "scenario. Participating teams in this competi-\n",
      "tion were: (Kocmi, 2020; Hernandez and Nguyen,\n",
      "2020; Scherrer et al., 2020; Roest et al., 2020; Lo,\n",
      "2020; Knowles et al., 2020; Zhang et al., 2020e;\n",
      "Krubi nski et al., 2020).\n",
      "AmericasNLP 2021 and 2023 ST In 2021, the\n",
      "AmericasNLP community organized a workshop\n",
      "on Machine Translation for 10 indigenous lan-\n",
      "guages of the Americas in 2021 (Mager et al.,\n",
      "2021) and 2023 (Ebrahimi et al., 2023) with an\n",
      "additional indigenous language (Chatino). The\n",
      "AmericasNLP shared task winner was (Vzquez\n",
      "et al., 2021) in 2021, and a more mixed result\n",
      "in 20237. Other participants in this shared task\n",
      "are (Nagoudi et al., 2021; Bollmann et al., 2021;\n",
      "Zheng et al., 2021; Knowles et al., 2021; Parida\n",
      "et al., 2021; Nagoudi et al., 2021). It is impor-\n",
      "tant to point at the importance of clean datata. For\n",
      "Quechua, (Moreno, 2021) got the best results gen-\n",
      "erating an additional amount of clean data.\n",
      "AmericasNLP 2022 Competition is a com-\n",
      "petition on Speech-to-Text translation is or-\n",
      "ganized and is targeting the following lan-\n",
      "guage pairs: BribriSpanish, GuaranSpanish,\n",
      "KotiriaPortuguese, WaikhanaPortuguese, and\n",
      "7Up to this moment, no official desciption papers for the\n",
      "2023 are published.QuechuaSpanish (Ebrahim et al., 2023)8.\n",
      "8 Ethical aspects\n",
      "When working with ILAs are also interacting with\n",
      "communities and nations that speak these lan-\n",
      "guages. In most cases, these speakers have been\n",
      "exposed to a colonial past, or to a local oppres-\n",
      "sion, by the majority language and culture. It is\n",
      "important to point to best practices and recom-\n",
      "mendations when performing our research. Bird\n",
      "(2020) and Liu et al. (2022) advocate to include\n",
      "community members as co-authors (Liu et al.,\n",
      "2022) as well as considering data and technology\n",
      "sovereignty. This is also aligned with the com-\n",
      "munity building aimed at by Zhang et al. (2022).\n",
      "Mager et al. (2023) summarizes the main aspects\n",
      "that should be considered as follows: i) Consul-\n",
      "tation, Negotiation and Mutual Understanding . It\n",
      "is important to inform the community about the\n",
      "planned research, negotiating a possible outcome,\n",
      "and reaching a mutual agreement on the direc-\n",
      "tions and details of the project should happen in\n",
      "all cases. ii) Respect of the local culture and in-\n",
      "volvement . As each community has its own cul-\n",
      "ture and view of the world, researchers should be\n",
      "familiar with the history and traditions of the com-\n",
      "munity. Also, it should be recommended that lo-\n",
      "cal researchers, speakers, or internal governments\n",
      "should be involved in the project. iii) Sharing and\n",
      "distribution of data and research . The product\n",
      "of the research should be available for use by the\n",
      "community, so they can take advantage of the gen-\n",
      "erated materials, like papers, books, or data.\n",
      "9 Conclusion\n",
      "Machine translation for ILA has gained interest in\n",
      "the NLP community over the last few years. Here,\n",
      "we provide an exhaustive overview of the basic\n",
      "MT concepts and the particular challenges for MT\n",
      "for ILA (in the context of low-resource scenarios\n",
      "and its relation to endangered languages). We ad-\n",
      "ditionally survey the current advances of MT for\n",
      "these languages.\n",
      "Limitations\n",
      "This papers aim is to give an introduction to re-\n",
      "searchers, students, of interested community in-\n",
      "digenous community members to the topic of Ma-\n",
      "chine Translation for Indigenous languages of the\n",
      "8http://turing.iimas.unam.mx/\n",
      "americasnlp/st.htmlAmericas. Therefore, this paper is not an in-depth\n",
      "survey of the literature on indigenous languages\n",
      "nor a more technical survey of low-resource ma-\n",
      "chine translation. We would point the reader to\n",
      "more specific surveys on these aspects (Haddow\n",
      "et al., 2022; Mager et al., 2018b).\n",
      "Ethical statement\n",
      "We could not find any specific Ethical issue for\n",
      "this paper or potential danger. Nevertheless, we\n",
      "want to point to the reader that working with in-\n",
      "digenous languages (in this case, MT) implies a\n",
      "set of ethical questions that are important to han-\n",
      "dle. For a deeper understanding of the matter, we\n",
      "suggest specialized literature to the reader (Mager\n",
      "et al., 2023; Bird, 2020; Schwartz, 2022).\n",
      "References\n",
      "Ahmed Abdelali, James Cowie, Steve Helmreich,\n",
      "Wanying Jin, Maria Pilar Milagros, Bill Ogden,\n",
      "Mansouri Rad, and Ron Zacharski. 2006. Guarani:\n",
      "A case study in resource development for quick\n",
      "ramp-up MT. In Proceedings of the 7th Confer-\n",
      "ence of the Association for Machine Translation in\n",
      "the Americas: Technical Papers , pages 19, Cam-\n",
      "bridge, Massachusetts, USA. Association for Ma-\n",
      "chine Translation in the Americas.\n",
      "Idris Abdulmumin, Bashir Shehu Galadanci, and Aliyu\n",
      "Garba. 2019. Tag-less back-translation. arXiv\n",
      "preprint arXiv:1912.10514 .\n",
      "Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019.\n",
      "Massively multilingual neural machine translation.\n",
      "InProceedings of the 2019 Conference of the North\n",
      "American Chapter of the Association for Computa-\n",
      "tional Linguistics: Human Language Technologies,\n",
      "Volume 1 (Long and Short Papers) , pages 3874\n",
      "3884.\n",
      "Benyamin Ahmadnia and Bonnie J Dorr. 2019. Aug-\n",
      "menting neural machine translation through round-\n",
      "trip training approach. Open Computer Science ,\n",
      "9(1):268278.\n",
      "Antonios Anastasopoulos and David Chiang. 2018.\n",
      "Tied multitask learning for neural speech translation.\n",
      "InProceedings of the 2018 Conference of the North\n",
      "American Chapter of the Association for Computa-\n",
      "tional Linguistics: Human Language Technologies,\n",
      "Volume 1 (Long Papers) , pages 8291, New Orleans,\n",
      "Louisiana. Association for Computational Linguis-\n",
      "tics.\n",
      "Stephen R Anderson and Stephen Anderson. 2012.\n",
      "Languages: A very short introduction , volume 320.\n",
      "Oxford University Press.Naveen Arivazhagan, Ankur Bapna, Orhan Firat,\n",
      "Roee Aharoni, Melvin Johnson, and Wolfgang\n",
      "Macherey. 2019a. The missing ingredient in zero-\n",
      "shot neural machine translation. arXiv preprint\n",
      "arXiv:1903.07091 .\n",
      "Naveen Arivazhagan, Ankur Bapna, Orhan Firat,\n",
      "Dmitry Lepikhin, Melvin Johnson, Maxim Krikun,\n",
      "Mia Xu Chen, Yuan Cao, George Foster, Colin\n",
      "Cherry, et al. 2019b. Massively multilingual neural\n",
      "machine translation in the wild: Findings and chal-\n",
      "lenges. arXiv preprint arXiv:1907.05019 .\n",
      "Mikel Artetxe, Gorka Labaka, Eneko Agirre, and\n",
      "Kyunghyun Cho. 2018. Unsupervised neural ma-\n",
      "chine translation. In 6th International Conference\n",
      "on Learning Representations, ICLR 2018 .\n",
      "Mikel Artetxe and Holger Schwenk. 2019. Mas-\n",
      "sively multilingual sentence embeddings for zero-\n",
      "shot cross-lingual transfer and beyond. Transac-\n",
      "tions of the Association for Computational Linguis-\n",
      "tics, 7:597610.\n",
      "Duygu Ataman and Marcello Federico. 2018. Compo-\n",
      "sitional representation of morphologically-rich input\n",
      "for neural machine translation. In Proceedings of\n",
      "the 56th Annual Meeting of the Association for Com-\n",
      "putational Linguistics (Volume 2: Short Papers) ,\n",
      "pages 305311.\n",
      "Duygu Ataman, Matteo Negri, Marco Turchi, and Mar-\n",
      "cello Federico. 2017. Linguistically motivated vo-\n",
      "cabulary reduction for neural machine translation\n",
      "from turkish to english.\n",
      "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\n",
      "ton. 2016. Layer normalization. stat, 1050:21.\n",
      "Mark C Baker. 1996. The polysynthesis parameter .\n",
      "Oxford University Press.\n",
      "Mona Baker. 2018. In other words: A coursebook on\n",
      "translation . Routledge.\n",
      "Loc Barrault, Magdalena Biesialska, Ond rej Bojar,\n",
      "Marta R. Costa-juss, Christian Federmann, Yvette\n",
      "Graham, Roman Grundkiewicz, Barry Haddow,\n",
      "Matthias Huck, Eric Joanis, Tom Kocmi, Philipp\n",
      "Koehn, Chi-kiu Lo, Nikola Ljubei c, Christof\n",
      "Monz, Makoto Morishita, Masaaki Nagata, Toshi-\n",
      "aki Nakazawa, Santanu Pal, Matt Post, and Marcos\n",
      "Zampieri. 2020. Findings of the 2020 conference on\n",
      "machine translation (WMT20). In Proceedings of\n",
      "the Fifth Conference on Machine Translation , pages\n",
      "155, Online. Association for Computational Lin-\n",
      "guistics.\n",
      "Andrew R Barron. 1993. Universal approximation\n",
      "bounds for superpositions of a sigmoidal func-\n",
      "tion. IEEE Transactions on Information theory ,\n",
      "39(3):930945.\n",
      "Christos Baziotis, Barry Haddow, and Alexandra\n",
      "Birch. 2020. Language model prior for low-\n",
      "resource neural machine translation. arXiv preprint\n",
      "arXiv:2004.14928 .Yonatan Belinkov and Yonatan Bisk. 2018. Synthetic\n",
      "and natural noise both break neural machine transla-\n",
      "tion. In International Conference on Learning Rep-\n",
      "resentations .\n",
      "Steven Bird. 2020. Decolonising speech and lan-\n",
      "guage technology. In Proceedings of the 28th Inter-\n",
      "national Conference on Computational Linguistics ,\n",
      "pages 35043519, Barcelona, Spain (Online). Inter-\n",
      "national Committee on Computational Linguistics.\n",
      "Graeme Blackwood, Miguel Ballesteros, and Todd\n",
      "Ward. 2018. Multilingual neural machine transla-\n",
      "tion with task-specific attention. In Proceedings of\n",
      "the 27th International Conference on Computational\n",
      "Linguistics , pages 31123122.\n",
      "Piotr Bojanowski, Edouard Grave, Armand Joulin, and\n",
      "Tomas Mikolov. 2017. Enriching word vectors with\n",
      "subword information. Transactions of the Associa-\n",
      "tion for Computational Linguistics , 5:135146.\n",
      "Marcel Bollmann, Rahul Aralikatte, Hctor Murri-\n",
      "eta Bello, Daniel Hershcovich, Miryam de Lhoneux,\n",
      "and Anders Sgaard. 2021. Moses and the\n",
      "character-based random babbling baseline:\n",
      "CoAStaL at AmericasNLP 2021 shared task.\n",
      "InProceedings of the First Workshop on Natural\n",
      "Language Processing for Indigenous Languages of\n",
      "the Americas , pages 248254, Online. Association\n",
      "for Computational Linguistics.\n",
      "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\n",
      "Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\n",
      "Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n",
      "Askell, Sandhini Agarwal, Ariel Herbert-V oss,\n",
      "Gretchen Krueger, Tom Henighan, Rewon Child,\n",
      "Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\n",
      "Clemens Winter, Christopher Hesse, Mark Chen,\n",
      "Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\n",
      "Chess, Jack Clark, Christopher Berner, Sam Mc-\n",
      "Candlish, Alec Radford, Ilya Sutskever, and Dario\n",
      "Amodei. 2020. Language models are few-shot\n",
      "learners.\n",
      "Gina Bustamante, Arturo Oncevay, and Roberto\n",
      "Zariquiey. 2020. No data to crawl? monolingual\n",
      "corpus creation from PDF files of truly low-resource\n",
      "languages in Peru. In Proceedings of the 12th Lan-\n",
      "guage Resources and Evaluation Conference , pages\n",
      "29142923, Marseille, France. European Language\n",
      "Resources Association.\n",
      "Lyle Campbell. 2000. American Indian languages: the\n",
      "historical linguistics of Native America , volume 4.\n",
      "Oxford University Press on Demand.\n",
      "Rich Caruana. 1997. Multitask learning. Machine\n",
      "learning , 28(1):4175.\n",
      "Isaac Caswell, Ciprian Chelba, and David Grangier.\n",
      "2019. Tagged back-translation. In Proceedings of\n",
      "the Fourth Conference on Machine Translation (Vol-\n",
      "ume 1: Research Papers) , pages 5363.Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao.\n",
      "2020. Evaluation of text generation: A survey.\n",
      "arXiv preprint arXiv:2006.14799 .\n",
      "zlem etino glu. 2017. A code-switching corpus of\n",
      "Turkish-German conversations. In Proceedings of\n",
      "the 11th Linguistic Annotation Workshop , pages 34\n",
      "40, Valencia, Spain. Association for Computational\n",
      "Linguistics.\n",
      "Bharathi Raja Chakravarthi, Ruba Priyadharshini,\n",
      "Shubhanker Banerjee, Richard Saldanha, John P.\n",
      "McCrae, Anand Kumar M, Parameswari Krishna-\n",
      "murthy, and Melvin Johnson. 2021. Findings of the\n",
      "shared task on machine translation in Dravidian lan-\n",
      "guages. In Proceedings of the First Workshop on\n",
      "Speech and Language Technologies for Dravidian\n",
      "Languages , pages 119125, Kyiv. Association for\n",
      "Computational Linguistics.\n",
      "Eirini Chatzikoumi. 2020. How to evaluate machine\n",
      "translation: A review of automated and human met-\n",
      "rics. Natural Language Engineering , 26(2):137\n",
      "161.\n",
      "Guanhua Chen, Shuming Ma, Yun Chen, Li Dong,\n",
      "Dongdong Zhang, Jia Pan, Wenping Wang, and Furu\n",
      "Wei. 2021. Zero-shot cross-lingual transfer of neu-\n",
      "ral machine translation with multilingual pretrained\n",
      "encoders. In Proceedings of the 2021 Conference on\n",
      "Empirical Methods in Natural Language Process-\n",
      "ing, pages 1526, Online and Punta Cana, Domini-\n",
      "can Republic. Association for Computational Lin-\n",
      "guistics.\n",
      "Yong Cheng. 2019. Joint training for pivot-based neu-\n",
      "ral machine translation. In Joint Training for Neural\n",
      "Machine Translation , pages 4154. Springer.\n",
      "Yong Cheng, Lu Jiang, and Wolfgang Macherey. 2019.\n",
      "Robust neural machine translation with doubly ad-\n",
      "versarial inputs. In Proceedings of the 57th Annual\n",
      "Meeting of the Association for Computational Lin-\n",
      "guistics , pages 43244333.\n",
      "Yong Cheng, Lu Jiang, Wolfgang Macherey, and Ja-\n",
      "cob Eisenstein. 2020. AdvAug: Robust adversar-\n",
      "ial augmentation for neural machine translation. In\n",
      "Proceedings of the 58th Annual Meeting of the Asso-\n",
      "ciation for Computational Linguistics , pages 5961\n",
      "5970, Online. Association for Computational Lin-\n",
      "guistics.\n",
      "Yong Cheng, Zhaopeng Tu, Fandong Meng, Junjie\n",
      "Zhai, and Yang Liu. 2018. Towards robust neural\n",
      "machine translation. In Proceedings of the 56th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics (Volume 1: Long Papers) , pages 1756\n",
      "1766.\n",
      "Luis Chiruzzo, Pedro Amarilla, Adolfo Ros, and Gus-\n",
      "tavo Gimnez Lugo. 2020. Development of a\n",
      "Guarani - Spanish parallel corpus. In Proceedings of\n",
      "the 12th Language Resources and Evaluation Con-\n",
      "ference , pages 26292633, Marseille, France. Euro-\n",
      "pean Language Resources Association.Luis Chiruzzo, Santiago Gngora, Aldo Alvarez, Gus-\n",
      "tavo Gimnez-Lugo, Marvin Agero-Torales, and\n",
      "Yliana Rodrguez. 2022. Jojajovai: A parallel\n",
      "guarani-spanish corpus for mt benchmarking. In\n",
      "Proceedings of the Thirteenth Language Resources\n",
      "and Evaluation Conference , pages 20982107.\n",
      "Kyunghyun Cho, Bart van Merrinboer, Caglar Gul-\n",
      "cehre, Dzmitry Bahdanau, Fethi Bougares, Holger\n",
      "Schwenk, and Yoshua Bengio. 2014. Learning\n",
      "phrase representations using rnn encoderdecoder\n",
      "for statistical machine translation. In Proceedings of\n",
      "the 2014 Conference on Empirical Methods in Nat-\n",
      "ural Language Processing (EMNLP) , pages 1724\n",
      "1734.\n",
      "Trevor Cohn and Mirella Lapata. 2007. Machine trans-\n",
      "lation by triangulation: Making effective use of\n",
      "multi-parallel corpora. In Proceedings of the 45th\n",
      "Annual Meeting of the Association of Computational\n",
      "Linguistics , pages 728735.\n",
      "Alexis Conneau and Guillaume Lample. 2019. Cross-\n",
      "lingual language model pretraining. In Advances\n",
      "in Neural Information Processing Systems , pages\n",
      "70577067.\n",
      "Alexis Conneau, Guillaume Lample, MarcAurelio\n",
      "Ranzato, Ludovic Denoyer, and Herv Jgou. 2017.\n",
      "Word translation without parallel data. arXiv\n",
      "preprint arXiv:1710.04087 .\n",
      "Raj Dabre, Chenhui Chu, and Anoop Kunchukuttan.\n",
      "2019. A survey of multilingual neural machine\n",
      "translation. arXiv preprint arXiv:1905.05395 .\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\n",
      "Kristina Toutanova. 2019. Bert: Pre-training of\n",
      "deep bidirectional transformers for language under-\n",
      "standing. In Proceedings of the 2019 Conference of\n",
      "the North American Chapter of the Association for\n",
      "Computational Linguistics: Human Language Tech-\n",
      "nologies, Volume 1 (Long and Short Papers) , pages\n",
      "41714186.\n",
      "Xiangyu Duan, Baijun Ji, Hao Jia, Min Tan, Min\n",
      "Zhang, Boxing Chen, Weihua Luo, and Yue Zhang.\n",
      "2020. Bilingual dictionary based neural machine\n",
      "translation without using parallel sentences. In Pro-\n",
      "ceedings of the 58th Annual Meeting of the Asso-\n",
      "ciation for Computational Linguistics , pages 1570\n",
      "1579.\n",
      "Abteen Ebrahim, Manuel Mager, Pavel Oncevay Ar-\n",
      "turo Danni Liu Koneru Sai Ugan Enes Yavuz\n",
      "Wiemerslage, Adam Denisov, Zhaolin Li, Jan\n",
      "Niehues, Monica Romero, Ivan G Torre, Tanel\n",
      "Alume, Jiaming Kong, Sergey Polezhaev, Yury\n",
      "Belousov, Wei-Rui Chen, Peter Sullivan, Ife\n",
      "Adebara, Bashar Talafha, Inciarte Alcides Al-\n",
      "coba, Muhammad Abdul-Mageed, Luis Chiruzzo,\n",
      "Rolando Coto-Solano, Hilaria Cruz, Sofa Flores-\n",
      "Solrzano, Aldo Andrs Alvarez Lpez, Ivan Meza-\n",
      "Ruiz, John E. Ortega, Alexis Palmer, Rodolfo Joel\n",
      "Zevallos Salazar, Kristine, Thang Vu Stenzel, andKatharina Kann. 2023. Findings of the second amer-\n",
      "icasnlp competition on speech-to-text translation.\n",
      "preprint .\n",
      "Abteen Ebrahimi, Manuel Mager, Arturo Once-\n",
      "vay, Vishrav Chaudhary, Luis Chiruzzo, Angela\n",
      "Fan, John Ortega, Ricardo Ramos, Annette Rios,\n",
      "Ivan Vladimir Meza Ruiz, Gustavo Gimnez-Lugo,\n",
      "Elisabeth Mager, Graham Neubig, Alexis Palmer,\n",
      "Rolando Coto-Solano, Thang Vu, and Katharina\n",
      "Kann. 2022. AmericasNLI: Evaluating zero-shot\n",
      "natural language understanding of pretrained multi-\n",
      "lingual models in truly low-resource languages. In\n",
      "Proceedings of the 60th Annual Meeting of the As-\n",
      "sociation for Computational Linguistics (Volume 1:\n",
      "Long Papers) , pages 62796299, Dublin, Ireland.\n",
      "Association for Computational Linguistics.\n",
      "Abteen Ebrahimi, Manuel Mager, Arturo Oncevay,\n",
      "Vishrav Chaudhary, Luis Chiruzzo, Angela Fan,\n",
      "John Ortega, Ricardo Ramos, Annette Rios, Ivan\n",
      "Vladimir, Gustavo A. Gimnez-Lugo, Elisabeth\n",
      "Mager, Graham Neubig, Alexis Palmer, Rolando\n",
      "A. Coto Solano, Ngoc Thang Vu, and Katharina\n",
      "Kann. 2021. Americasnli: Evaluating zero-shot nat-\n",
      "ural language understanding of pretrained multilin-\n",
      "gual models in truly low-resource languages.\n",
      "Abteen Ebrahimi, Manuel Mager, Arturo Oncevay,\n",
      "Enora Rice, Cynthia Montao, John Ortega, Shruti\n",
      "Rijhwani, Alexis Palmer, Rolando Coto-Solano, Hi-\n",
      "laria Cruz, and Katharina Kann. 2023. Findings\n",
      "of the AmericasNLP 2023 shared task on machine\n",
      "translation into indigenous languages. In Proceed-\n",
      "ings of the Third Workshop on Natural Language\n",
      "Processing for Indigenous Languages of the Amer-\n",
      "icas. Association for Computational Linguistics.\n",
      "Javid Ebrahimi, Daniel Lowd, and Dejing Dou. 2018.\n",
      "On adversarial examples for character-level neural\n",
      "machine translation. In Proceedings of the 27th In-\n",
      "ternational Conference on Computational Linguis-\n",
      "tics, pages 653663, Santa Fe, New Mexico, USA.\n",
      "Association for Computational Linguistics.\n",
      "Sergey Edunov, Alexei Baevski, and Michael Auli.\n",
      "2019. Pre-trained language model representations\n",
      "for language generation. In Proceedings of the 2019\n",
      "Conference of the North American Chapter of the\n",
      "Association for Computational Linguistics: Human\n",
      "Language Technologies, Volume 1 (Long and Short\n",
      "Papers) , pages 40524059.\n",
      "Sergey Edunov, Myle Ott, Michael Auli, and David\n",
      "Grangier. 2018. Understanding back-translation at\n",
      "scale. In Proceedings of the 2018 Conference on\n",
      "Empirical Methods in Natural Language Process-\n",
      "ing, pages 489500.\n",
      "Marzieh Fadaee, Arianna Bisazza, and Christof Monz.\n",
      "2017. Data augmentation for low-resource neural\n",
      "machine translation. In Proceedings of the 55th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics (Volume 2: Short Papers) , pages 567\n",
      "573.Isaac Feldman and Rolando Coto-Solano. 2020. Neu-\n",
      "ral machine translation models with back-translation\n",
      "for the extremely low-resource indigenous language\n",
      "Bribri. In Proceedings of the 28th International\n",
      "Conference on Computational Linguistics , pages\n",
      "39653976, Barcelona, Spain (Online). Interna-\n",
      "tional Committee on Computational Linguistics.\n",
      "Manuel Fernndez-Delgado, Eva Cernadas, Senn\n",
      "Barro, and Dinani Amorim. 2014. Do we need hun-\n",
      "dreds of classifiers to solve real world classification\n",
      "problems? The journal of machine learning re-\n",
      "search , 15(1):31333181.\n",
      "Alexander Fraser. 2020. Findings of the WMT 2020\n",
      "shared tasks in unsupervised MT and very low re-\n",
      "source supervised MT. In Proceedings of the Fifth\n",
      "Conference on Machine Translation , pages 765\n",
      "771, Online. Association for Computational Lin-\n",
      "guistics.\n",
      "Ana-Paula Galarreta, Andrs Melgar, and Arturo On-\n",
      "cevay. 2017. Corpus creation and initial SMT ex-\n",
      "periments between Spanish and Shipibo-konibo. In\n",
      "Proceedings of the International Conference Recent\n",
      "Advances in Natural Language Processing, RANLP\n",
      "2017 , pages 238244, Varna, Bulgaria. INCOMA\n",
      "Ltd.\n",
      "Candace Kaleimamoowahinekapu Galla. 2016. Indige-\n",
      "nous language revitalization, promotion, and educa-\n",
      "tion: Function of digital technology. Computer As-\n",
      "sisted Language Learning , 29(7):11371151.\n",
      "Xavier Garcia, Pierre Foret, Thibault Sellam, and\n",
      "Ankur P Parikh. 2020. A multilingual view of\n",
      "unsupervised machine translation. arXiv preprint\n",
      "arXiv:2002.02955 .\n",
      "Mozhdeh Gheini, Xiang Ren, and Jonathan May. 2021.\n",
      "Cross-attention is all you need: Adapting pretrained\n",
      "Transformers for machine translation. In Proceed-\n",
      "ings of the 2021 Conference on Empirical Methods\n",
      "in Natural Language Processing , pages 17541765,\n",
      "Online and Punta Cana, Dominican Republic. Asso-\n",
      "ciation for Computational Linguistics.\n",
      "Santiago Gngora, Nicols Giossa, and Luis Chiruzzo.\n",
      "2021. Experiments on a Guarani corpus of news\n",
      "and social media. In Proceedings of the First Work-\n",
      "shop on Natural Language Processing for Indige-\n",
      "nous Languages of the Americas , pages 153158,\n",
      "Online. Association for Computational Linguistics.\n",
      "Santiago Gngora, Nicols Giossa, and Luis Chiruzzo.\n",
      "2022. Can we use word embeddings for enhancing\n",
      "Guarani-Spanish machine translation? In Proceed-\n",
      "ings of the Fifth Workshop on the Use of Compu-\n",
      "tational Methods in the Study of Endangered Lan-\n",
      "guages , pages 127132, Dublin, Ireland. Associa-\n",
      "tion for Computational Linguistics.\n",
      "Yvette Graham, Barry Haddow, and Philipp Koehn.\n",
      "2020. Statistical power and translationese in ma-\n",
      "chine translation evaluation. In Proceedings of the2020 Conference on Empirical Methods in Natural\n",
      "Language Processing (EMNLP) , pages 7281, On-\n",
      "line. Association for Computational Linguistics.\n",
      "Stig-Arne Grnroos, Sami Virpioja, Peter Smit, and\n",
      "Mikko Kurimo. 2014. Morfessor flatcat: An hmm-\n",
      "based method for unsupervised and semi-supervised\n",
      "learning of morphology. In Proceedings of COLING\n",
      "2014, the 25th International Conference on Compu-\n",
      "tational Linguistics: Technical Papers , pages 1177\n",
      "1185.\n",
      "Jiatao Gu, Yong Wang, Kyunghyun Cho, and Vic-\n",
      "tor OK Li. 2019. Improved zero-shot neural ma-\n",
      "chine translation via ignoring spurious correlations.\n",
      "InProceedings of the 57th Annual Meeting of the\n",
      "Association for Computational Linguistics , pages\n",
      "12581268.\n",
      "Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun\n",
      "Cho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,\n",
      "Holger Schwenk, and Yoshua Bengio. 2015. On us-\n",
      "ing monolingual corpora in neural machine transla-\n",
      "tion. arXiv preprint arXiv:1503.03535 .\n",
      "Ximena Gutierrez-Vasques, Gerardo Sierra, and\n",
      "Isaac Hernandez Pompa. 2016. Axolotl: a web\n",
      "accessible parallel corpus for Spanish-Nahuatl. In\n",
      "Proceedings of the Tenth International Conference\n",
      "on Language Resources and Evaluation (LREC16) ,\n",
      "pages 42104214, Portoro, Slovenia. European\n",
      "Language Resources Association (ELRA).\n",
      "Barry Haddow, Rachel Bawden, Antonio Valerio\n",
      "Miceli Barone, Jind rich Helcl, and Alexandra Birch.\n",
      "2022. Survey of low-resource machine translation.\n",
      "Computational Linguistics , pages 167.\n",
      "Lifeng Han. 2016. Machine translation evaluation re-\n",
      "sources and methods: A survey. arXiv preprint\n",
      "arXiv:1605.04515 .\n",
      "Franois Hernandez and Vincent Nguyen. 2020. The\n",
      "ubiqus English-Inuktitut system for WMT20. In\n",
      "Proceedings of the Fifth Conference on Machine\n",
      "Translation , pages 213217, Online. Association for\n",
      "Computational Linguistics.\n",
      "Nikolaus P Himmelmann. 2008. Language documen-\n",
      "tation: What is it and what is it good for? In Es-\n",
      "sentials of language documentation , pages 130. De\n",
      "Gruyter Mouton.\n",
      "Vu Cong Duy Hoang, Philipp Koehn, Gholamreza\n",
      "Haffari, and Trevor Cohn. 2018. Iterative back-\n",
      "translation for neural machine translation. In Pro-\n",
      "ceedings of the 2nd Workshop on Neural Machine\n",
      "Translation and Generation , pages 1824.\n",
      "Sepp Hochreiter and Jrgen Schmidhuber. 1997.\n",
      "Long short-term memory. Neural computation ,\n",
      "9(8):17351780.\n",
      "J Edward Hu, Huda Khayrallah, Ryan Culkin, Patrick\n",
      "Xia, Tongfei Chen, Matt Post, and Benjamin\n",
      "Van Durme. 2019. Improved lexically constraineddecoding for translation and monolingual rewriting.\n",
      "InProceedings of the 2019 Conference of the North\n",
      "American Chapter of the Association for Computa-\n",
      "tional Linguistics: Human Language Technologies,\n",
      "Volume 1 (Long and Short Papers) , pages 839850.\n",
      "W John Hutchins. 2004. The georgetown-ibm experi-\n",
      "ment demonstrated in january 1954. In Conference\n",
      "of the Association for Machine Translation in the\n",
      "Americas , pages 102114. Springer.\n",
      "Sbastien Jean, Kyunghyun Cho, Roland Memisevic,\n",
      "and Yoshua Bengio. 2015. On using very large\n",
      "target vocabulary for neural machine translation.\n",
      "InProceedings of the 53rd Annual Meeting of the\n",
      "Association for Computational Linguistics and the\n",
      "7th International Joint Conference on Natural Lan-\n",
      "guage Processing (Volume 1: Long Papers) , pages\n",
      "110, Beijing, China. Association for Computa-\n",
      "tional Linguistics.\n",
      "Eric Joanis, Rebecca Knowles, Roland Kuhn, Samuel\n",
      "Larkin, Patrick Littell, Chi-kiu Lo, Darlene Stewart,\n",
      "and Jeffrey Micher. 2020. The Nunavut Hansard\n",
      "InuktitutEnglish parallel corpus 3.0 with prelimi-\n",
      "nary machine translation results. In Proceedings of\n",
      "the 12th Language Resources and Evaluation Con-\n",
      "ference , pages 25622572, Marseille, France. Euro-\n",
      "pean Language Resources Association.\n",
      "Melvin Johnson, Mike Schuster, Quoc V Le, Maxim\n",
      "Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\n",
      "Fernanda Vigas, Martin Wattenberg, Greg Corrado,\n",
      "et al. 2017. Googles multilingual neural machine\n",
      "translation system: Enabling zero-shot translation.\n",
      "Transactions of the Association for Computational\n",
      "Linguistics , 5:339351.\n",
      "David Kamholz, Jonathan Pool, and Susan M Colow-\n",
      "ick. 2014. Panlex: Building a resource for panlin-\n",
      "gual lexical translation. In LREC , pages 31453150.\n",
      "Katharina Kann, Jesus Manuel Mager Hois,\n",
      "Ivan Vladimir Meza-Ruiz, and Hinrich Schtze.\n",
      "2018. Fortification of neural morphological\n",
      "segmentation models for polysynthetic minimal-\n",
      "resource languages. In Proceedings of the 2018\n",
      "Conference of the North American Chapter of the\n",
      "Association for Computational Linguistics: Human\n",
      "Language Technologies, Volume 1 (Long Papers) ,\n",
      "pages 4757, New Orleans, Louisiana. Association\n",
      "for Computational Linguistics.\n",
      "Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua\n",
      "Bengio. 2017. Generalization in deep learning.\n",
      "arXiv preprint arXiv:1710.05468 .\n",
      "Huda Khayrallah, Brian Thompson, Matt Post, and\n",
      "Philipp Koehn. 2020. Simulated multiple reference\n",
      "training improves low-resource machine translation.\n",
      "arXiv preprint arXiv:2004.14524 .\n",
      "Rebecca Knowles, Darlene Stewart, Samuel Larkin,\n",
      "and Patrick Littell. 2020. NRC systems for the 2020\n",
      "Inuktitut-English news translation task. In Proceed-\n",
      "ings of the Fifth Conference on Machine Translation ,pages 156170, Online. Association for Computa-\n",
      "tional Linguistics.\n",
      "Rebecca Knowles, Darlene Stewart, Samuel Larkin,\n",
      "and Patrick Littell. 2021. NRC-CNRC machine\n",
      "translation systems for the 2021 AmericasNLP\n",
      "shared task. In Proceedings of the First Workshop on\n",
      "Natural Language Processing for Indigenous Lan-\n",
      "guages of the Americas , pages 224233, Online. As-\n",
      "sociation for Computational Linguistics.\n",
      "Sosuke Kobayashi. 2018. Contextual augmentation:\n",
      "Data augmentation by words with paradigmatic re-\n",
      "lations. In Proceedings of the 2018 Conference of\n",
      "the North American Chapter of the Association for\n",
      "Computational Linguistics: Human Language Tech-\n",
      "nologies, Volume 2 (Short Papers) , pages 452457.\n",
      "Tom Kocmi. 2020. CUNI submission for the Inuk-\n",
      "titut language in WMT news 2020. In Proceed-\n",
      "ings of the Fifth Conference on Machine Translation ,\n",
      "pages 171174, Online. Association for Computa-\n",
      "tional Linguistics.\n",
      "Michael Krauss. 1992. The worlds languages in crisis.\n",
      "Language , 68(1):410.\n",
      "Mateusz Krubi nski, Marcin Chochowski, Bartomiej\n",
      "Boczek, Mikoaj Koszowski, Adam Dobrowolski,\n",
      "Marcin Szyma nski, and Pawe Przybysz. 2020.\n",
      "Samsung R&D institute Poland submission to\n",
      "WMT20 news translation task. In Proceedings of\n",
      "the Fifth Conference on Machine Translation , pages\n",
      "181190, Online. Association for Computational\n",
      "Linguistics.\n",
      "Surafel M Lakew, Quintino F Lotito, Matteo Negri,\n",
      "Marco Turchi, and Marcello Federico. 2018. Im-\n",
      "proving zero-shot translation of low-resource lan-\n",
      "guages. In Proceedings of the 14h IWSLT , pages\n",
      "113119.\n",
      "Guillaume Lample, Alexis Conneau, Ludovic Denoyer,\n",
      "and MarcAurelio Ranzato. 2017. Unsupervised\n",
      "machine translation using monolingual corpora only.\n",
      "arXiv preprint arXiv:1711.00043 .\n",
      "Sahinur Rahman Laskar, Abdullah Faiz Ur Rah-\n",
      "man Khilji, Partha Pakray, and Sivaji Bandyopad-\n",
      "hyay. 2020. Zero-shot neural machine translation:\n",
      "Russian-Hindi @LoResMT 2020. In Proceedings\n",
      "of the 3rd Workshop on Technologies for MT of Low\n",
      "Resource Languages , pages 3842, Suzhou, China.\n",
      "Association for Computational Linguistics.\n",
      "Yichong Leng, Xu Tan, Tao Qin, Xiang-Yang Li, and\n",
      "Tie-Yan Liu. 2019. Unsupervised pivot translation\n",
      "for distant languages. In Proceedings of the 57th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics , pages 175183.\n",
      "M Paul Lewis. 2009. Ethnologue: Languages of the\n",
      "world . SIL international.\n",
      "M Paul Lewis and Gary F Simons. 2010. Assessing\n",
      "endangerment: expanding fishmans gids. Revue\n",
      "roumaine de linguistique , 55(2):103120.Mike Lewis, Yinhan Liu, Naman Goyal, Mar-\n",
      "jan Ghazvininejad, Abdelrahman Mohamed, Omer\n",
      "Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\n",
      "Bart: Denoising sequence-to-sequence pre-training\n",
      "for natural language generation, translation, and\n",
      "comprehension. arXiv preprint arXiv:1910.13461 .\n",
      "Zuchao Li, Rui Wang, Kehai Chen, Masso Utiyama,\n",
      "Eiichiro Sumita, Zhuosheng Zhang, and Hai Zhao.\n",
      "2020. Data-dependent gaussian prior objective for\n",
      "language generation. In International Conference\n",
      "on Learning Representations .\n",
      "Zehui Lin, Xiao Pan, Mingxuan Wang, Xipeng Qiu,\n",
      "Jiangtao Feng, Hao Zhou, and Lei Li. 2020. Pre-\n",
      "training multilingual neural machine translation by\n",
      "leveraging alignment information. In Proceed-\n",
      "ings of the 2020 Conference on Empirical Methods\n",
      "in Natural Language Processing (EMNLP) , pages\n",
      "26492663, Online. Association for Computational\n",
      "Linguistics.\n",
      "Wang Ling, Isabel Trancoso, Chris Dyer, and Alan W\n",
      "Black. 2015. Character-based neural machine trans-\n",
      "lation. arXiv preprint arXiv:1511.04586 .\n",
      "Patrick Littell, Anna Kazantseva, Roland Kuhn, Aidan\n",
      "Pine, Antti Arppe, Christopher Cox, and Marie-\n",
      "Odile Junker. 2018. Indigenous language technolo-\n",
      "gies in Canada: Assessment, challenges, and suc-\n",
      "cesses. In Proceedings of the 27th International\n",
      "Conference on Computational Linguistics , pages\n",
      "26202632, Santa Fe, New Mexico, USA. Associ-\n",
      "ation for Computational Linguistics.\n",
      "Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\n",
      "Edunov, Marjan Ghazvininejad, Mike Lewis, and\n",
      "Luke Zettlemoyer. 2020. Multilingual denoising\n",
      "pre-training for neural machine translation. arXiv\n",
      "preprint arXiv:2001.08210 .\n",
      "Zihan Liu, Yan Xu, Genta Indra Winata, and Pascale\n",
      "Fung. 2019. Incorporating word and subword units\n",
      "in unsupervised machine translation using language\n",
      "model rescoring. In Proceedings of the Fourth Con-\n",
      "ference on Machine Translation (Volume 2: Shared\n",
      "Task Papers, Day 1) , pages 275282.\n",
      "Zoey Liu, Crystal Richardson, Richard Hatcher Jr, and\n",
      "Emily Prudhommeaux. 2022. Not always about\n",
      "you: Prioritizing community needs when develop-\n",
      "ing endangered language technology. arXiv preprint\n",
      "arXiv:2204.05541 .\n",
      "Chi-kiu Lo. 2020. Extended study on using pretrained\n",
      "language models and YiSi-1 for machine transla-\n",
      "tion evaluation. In Proceedings of the Fifth Confer-\n",
      "ence on Machine Translation , pages 895902, On-\n",
      "line. Association for Computational Linguistics.\n",
      "Wolfgang Lrscher. 2005. The translation process:\n",
      "Methods and problems of its investigation. Meta:\n",
      "journal des traducteurs/Meta: Translators Journal ,\n",
      "50(2):597608.Bruce T Lowerre. 1976. The harpy speech recognition\n",
      "system. Technical report, CARNEGIE-MELLON\n",
      "UNIV PITTSBURGH PA DEPT OF COMPUTER\n",
      "SCIENCE.\n",
      "Yichao Lu, Phillip Keung, Faisal Ladhak, Vikas Bhard-\n",
      "waj, Shaonan Zhang, and Jason Sun. 2018. A neu-\n",
      "ral interlingua for multilingual machine translation.\n",
      "InProceedings of the Third Conference on Machine\n",
      "Translation: Research Papers , pages 8492.\n",
      "Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals,\n",
      "and Wojciech Zaremba. 2015. Addressing the rare\n",
      "word problem in neural machine translation. In Pro-\n",
      "ceedings of the 53rd Annual Meeting of the Associ-\n",
      "ation for Computational Linguistics and the 7th In-\n",
      "ternational Joint Conference on Natural Language\n",
      "Processing (Volume 1: Long Papers) , pages 1119,\n",
      "Beijing, China. Association for Computational Lin-\n",
      "guistics.\n",
      "Manuel Mager, Dinico Carrillo, and Ivan Meza.\n",
      "2018a. Probabilistic finite-state morphological seg-\n",
      "menter for wixarika (huichol) language. Journal of\n",
      "Intelligent & Fuzzy Systems , 34(5):30813087.\n",
      "Manuel Mager, zlem etino glu, and Katharina Kann.\n",
      "2019. Subword-level language identification for\n",
      "intra-word code-switching. In Proceedings of the\n",
      "2019 Conference of the North American Chapter of\n",
      "the Association for Computational Linguistics: Hu-\n",
      "man Language Technologies, Volume 1 (Long and\n",
      "Short Papers) , pages 20052011, Minneapolis, Min-\n",
      "nesota. Association for Computational Linguistics.\n",
      "Manuel Mager, zlem etino glu, and Katharina\n",
      "Kann. 2020. Tackling the low-resource chal-\n",
      "lenge for canonical segmentation. arXiv preprint\n",
      "arXiv:2010.02804 .\n",
      "Manuel Mager, Ximena Gutierrez-Vasques, Gerardo\n",
      "Sierra, and Ivan Meza-Ruiz. 2018b. Challenges of\n",
      "language technologies for the indigenous languages\n",
      "of the Americas. In Proceedings of the 27th Inter-\n",
      "national Conference on Computational Linguistics ,\n",
      "pages 5569, Santa Fe, New Mexico, USA. Associ-\n",
      "ation for Computational Linguistics.\n",
      "Manuel Mager, Elisabeth Mager, Katharina Kann,\n",
      "and Ngoc Thang Vu. 2023. Ethical considerations\n",
      "for machine translation of indigenous languages:\n",
      "Giving a voice to the speakers. arXiv preprint\n",
      "arXiv:2305.19474 .\n",
      "Manuel Mager, Elisabeth Mager, Alfonso Medina-\n",
      "Urrea, Ivan Vladimir Meza Ruiz, and Katharina\n",
      "Kann. 2018c. Lost in translation: Analysis of in-\n",
      "formation loss during machine translation between\n",
      "polysynthetic and fusional languages. In Proceed-\n",
      "ings of the Workshop on Computational Modeling\n",
      "of Polysynthetic Languages , pages 7383, Santa Fe,\n",
      "New Mexico, USA. Association for Computational\n",
      "Linguistics.Manuel Mager and Ivan Meza. 2021. Retos en con-\n",
      "struccin de traductores automticos para lenguas\n",
      "indgenas de Mxico. Digital Scholarship in the Hu-\n",
      "manities , 36.\n",
      "Manuel Mager, Arturo Oncevay, Abteen Ebrahimi,\n",
      "John Ortega, Annette Rios, Angela Fan, Xi-\n",
      "mena Gutierrez-Vasques, Luis Chiruzzo, Gustavo\n",
      "Gimnez-Lugo, Ricardo Ramos, Anna Currey,\n",
      "Vishrav Chaudhary, Ivan Vladimir Meza Ruiz,\n",
      "Rolando Coto-Solano, Alexis Palmer, Elisabeth\n",
      "Mager, Ngoc Thang Vu, Graham Neubig, and\n",
      "Katharina Kann. 2021. Findings of the Americas-\n",
      "NLP 2021 Shared Task on Open Machine Transla-\n",
      "tion for Indigenous Languages of the Americas. In\n",
      "Proceedings of theThe First Workshop on NLP for\n",
      "Indigenous Languages of the Americas , Online. As-\n",
      "sociation for Computational Linguistics.\n",
      "Manuel Mager, Arturo Oncevay, Elisabeth Mager,\n",
      "Katharina Kann, and Thang Vu. 2022. BPE vs. mor-\n",
      "phological segmentation: A case study on machine\n",
      "translation of four polysynthetic languages. In Find-\n",
      "ings of the Association for Computational Linguis-\n",
      "tics: ACL 2022 , pages 961971, Dublin, Ireland.\n",
      "Association for Computational Linguistics.\n",
      "Chaitanya Malaviya, Graham Neubig, and Patrick Lit-\n",
      "tell. 2017. Learning language representations for ty-\n",
      "pology prediction. In Proceedings of the 2017 Con-\n",
      "ference on Empirical Methods in Natural Language\n",
      "Processing , pages 25292535.\n",
      "Benjamin Marie, Raphael Rubino, and Atsushi Fujita.\n",
      "2020. Tagged back-translation revisited: Why does\n",
      "it really work? In Proceedings of the 58th Annual\n",
      "Meeting of the Association for Computational Lin-\n",
      "guistics , pages 59905997, Online. Association for\n",
      "Computational Linguistics.\n",
      "Joel Martin, Howard Johnson, Benoit Farley, and Anna\n",
      "Maclachlan. 2003. Aligning and using an english-\n",
      "inuktitut parallel corpus. In Proceedings of the HLT-\n",
      "NAACL 2003 Workshop on Building and using par-\n",
      "allel texts: data driven machine translation and\n",
      "beyond-Volume 3 , pages 115118. Association for\n",
      "Computational Linguistics.\n",
      "Thomas Mayer and Michael Cysouw. 2014. Creat-\n",
      "ing a massively parallel bible corpus. Oceania ,\n",
      "135(273):40.\n",
      "Arya D. McCarthy, Rachel Wicks, Dylan Lewis,\n",
      "Aaron Mueller, Winston Wu, Oliver Adams, Gar-\n",
      "rett Nicolai, Matt Post, and David Yarowsky. 2020.\n",
      "The johns hopkins university bible corpus: 1600+\n",
      "tongues for typological exploration. In Proceedings\n",
      "of The 12th Language Resources and Evaluation\n",
      "Conference , pages 28842892, Marseille, France.\n",
      "European Language Resources Association.\n",
      "Norman A McQuown. 1955. The indigenous lan-\n",
      "guages of latin america. American Anthropologist ,\n",
      "57(3):501570.Antonio Valerio Miceli-Barone, Jind rich Helcl, Rico\n",
      "Sennrich, Barry Haddow, and Alexandra Birch.\n",
      "2017. Deep architectures for neural machine trans-\n",
      "lation. In Proceedings of the Second Conference on\n",
      "Machine Translation , pages 99107.\n",
      "Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-\n",
      "frey Dean. 2013. Efficient estimation of word\n",
      "representations in vector space. arXiv preprint\n",
      "arXiv:1301.3781 .\n",
      "Marianne Mithun. 1986. On the nature of noun incor-\n",
      "poration. Language , 62(1):3237.\n",
      "Oscar Moreno. 2021. The REPU CS Spanish\n",
      "Quechua submission to the AmericasNLP 2021\n",
      "shared task on open machine translation. In Pro-\n",
      "ceedings of the First Workshop on Natural Language\n",
      "Processing for Indigenous Languages of the Ameri-\n",
      "cas, pages 241247, Online. Association for Com-\n",
      "putational Linguistics.\n",
      "El Moatez Billah Nagoudi, Wei-Rui Chen, Muham-\n",
      "mad Abdul-Mageed, and Hasan Cavusoglu. 2021.\n",
      "IndT5: A text-to-text transformer for 10 indigenous\n",
      "languages. In Proceedings of the First Workshop on\n",
      "Natural Language Processing for Indigenous Lan-\n",
      "guages of the Americas , pages 265271, Online. As-\n",
      "sociation for Computational Linguistics.\n",
      "Wilhelmina Nekoto, Vukosi Marivate, Tshinondiwa\n",
      "Matsila, Timi Fasubaa, Taiwo Fagbohungbe,\n",
      "Solomon Oluwole Akinola, Shamsuddeen Muham-\n",
      "mad, Salomon Kabongo Kabenamualu, Salomey\n",
      "Osei, Freshia Sackey, Rubungo Andre Niyongabo,\n",
      "Ricky Macharm, Perez Ogayo, Orevaoghene Ahia,\n",
      "Musie Meressa Berhe, Mofetoluwa Adeyemi,\n",
      "Masabata Mokgesi-Selinga, Lawrence Okegbemi,\n",
      "Laura Martinus, Kolawole Tajudeen, Kevin Degila,\n",
      "Kelechi Ogueji, Kathleen Siminyu, Julia Kreutzer,\n",
      "Jason Webster, Jamiil Toure Ali, Jade Abbott,\n",
      "Iroro Orife, Ignatius Ezeani, Idris Abdulkadir\n",
      "Dangana, Herman Kamper, Hady Elsahar, Good-\n",
      "ness Duru, Ghollah Kioko, Murhabazi Espoir,\n",
      "Elan van Biljon, Daniel Whitenack, Christopher\n",
      "Onyefuluchi, Chris Chinenye Emezue, Bonaventure\n",
      "F. P. Dossou, Blessing Sibanda, Blessing Bassey,\n",
      "Ayodele Olabiyi, Arshath Ramkilowan, Alp ktem,\n",
      "Adewale Akinfaderin, and Abdallah Bashir. 2020.\n",
      "Participatory research for low-resourced machine\n",
      "translation: A case study in African languages.\n",
      "InFindings of the Association for Computational\n",
      "Linguistics: EMNLP 2020 , pages 21442160,\n",
      "Online. Association for Computational Linguistics.\n",
      "Graham Neubig. 2017. Neural machine translation\n",
      "and sequence-to-sequence models: A tutorial. arXiv\n",
      "preprint arXiv:1703.01619 .\n",
      "Graham Neubig and Junjie Hu. 2018. Rapid adapta-\n",
      "tion of neural machine translation to new languages.\n",
      "InProceedings of the 2018 Conference on Empiri-\n",
      "cal Methods in Natural Language Processing , pages\n",
      "875880.Tan Ngoc Le and Fatiha Sadat. 2020. Revitalization\n",
      "of indigenous languages through pre-processing and\n",
      "neural machine translation: The case of Inuktitut.\n",
      "InProceedings of the 28th International Conference\n",
      "on Computational Linguistics , pages 46614666,\n",
      "Barcelona, Spain (Online). International Committee\n",
      "on Computational Linguistics.\n",
      "Dat Quoc Nguyen, Kairit Sirts, and Mark Johnson.\n",
      "2015. Improving topic coherence with latent fea-\n",
      "ture word representations in MAP estimation for\n",
      "topic modeling. In Proceedings of the Australasian\n",
      "Language Technology Association Workshop 2015 ,\n",
      "pages 116121, Parramatta, Australia.\n",
      "Toan Q Nguyen and David Chiang. 2017. Trans-\n",
      "fer learning across low-resource, related languages\n",
      "for neural machine translation. In Proceedings of\n",
      "the Eighth International Joint Conference on Natu-\n",
      "ral Language Processing (Volume 2: Short Papers) ,\n",
      "pages 296301.\n",
      "Eugene Nida. 1945. Linguistics and ethnology in\n",
      "translation-problems. Word , 1(2):194208.\n",
      "Jan Niehues and Eunah Cho. 2017. Exploiting linguis-\n",
      "tic resources for neural machine translation using\n",
      "multi-task learning. In Proceedings of the Second\n",
      "Conference on Machine Translation , pages 8089,\n",
      "Copenhagen, Denmark. Association for Computa-\n",
      "tional Linguistics.\n",
      "Jan Niehues, Eunah Cho, Thanh-Le Ha, and Alex\n",
      "Waibel. 2016. Pre-translation for neural machine\n",
      "translation. In Proceedings of COLING 2016, the\n",
      "26th International Conference on Computational\n",
      "Linguistics: Technical Papers , pages 18281836,\n",
      "Osaka, Japan. The COLING 2016 Organizing Com-\n",
      "mittee.\n",
      "Farhad Nooralahzadeh, Giannis Bekoulis, Johannes\n",
      "Bjerva, and Isabelle Augenstein. 2020. Zero-shot\n",
      "cross-lingual transfer with meta learning. arXiv\n",
      "preprint arXiv:2003.02739 .\n",
      "Atul Kr. Ojha, Valentin Malykh, Alina Karakanta, and\n",
      "Chao-Hong Liu. 2020. Findings of the LoResMT\n",
      "2020 shared task on zero-shot for low-resource lan-\n",
      "guages. In Proceedings of the 3rd Workshop on\n",
      "Technologies for MT of Low Resource Languages ,\n",
      "pages 3337, Suzhou, China. Association for Com-\n",
      "putational Linguistics.\n",
      "Matthew Olson, Abraham Wyner, and Richard Berk.\n",
      "2018. Modern neural networks generalize on small\n",
      "data sets. In Advances in Neural Information Pro-\n",
      "cessing Systems , pages 36193628.\n",
      "Arturo Oncevay. 2021. Peru is multilingual, its ma-\n",
      "chine translation should be too? In Proceedings\n",
      "of the First Workshop on Natural Language Pro-\n",
      "cessing for Indigenous Languages of the Americas ,\n",
      "pages 194201, Online. Association for Computa-\n",
      "tional Linguistics.Iroro Orife, Julia Kreutzer, Blessing Sibanda, Daniel\n",
      "Whitenack, Kathleen Siminyu, Laura Martinus,\n",
      "Jamiil Toure Ali, Jade Abbott, Vukosi Marivate,\n",
      "Salomon Kabongo, et al. 2020. Masakhane\n",
      "machine translation for africa. arXiv preprint\n",
      "arXiv:2003.11529 .\n",
      "John E Ortega, Richard Castro Mamani, and\n",
      "Kyunghyun Cho. 2020a. Neural machine translation\n",
      "with a polysynthetic low resource language. Ma-\n",
      "chine Translation , 34(4):325346.\n",
      "John E Ortega, Richard Alexander Castro-Mamani, and\n",
      "Jaime Rafael Montoya Samame. 2020b. Overcom-\n",
      "ing resistance: The normalization of an Amazonian\n",
      "tribal language. In Proceedings of the 3rd Work-\n",
      "shop on Technologies for MT of Low Resource Lan-\n",
      "guages , pages 113, Suzhou, China. Association for\n",
      "Computational Linguistics.\n",
      "Yirong Pan, Xiao Li, Yating Yang, and Rui Dong. 2020.\n",
      "Multi-task neural model for agglutinative language\n",
      "translation. In Proceedings of the 58th Annual Meet-\n",
      "ing of the Association for Computational Linguis-\n",
      "tics: Student Research Workshop , pages 103110,\n",
      "Online. Association for Computational Linguistics.\n",
      "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-\n",
      "Jing Zhu. 2002. Bleu: a method for automatic eval-\n",
      "uation of machine translation. In Proceedings of the\n",
      "40th Annual Meeting of the Association for Com-\n",
      "putational Linguistics , pages 311318, Philadelphia,\n",
      "Pennsylvania, USA. Association for Computational\n",
      "Linguistics.\n",
      "Shantipriya Parida, Subhadarshi Panda, Amulya Dash,\n",
      "Esau Villatoro-Tello, A. Seza Do gruz, Rosa M.\n",
      "Ortega-Mendoza, Amadeo Hernndez, Yashvardhan\n",
      "Sharma, and Petr Motlicek. 2021. Open machine\n",
      "translation for low resource South American lan-\n",
      "guages (AmericasNLP 2021 shared task contribu-\n",
      "tion). In Proceedings of the First Workshop on Natu-\n",
      "ral Language Processing for Indigenous Languages\n",
      "of the Americas , pages 218223, Online. Associa-\n",
      "tion for Computational Linguistics.\n",
      "Jeffrey Pennington, Richard Socher, and Christopher\n",
      "Manning. 2014. GloVe: Global vectors for word\n",
      "representation. In Proceedings of the 2014 Con-\n",
      "ference on Empirical Methods in Natural Language\n",
      "Processing (EMNLP) , pages 15321543, Doha,\n",
      "Qatar. Association for Computational Linguistics.\n",
      "Asya Pereltsvaig. 2020. Languages of the World .\n",
      "Cambridge University Press.\n",
      "Matthew Peters, Mark Neumann, Mohit Iyyer, Matt\n",
      "Gardner, Christopher Clark, Kenton Lee, and Luke\n",
      "Zettlemoyer. 2018. Deep contextualized word rep-\n",
      "resentations. In Proceedings of the 2018 Confer-\n",
      "ence of the North American Chapter of the Associ-\n",
      "ation for Computational Linguistics: Human Lan-\n",
      "guage Technologies, Volume 1 (Long Papers) , pages\n",
      "22272237.Alberto Poncelas, Maja Popovi c, Dimitar Shterionov,\n",
      "Gideon Maillette de Buy Wenniger, and Andy Way.\n",
      "2019. Combining pbsmt and nmt back-translated\n",
      "data for efficient nmt. In Proceedings of the Inter-\n",
      "national Conference on Recent Advances in Natural\n",
      "Language Processing (RANLP 2019) , pages 922\n",
      "931.\n",
      "Maja Popovi c. 2017. chrf++: words helping character\n",
      "n-grams. In Proceedings of the second conference\n",
      "on machine translation , pages 612618.\n",
      "Nima Pourdamghani and Kevin Knight. 2019. Neigh-\n",
      "bors helping the poor: improving low-resource ma-\n",
      "chine translation using related languages. Machine\n",
      "Translation , 33(3):239258.\n",
      "Ofir Press and Lior Wolf. 2017. Using the output em-\n",
      "bedding to improve language models. In Proceed-\n",
      "ings of the 15th Conference of the European Chap-\n",
      "ter of the Association for Computational Linguistics:\n",
      "Volume 2, Short Papers , pages 157163.\n",
      "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\n",
      "Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\n",
      "Wei Li, and Peter J Liu. 2020. Exploring the limits\n",
      "of transfer learning with a unified text-to-text trans-\n",
      "former. Journal of Machine Learning Research ,\n",
      "21:167.\n",
      "Alessandro Raganato, Ral Vzquez, Mathias Creutz,\n",
      "and Jrg Tiedemann. 2021. An empirical investi-\n",
      "gation of word alignment supervision for zero-shot\n",
      "multilingual neural machine translation. In Pro-\n",
      "ceedings of the 2021 Conference on Empirical Meth-\n",
      "ods in Natural Language Processing , pages 8449\n",
      "8456, Online and Punta Cana, Dominican Republic.\n",
      "Association for Computational Linguistics.\n",
      "Surangika Ranathunga, En-Shiun Annie Lee, Mar-\n",
      "jana Prifti Skenduli, Ravi Shekhar, Mehreen Alam,\n",
      "and Rishemjit Kaur. 2021. Neural machine trans-\n",
      "lation for low-resource languages: A survey. arXiv\n",
      "preprint arXiv:2106.15115 .\n",
      "Shuo Ren, Yu Wu, Shujie Liu, Ming Zhou, and\n",
      "Shuai Ma. 2020. A retrieve-and-rewrite initializa-\n",
      "tion method for unsupervised machine translation.\n",
      "InProceedings of the 58th Annual Meeting of the\n",
      "Association for Computational Linguistics , pages\n",
      "34983504, Online. Association for Computational\n",
      "Linguistics.\n",
      "Parker Riley, Isaac Caswell, Markus Freitag, and David\n",
      "Grangier. 2020. Translationese as a language in\n",
      "multilingual NMT. In Proceedings of the 58th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics , pages 77377746, Online. Association\n",
      "for Computational Linguistics.\n",
      "Christian Roest, Lukas Edman, Gosse Minnema, Kevin\n",
      "Kelly, Jennifer Spenader, and Antonio Toral. 2020.\n",
      "Machine translation for EnglishInuktitut with seg-\n",
      "mentation, data acquisition and pre-training. In\n",
      "Proceedings of the Fifth Conference on MachineTranslation , pages 274281, Online. Association for\n",
      "Computational Linguistics.\n",
      "David Rolnick, Andreas Veit, Serge Belongie, and Nir\n",
      "Shavit. 2017. Deep learning is robust to massive la-\n",
      "bel noise. arXiv preprint arXiv:1705.10694 .\n",
      "Carlos Barron Romero, Jess Manuel Mager Hois, and\n",
      "Fernando Reyes Avils. 2016. Richard feynman, los\n",
      "alfabetos y los lenguajes. Relingstica aplicada ,\n",
      "(19):2.\n",
      "Mnica Jasso Rosales, Manuel Mager, and Ivan\n",
      "Vladimir Meza Ruz. Towards a twitter corpus of\n",
      "the indigenous languages of the americas.\n",
      "Devendra Singh Sachan and Graham Neubig. 2018.\n",
      "Parameter sharing methods for multilingual self-\n",
      "attentional translation models. arXiv preprint\n",
      "arXiv:1809.00252 .\n",
      "Elizabeth Salesky, David Etter, and Matt Post. 2021.\n",
      "Robust open-vocabulary translation from visual text\n",
      "representations. arXiv preprint arXiv:2104.08211 .\n",
      "Jonne Saleva and Constantine Lignos. 2021. The effec-\n",
      "tiveness of morphology-aware segmentation in low-\n",
      "resource neural machine translation. In Proceedings\n",
      "of the 16th Conference of the European Chapter of\n",
      "the Association for Computational Linguistics: Stu-\n",
      "dent Research Workshop , pages 164174, Online.\n",
      "Association for Computational Linguistics.\n",
      "Motoki Sano, Jun Suzuki, and Shun Kiyono. 2019. Ef-\n",
      "fective adversarial regularization for neural machine\n",
      "translation. In Proceedings of the 57th Annual Meet-\n",
      "ing of the Association for Computational Linguis-\n",
      "tics, pages 204210.\n",
      "Yves Scherrer, Stig-Arne Grnroos, and Sami Virpi-\n",
      "oja. 2020. The University of Helsinki and aalto\n",
      "university submissions to the WMT 2020 news and\n",
      "low-resource translation tasks. In Proceedings of\n",
      "the Fifth Conference on Machine Translation , pages\n",
      "11291138, Online. Association for Computational\n",
      "Linguistics.\n",
      "Lane Schwartz. 2022. Primum non nocere: Before\n",
      "working with indigenous data, the acl must confront\n",
      "ongoing colonialism. In Proceedings of the 60th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics (Volume 2: Short Papers) , pages 724\n",
      "731.\n",
      "Lane Schwartz, Francis Tyers, Lori Levin, Christo\n",
      "Kirov, Patrick Littell, Chi-kiu Lo, Emily\n",
      "Prudhommeaux, Hyunji Hayley Park, Ken-\n",
      "neth Steimel, Rebecca Knowles, et al. 2020. Neural\n",
      "polysynthetic language modelling. arXiv preprint\n",
      "arXiv:2005.05477 .\n",
      "Lee Sechrest, Todd L Fay, and SM Hafeez Zaidi. 1972.\n",
      "Problems of translation in cross-cultural research.\n",
      "Journal of cross-cultural psychology , 3(1):4156.Rico Sennrich, Barry Haddow, and Alexandra Birch.\n",
      "2016a. Improving neural machine translation mod-\n",
      "els with monolingual data. In Proceedings of the\n",
      "54th Annual Meeting of the Association for Compu-\n",
      "tational Linguistics (Volume 1: Long Papers) , pages\n",
      "8696, Berlin, Germany. Association for Computa-\n",
      "tional Linguistics.\n",
      "Rico Sennrich, Barry Haddow, and Alexandra Birch.\n",
      "2016b. Improving neural machine translation mod-\n",
      "els with monolingual data. In Proceedings of the\n",
      "54th Annual Meeting of the Association for Compu-\n",
      "tational Linguistics (Volume 1: Long Papers) , pages\n",
      "8696.\n",
      "Rico Sennrich, Barry Haddow, and Alexandra Birch.\n",
      "2016c. Neural machine translation of rare words\n",
      "with subword units. In Proceedings of the 54th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics (Volume 1: Long Papers) , pages 1715\n",
      "1725, Berlin, Germany. Association for Computa-\n",
      "tional Linguistics.\n",
      "Joan Serra, Didac Suris, Marius Miron, and Alexan-\n",
      "dros Karatzoglou. 2018. Overcoming catastrophic\n",
      "forgetting with hard attention to the task. In In-\n",
      "ternational Conference on Machine Learning , pages\n",
      "45484557. PMLR.\n",
      "Aditya Siddhant, Ankur Bapna, Yuan Cao, Orhan Fi-\n",
      "rat, Mia Chen, Sneha Kudugunta, Naveen Arivazha-\n",
      "gan, and Yonghui Wu. 2020. Leveraging mono-\n",
      "lingual data with self-supervision for multilingual\n",
      "neural machine translation. In Proceedings of the\n",
      "58th Annual Meeting of the Association for Compu-\n",
      "tational Linguistics , pages 28272835, Online. As-\n",
      "sociation for Computational Linguistics.\n",
      "Gerardo Sierra Martnez, Cynthia Montao, Gemma\n",
      "Bel-Enguix, Diego Crdova, and Margarita\n",
      "Mota Montoya. 2020. CPLM, a parallel corpus for\n",
      "Mexican languages: Development and interface.\n",
      "InProceedings of the 12th Language Resources\n",
      "and Evaluation Conference , pages 29472952,\n",
      "Marseille, France. European Language Resources\n",
      "Association.\n",
      "Gary F Simons and M Paul Lewis. 2013. The worlds\n",
      "languages in crisis. Responses to language endan-\n",
      "germent: In honor of Mickey Noonan. New direc-\n",
      "tions in language documentation and language revi-\n",
      "talization , 3:20.\n",
      "Peter Smit, Sami Virpioja, Stig-Arne Grnroos, Mikko\n",
      "Kurimo, et al. 2014. Morfessor 2.0: Toolkit for sta-\n",
      "tistical morphological segmentation. In The 14th\n",
      "Conference of the European Chapter of the Associa-\n",
      "tion for Computational Linguistics (EACL), Gothen-\n",
      "burg, Sweden, April 26-30, 2014 . Aalto University.\n",
      "Anders Sgaard, Sebastian Ruder, and Ivan Vuli c.\n",
      "2018. On the limitations of unsupervised bilingual\n",
      "dictionary induction. In Proceedings of the 56th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics (Volume 1: Long Papers) , pages 778\n",
      "788.Haiyue Song, Raj Dabre, Zhuoyuan Mao, Fei Cheng,\n",
      "Sadao Kurohashi, and Eiichiro Sumita. 2020. Pre-\n",
      "training via leveraging assisting languages and data\n",
      "selection for neural machine translation. arXiv\n",
      "preprint arXiv:2001.08353 .\n",
      "Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\n",
      "Yan Liu. 2019. Mass: Masked sequence to se-\n",
      "quence pre-training for language generation. In In-\n",
      "ternational Conference on Machine Learning , pages\n",
      "59265936.\n",
      "Xabier Soto, Dimitar Shterionov, Alberto Poncelas,\n",
      "and Andy Way. 2020. Selecting backtranslated data\n",
      "from multiple sources for improved neural machine\n",
      "translation. In Proceedings of the 58th Annual\n",
      "Meeting of the Association for Computational Lin-\n",
      "guistics , pages 38983908, Online. Association for\n",
      "Computational Linguistics.\n",
      "Tejas Srinivasan, Ramon Sanabria, and Florian Metze.\n",
      "2019. Multitask learning for different subword seg-\n",
      "mentations in neural machine translation. arXiv\n",
      "preprint arXiv:1910.12368 .\n",
      "Dario Stojanovski, Viktor Hangya, Matthias Huck, and\n",
      "Alexander Fraser. 2019. The lmu munich unsuper-\n",
      "vised machine translation system for wmt19. In\n",
      "Proceedings of the Fourth Conference on Machine\n",
      "Translation (Volume 2: Shared Task Papers, Day 1) ,\n",
      "pages 393399.\n",
      "Haipeng Sun, Rui Wang, Kehai Chen, Masao Utiyama,\n",
      "Eiichiro Sumita, and Tiejun Zhao. 2020. Robust un-\n",
      "supervised neural machine translation with adversar-\n",
      "ial training. arXiv preprint arXiv:2002.12549 .\n",
      "Xu Tan, Yichong Leng, Jiale Chen, Yi Ren, Tao\n",
      "Qin, and Tie-Yan Liu. 2019. A study of multi-\n",
      "lingual neural machine translation. arXiv preprint\n",
      "arXiv:1912.11625 .\n",
      "Sarah G Thomason. 2015. Endangered languages .\n",
      "Cambridge University Press.\n",
      "Jrg Tiedemann. 2016. Opusparallel corpora for ev-\n",
      "eryone. Baltic Journal of Modern Computing , page\n",
      "384.\n",
      "Jrg Tiedemann. 2018. Emerging language spaces\n",
      "learned from massively multilingual corpora. arXiv\n",
      "preprint arXiv:1802.00273 .\n",
      "Atnafu Lambebo Tonja, Christian Maldonado-\n",
      "Sifuentes, David Alejandro Mendoza Castillo, Olga\n",
      "Kolesnikova, No Castro-Snchez, Grigori Sidorov,\n",
      "and Alexander Gelbukh. 2023. Parallel corpus\n",
      "for indigenous language translation: Spanish-\n",
      "mazatec and spanish-mixtec. arXiv preprint\n",
      "arXiv:2305.17404 .\n",
      "Antonio Toral, Sheila Castilho, Ke Hu, and Andy\n",
      "Way. 2018. Attaining the unattainable? reassess-\n",
      "ing claims of human parity in neural machine trans-\n",
      "lation. In Proceedings of the Third Conference on\n",
      "Machine Translation: Research Papers , pages 113\n",
      "123.Hai-Long Trieu, Duc-Vu Tran, Ashwin Ittoo, and Le-\n",
      "Minh Nguyen. 2019. Leveraging additional re-\n",
      "sources for improving statistical machine translation\n",
      "on asian low-resource languages. ACM Trans. Asian\n",
      "Low-Resour. Lang. Inf. Process. , 18(3).\n",
      "Masao Utiyama and Hitoshi Isahara. 2007. A com-\n",
      "parison of pivot methods for phrase-based statistical\n",
      "machine translation. In Human Language Technolo-\n",
      "gies 2007: The Conference of the North American\n",
      "Chapter of the Association for Computational Lin-\n",
      "guistics; Proceedings of the Main Conference , pages\n",
      "484491.\n",
      "Clara Vania and Adam Lopez. 2017. From characters\n",
      "to words to in between: Do we capture morphol-\n",
      "ogy? In Proceedings of the 55th Annual Meeting of\n",
      "the Association for Computational Linguistics (Vol-\n",
      "ume 1: Long Papers) , pages 20162027, Vancouver,\n",
      "Canada. Association for Computational Linguistics.\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\n",
      "Uszkoreit, Llion Jones, Aidan N Gomez, ukasz\n",
      "Kaiser, and Illia Polosukhin. 2017. Attention is all\n",
      "you need. In Advances in neural information pro-\n",
      "cessing systems , pages 59986008.\n",
      "Ral Vzquez, Yves Scherrer, Sami Virpioja, and Jrg\n",
      "Tiedemann. 2021. The Helsinki submission to the\n",
      "AmericasNLP shared task. In Proceedings of the\n",
      "First Workshop on Natural Language Processing for\n",
      "Indigenous Languages of the Americas , pages 255\n",
      "264, Online. Association for Computational Lin-\n",
      "guistics.\n",
      "Ivan Vuli c, Goran Glava, Roi Reichart, and Anna Ko-\n",
      "rhonen. 2019. Do we really need fully unsuper-\n",
      "vised cross-lingual embeddings? In Proceedings of\n",
      "the 2019 Conference on Empirical Methods in Nat-\n",
      "ural Language Processing and the 9th International\n",
      "Joint Conference on Natural Language Processing\n",
      "(EMNLP-IJCNLP) , pages 44074418, Hong Kong,\n",
      "China. Association for Computational Linguistics.\n",
      "Ivan Vuli c, Sebastian Ruder, and Anders Sgaard.\n",
      "2020. Are all good word vector spaces isomorphic?\n",
      "arXiv preprint arXiv:2004.04070 .\n",
      "Liang Wang, Wei Zhao, Ruoyu Jia, Sujian Li, and\n",
      "Jingming Liu. 2019a. Denoising based sequence-\n",
      "to-sequence pre-training for text generation. In Pro-\n",
      "ceedings of the 2019 Conference on Empirical Meth-\n",
      "ods in Natural Language Processing and the 9th In-\n",
      "ternational Joint Conference on Natural Language\n",
      "Processing (EMNLP-IJCNLP) , pages 39944006.\n",
      "Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu,\n",
      "Changliang Li, Derek F. Wong, and Lidia S. Chao.\n",
      "2019b. Learning deep transformer models for ma-\n",
      "chine translation. In Proceedings of the 57th Annual\n",
      "Meeting of the Association for Computational Lin-\n",
      "guistics , pages 18101822, Florence, Italy. Associa-\n",
      "tion for Computational Linguistics.Rui Wang, Xu Tan, Renqian Luo, Tao Qin, and Tie-\n",
      "Yan Liu. 2021. A survey on low-resource neural\n",
      "machine translation. In Proceedings of the Thirtieth\n",
      "International Joint Conference on Artificial Intel-\n",
      "ligence, IJCAI-21 , pages 46364643. International\n",
      "Joint Conferences on Artificial Intelligence Organi-\n",
      "zation. Survey Track.\n",
      "Xinyi Wang, Hieu Pham, Philip Arthur, and Gra-\n",
      "ham Neubig. 2019c. Multilingual neural machine\n",
      "translation with soft decoupled encoding. In Inter-\n",
      "national Conference on Learning Representations\n",
      "(ICLR) , New Orleans, LA, USA.\n",
      "Xinyi Wang, Yulia Tsvetkov, and Graham Neubig.\n",
      "2020. Balancing training for multilingual neural\n",
      "machine translation. In Proceedings of the 58th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics , pages 85268537, Online. Association\n",
      "for Computational Linguistics.\n",
      "Karl Weiss, Taghi M Khoshgoftaar, and DingDing\n",
      "Wang. 2016. A survey of transfer learning. Jour-\n",
      "nal of Big Data , 3(1):9.\n",
      "John Wieting, Taylor Berg-Kirkpatrick, Kevin Gimpel,\n",
      "and Graham Neubig. 2019. Beyond bleu: Train-\n",
      "ing neural machine translation with semantic sim-\n",
      "ilarity. In Proceedings of the 57th Annual Meet-\n",
      "ing of the Association for Computational Linguis-\n",
      "tics, pages 43444355.\n",
      "Hua Wu and Haifeng Wang. 2007. Pivot language ap-\n",
      "proach for phrase-based statistical machine transla-\n",
      "tion. Machine Translation , 21(3):165181.\n",
      "Xing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han,\n",
      "and Songlin Hu. 2019. Conditional bert contextual\n",
      "augmentation. In International Conference on Com-\n",
      "putational Science , pages 8495. Springer.\n",
      "Haoran Xu, Benjamin Van Durme, and Kenton Murray.\n",
      "2021. BERT, mBERT, or BiBERT? a study on con-\n",
      "textualized embeddings for neural machine transla-\n",
      "tion. In Proceedings of the 2021 Conference on Em-\n",
      "pirical Methods in Natural Language Processing ,\n",
      "pages 66636675, Online and Punta Cana, Domini-\n",
      "can Republic. Association for Computational Lin-\n",
      "guistics.\n",
      "Linting Xue, Noah Constant, Adam Roberts, Mi-\n",
      "hir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\n",
      "Barua, and Colin Raffel. 2021. mt5: A massively\n",
      "multilingual pre-trained text-to-text transformer. In\n",
      "Proceedings of the 2021 Conference of the North\n",
      "American Chapter of the Association for Computa-\n",
      "tional Linguistics: Human Language Technologies ,\n",
      "pages 483498.\n",
      "Delfino Zacaras Mrquez and Ivan Vladimir\n",
      "Meza Ruiz. 2021. Ayuuk-Spanish neural ma-\n",
      "chine translator. In Proceedings of the First\n",
      "Workshop on Natural Language Processing for\n",
      "Indigenous Languages of the Americas , pages\n",
      "168172, Online. Association for Computational\n",
      "Linguistics.Lenka Zajcov. 2017. Lenguas indgenas en\n",
      "la legislacin de los pases hispanoamericanos.\n",
      "Onomzein , (NE III):171203.\n",
      "Poorya Zaremoodi, Wray Buntine, and Gholamreza\n",
      "Haffari. 2018. Adaptive knowledge sharing in\n",
      "multi-task learning: Improving low-resource neural\n",
      "machine translation. In Proceedings of the 56th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics (Volume 2: Short Papers) , pages 656\n",
      "661.\n",
      "Rodolfo Zevallos, John Ortega, William Chen, Richard\n",
      "Castro, Nria Bel, Cesar Toshio, Renzo Venturas,\n",
      "Aradiel, and Hilario Nelsi Melgarejo. 2022. Intro-\n",
      "ducing QuBERT: A large monolingual corpus and\n",
      "BERT model for Southern Quechua. In Proceed-\n",
      "ings of the Third Workshop on Deep Learning for\n",
      "Low-Resource Natural Language Processing , pages\n",
      "113, Hybrid. Association for Computational Lin-\n",
      "guistics.\n",
      "Biao Zhang, Philip Williams, Ivan Titov, and Rico\n",
      "Sennrich. 2020a. Improving massively multilingual\n",
      "neural machine translation and zero-shot translation.\n",
      "arXiv preprint arXiv:2004.11867 .\n",
      "Biao Zhang, Philip Williams, Ivan Titov, and Rico\n",
      "Sennrich. 2020b. Improving massively multilingual\n",
      "neural machine translation and zero-shot translation.\n",
      "InProceedings of the 58th Annual Meeting of the\n",
      "Association for Computational Linguistics , pages\n",
      "16281639, Online. Association for Computational\n",
      "Linguistics.\n",
      "Shiyue Zhang, Ben Frey, and Mohit Bansal. 2022.\n",
      "How can nlp help revitalize endangered languages?\n",
      "a case study and roadmap for the cherokee language.\n",
      "arXiv preprint arXiv:2204.11909 .\n",
      "Shiyue Zhang, Benjamin Frey, and Mohit Bansal.\n",
      "2020c. ChrEn: Cherokee-English machine transla-\n",
      "tion for endangered language revitalization. In Pro-\n",
      "ceedings of the 2020 Conference on Empirical Meth-\n",
      "ods in Natural Language Processing (EMNLP) ,\n",
      "pages 577595, Online. Association for Computa-\n",
      "tional Linguistics.\n",
      "Shiyue Zhang, Benjamin Frey, and Mohit Bansal.\n",
      "2021. ChrEnTranslate: Cherokee-English machine\n",
      "translation demo with quality estimation and correc-\n",
      "tive feedback. In Proceedings of the 59th Annual\n",
      "Meeting of the Association for Computational Lin-\n",
      "guistics and the 11th International Joint Conference\n",
      "on Natural Language Processing: System Demon-\n",
      "strations , pages 272279, Online. Association for\n",
      "Computational Linguistics.\n",
      "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\n",
      "Weinberger, and Yoav Artzi. 2020d. Bertscore:\n",
      "Evaluating text generation with bert. In ICLR .\n",
      "Yuhao Zhang, Ziyang Wang, Runzhe Cao, Binghao\n",
      "Wei, Weiqiao Shan, Shuhan Zhou, Abudurexiti Re-\n",
      "heman, Tao Zhou, Xin Zeng, Laohu Wang, YongyuMu, Jingnan Zhang, Xiaoqian Liu, Xuanjun Zhou,\n",
      "Yinqiao Li, Bei Li, Tong Xiao, and Jingbo Zhu.\n",
      "2020e. The NiuTrans machine translation systems\n",
      "for WMT20. In Proceedings of the Fifth Confer-\n",
      "ence on Machine Translation , pages 338345, On-\n",
      "line. Association for Computational Linguistics.\n",
      "Zhuosheng Zhang, Yafang Huang, and Hai Zhao. 2019.\n",
      "Open vocabulary learning for neural chinese pinyin\n",
      "ime. In Proceedings of the 57th Annual Meeting\n",
      "of the Association for Computational Linguistics ,\n",
      "pages 15841594.\n",
      "Francis Zheng, Machel Reid, Edison Marrese-Taylor,\n",
      "and Yutaka Matsuo. 2021. Low-resource machine\n",
      "translation using cross-lingual language model pre-\n",
      "training. In Proceedings of the First Workshop on\n",
      "Natural Language Processing for Indigenous Lan-\n",
      "guages of the Americas , pages 234240, Online. As-\n",
      "sociation for Computational Linguistics.\n",
      "Hao Zheng, Yong Cheng, and Yang Liu. 2017.\n",
      "Maximum expected likelihood estimation for zero-\n",
      "resource neural machine translation. In IJCAI ,\n",
      "pages 42514257.\n",
      "Shuyan Zhou, Xiangkai Zeng, Yingqi Zhou, Antonios\n",
      "Anastasopoulos, and Graham Neubig. 2019. Im-\n",
      "proving robustness of neural machine translation\n",
      "with multi-task learning. In Proceedings of the\n",
      "Fourth Conference on Machine Translation (Volume\n",
      "2: Shared Task Papers, Day 1) , pages 565571, Flo-\n",
      "rence, Italy. Association for Computational Linguis-\n",
      "tics.\n",
      "Changfeng Zhu, Heng Yu, Shanbo Cheng, and Weihua\n",
      "Luo. 2020a. Language-aware interlingua for multi-\n",
      "lingual neural machine translation. In Proceedings\n",
      "of the 58th Annual Meeting of the Association for\n",
      "Computational Linguistics , pages 16501655, On-\n",
      "line. Association for Computational Linguistics.\n",
      "Jinhua Zhu, Fei Gao, Lijun Wu, Yingce Xia, Tao\n",
      "Qin, Wengang Zhou, Xueqi Cheng, and Tie-Yan\n",
      "Liu. 2019. Soft contextual data augmentation\n",
      "for neural machine translation. arXiv preprint\n",
      "arXiv:1905.10523 .\n",
      "Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,\n",
      "Wengang Zhou, Houqiang Li, and Tie-Yan Liu.\n",
      "2020b. Incorporating bert into neural machine trans-\n",
      "lation. arXiv preprint arXiv:2002.06823 .\n",
      "Barret Zoph, Deniz Yuret, Jonathan May, and\n",
      "Kevin Knight. 2016. Transfer learning for low-\n",
      "resource neural machine translation. arXiv preprint\n",
      "arXiv:1604.02201 .A Appendix\n",
      "In this appendix we expand the information re-\n",
      "garding current work on MT for LRL.\n",
      "A.1 Expanded LR work on Multilingual\n",
      "supervised training\n",
      "Arivazhagan et al. (2019a) introduce a represen-\n",
      "tational invariance training objective across lan-\n",
      "guages that achieves comparable results with piv-\n",
      "oting methods. Promising results of multilingual\n",
      "models have encouraged experiments with models\n",
      "trained on a massive amount of language pairs, re-\n",
      "sulting in large multilingual models: Aharoni et al.\n",
      "(2019) train a single model on 102 languages to\n",
      "and from English in contrast to the 58 languages\n",
      "used by Neubig and Hu (2018).\n",
      "The negative aspect of this approach is the size\n",
      "of the network. Arivazhagan et al. (2019b) per-\n",
      "form an extensive study on 102 language pairs\n",
      "to explore different settings and training setups\n",
      "and achieve good results for LRLs, while main-\n",
      "taining good performance for high-resource lan-\n",
      "guages. Related massively multilingual NMT\n",
      "systems have been trained for analytic proposes\n",
      "(Tiedemann, 2018; Malaviya et al., 2017) and\n",
      "general zero-shot transfer learning (Artetxe and\n",
      "Schwenk, 2019). mRASP (Lin et al., 2020) use\n",
      "for pretraining of the multilingual model and add\n",
      "a randomly aligned substitution loss that aims to\n",
      "bring words and phrases closer in the cross-lingual\n",
      "space.\n",
      "Zhang et al. (2020a) explores the main problems\n",
      "that arise for such models: multilingual NMT usu-\n",
      "ally underperforms bilingual models (Arivazha-\n",
      "gan et al., 2019b), the larger the number of lan-\n",
      "guages gets the more the performance drops (Aha-\n",
      "roni et al., 2019), languages in datasets used for\n",
      "multilingual training are unbalanced in size, and\n",
      "poor zero-shot performance compared to pivot\n",
      "models (cf. 6.3). Zhang et al. (2020a) ad-\n",
      "dresses these problems with a language-aware in-\n",
      "put layer, a deep transformer architecture (Wang\n",
      "et al., 2019b), and an online back-translation\n",
      "approach. These modifications boost zero-shot\n",
      "translation performance for multilingual models.\n",
      "To improve the problem of imbalanced and lin-\n",
      "guistically diverse training data, mostly heuristic\n",
      "methods have been proposed: Arivazhagan et al.\n",
      "(2019b) samples training data from different lan-\n",
      "guages based on a data size scaled by temperature\n",
      "term. These heuristics have an impact on perfor-mance, and ignore other factors that are not size.\n",
      "Oversampling of data is used by Johnson et al.\n",
      "(2017); Neubig and Hu (2018); Conneau and Lam-\n",
      "ple (2019). Wang et al. (2020) proposes a differ-\n",
      "entiable data selection method that automatically\n",
      "learns to weight training data, optimizing transla-\n",
      "tion on all languages.\n",
      "Multilingual modeling Sharing all parameters\n",
      "except for the attention mechanism shows im-\n",
      "provements compared with sharing everything in\n",
      "an RNN NMT model (Blackwood et al., 2018).\n",
      "Sachan and Neubig (2018) explores parameter\n",
      "sharing in the transformer architecture for the de-\n",
      "coder in the one-to-many translation setting and\n",
      "shows that transformers are more suitable than\n",
      "RNNs for this task. Also, parameter sharing in\n",
      "the decoder and embedding layer further improves\n",
      "performance. Lu et al. (2018) proposes a shared\n",
      "layer intended to capture the interlingua knowl-\n",
      "edge and an extension to the typical RNN network\n",
      "with multiple blocks along with a trainable routing\n",
      "network. The routing network enables adaptive\n",
      "collaboration by dynamic sharing of blocks condi-\n",
      "tioned on the task at hand, input, and model state\n",
      "(Zaremoodi et al., 2018). Zhang et al. (2020a) pro-\n",
      "poses a language-aware layer to improve such ar-\n",
      "chitectures further. With a similar idea, Zhu et al.\n",
      "(2020a) incorporates two special language embed-\n",
      "dings into the self-attention mechanism. The first\n",
      "encodes the unique characteristics of each lan-\n",
      "guage, while the second captures common seman-\n",
      "tics across languages.\n",
      "One problem in multilingual NMT systems is\n",
      "the translation into the wrong language. To ad-\n",
      "dress this problem, Zhang et al. (2020b) add\n",
      "a language-aware layer normalization and a lin-\n",
      "ear transformation that is inserted between the\n",
      "encoder and the decoder to induce a language-\n",
      "specific translation. Raganato et al. (2021) explore\n",
      "to weight the target language label with jointly\n",
      "training one cross attention head with word align-\n",
      "ments.\n",
      "Other modifications of NMT model archi-\n",
      "tectures to improve their performance on low-\n",
      "resource languages include: deep RNNs (Miceli-\n",
      "Barone et al., 2017), normalization layers (Ba\n",
      "et al., 2016), direct lexical connections (Nguyen\n",
      "et al., 2015), word embedding layers conducive to\n",
      "lexical sharing (Wang et al., 2019c).A.2 Extended Multi-task training\n",
      "Zhou et al. (2019) uses this approach, but extends\n",
      "it with a cascade architecture: the first decoder\n",
      "reads the encoder, and the second decoder reads\n",
      "the encoder and the first decoder (Niehues et al.,\n",
      "2016; Anastasopoulos and Chiang, 2018). The\n",
      "auxiliary task (first decoder) is a denoising de-\n",
      "coder. With RNN NMT architectures, one can\n",
      "further decide if the attention mechanism should\n",
      "be shared among tasks (Niehues and Cho, 2017).\n",
      "The authors compare all architectures and find that\n",
      "they perform similarly, with only sharing the en-\n",
      "coder being slightly better.\n",
      "Using linguistic information as an auxiliary task\n",
      "has not yet been explored exhaustively. Niehues\n",
      "and Cho (2017) studies the usage of part-of-speech\n",
      "(POS) and named entity (NE) tags, finding that\n",
      "training on named entity recognition (NER), POS\n",
      "tagging and MT together improves performance\n",
      "the most. For agglutinative languages, morpho-\n",
      "logical auxiliary tasks can be beneficial: Pan et al.\n",
      "(2020) uses stemming with fully shared parame-\n",
      "ters.\n",
      "As an alternative to linguistically informed aux-\n",
      "iliary tasks Srinivasan et al. (2019) uses multiple\n",
      "BPE vocabulary sizes to generate different seg-\n",
      "mentations. Each segmentation is treated as an in-\n",
      "dividual task.\n",
      "A.3 Data augmentation\n",
      "Back-translation Caswell et al. (2019) shows\n",
      "that adding a special tag to the synthetic data im-\n",
      "proves performance. A technique that exploits this\n",
      "idea is training an initial translation model with\n",
      "synthetic data generated via BT and then finetune\n",
      "it with gold data (Abdulmumin et al., 2019). This\n",
      "simple yet effective training algorithm improves\n",
      "NMT for LRLs; however, it can also degrade per-\n",
      "formance on HRLs if trained without a tagging\n",
      "strategy (Marie et al., 2020).\n",
      "Multiple improvements of BT have been pro-\n",
      "posed. Edunov et al. (2018) shows that sampling\n",
      "or noisy beam search can generate more effective\n",
      "pseudo-parallel data. However, for LRLs an op-\n",
      "timal beam search and greedy decoding are bet-\n",
      "ter. A factor that influences BTs effectiveness\n",
      "is the quality of the initial MT systems (Hoang\n",
      "et al., 2018). Using back-translated data from mul-\n",
      "tiple sources (Poncelas et al., 2019) or optimizing\n",
      "the ranking of back-translated data yields further\n",
      "gains (Soto et al., 2020).BT results in gains when the parallel corpora are\n",
      "naturally occurring text and not translationese, as\n",
      "the latter would only improve automatic n metrics\n",
      "(Toral et al., 2018; Graham et al., 2020). ?shows\n",
      "that BT produces more fluent text and is preferred\n",
      "by humans. Additionally, translationese and origi-\n",
      "nal data can be modeled as separate languages in a\n",
      "multilingual model (Riley et al., 2020). BT is also\n",
      "a central part of unsupervised MT (UMT; cf. 6.4)\n",
      "and zero-shot MT (Gu et al., 2019).\n",
      "Sentence modification Zhu et al. (2019) pro-\n",
      "poses to replace a randomly chosen word in a sen-\n",
      "tence with a soft-word . That means that, instead\n",
      "of sampling a word from the lexical distribution\n",
      "of a LM like Kobayashi (2018), the authors use\n",
      "the hidden state vector of the LM directly. Wu\n",
      "et al. (2019) substitutes the RNN LMs from pre-\n",
      "vious work and use BERT (Devlin et al., 2019)\n",
      " a transformer trained with a masked language\n",
      "modeling objective  instead. The authors finetune\n",
      "BERT with a conditional masked language mod-\n",
      "eling objective that tries to avoid the prediction of\n",
      "words that do not correspond to the original sen-\n",
      "tence meaning.\n",
      "Another way to augmented MT data is by para-\n",
      "phrasing. If a good paraphrase system exists, this\n",
      "can increase the number of training instances (Hu\n",
      "et al., 2019). Paraphrasing can also be used at\n",
      "training time by sampling paraphrases of the refer-\n",
      "ence sentence from a paraphraser and training the\n",
      "MT model to predict the distribution of the para-\n",
      "phraser (Khayrallah et al., 2020). This helps the\n",
      "model to generalize. Wieting et al. (2019) propose\n",
      "a similar approach, using minimum risk training to\n",
      "optimize BLEU. To avoid BLEUs constraints to a\n",
      "specific reference, they use paraphrasing to diver-\n",
      "sify the given reference.\n",
      "Finally, existing data can be augmented by\n",
      "adding noise. This noise can be continuous or dis-\n",
      "crete. In the case of applying continuous noise,\n",
      "noise vectors are added to the word embeddings\n",
      "(Cheng et al., 2018; Sano et al., 2019). Discrete\n",
      "noise is realized by inserting, deleting, or replac-\n",
      "ing words, BPE tokens, or characters to expand\n",
      "the training set in an adversarial fashion (Belinkov\n",
      "and Bisk, 2018; Ebrahimi et al., 2018; ?; Cheng\n",
      "et al., 2019, 2020).\n",
      "Pivoting While it is simple to implement and\n",
      "effective, pivot-based approaches suffer from er-\n",
      "ror propagation. To overcome that for NMT, jointtraining Zheng et al. (2017); Cheng (2019) and\n",
      "round-trip training (Ahmadnia and Dorr, 2019)\n",
      "have been proposed.\n",
      "Pivoting with NMT systems has been used\n",
      "for translating Japanese, Indonesian, and Malay\n",
      "into Vietnamese (Trieu et al., 2019), translation\n",
      "of related languages (Pourdamghani and Knight,\n",
      "2019), multilingual zero-shot MT (Lakew et al.,\n",
      "2018), and UMT (cf. 6.4) between distant lan-\n",
      "guage pairs (Leng et al., 2019).\n",
      "A.4 Recent low-resource Shared Tasks\n",
      "First, the LoResMT 2020 shared task (Ojha\n",
      "et al., 2020) explores the case of language pairs\n",
      "which have no parallel data between them (Hindi\n",
      "Bhojpuri, HindiMagahi, and RussianHindi).\n",
      "The winning system (Laskar et al., 2020) uses\n",
      "a MASS model in a zero-shot fashion with ad-\n",
      "ditional monolingual data (see 6.4). Second,\n",
      "the WMT 2020 shared tasks on UMT and very\n",
      "low-resource supervised MT (Fraser, 2020) pro-\n",
      "vide text and 60k aligned phrases for German\n",
      "Upper Sorbian., The most important technique in\n",
      "all tracks is transfer learning, achieving surpris-\n",
      "ingly good results. For the AmericasNLP 2021\n",
      "shared task on open MT (Mager et al., 2021), 10\n",
      "indigenous language languages were paired with\n",
      "Spanish, resulting in an extreme low-resource set-\n",
      "ting (4k to 125k paired sentences), with challenges\n",
      "out as domain, dialectical, and orthographic mis-\n",
      "matches between splits and datasets. The best\n",
      "systems shows that data cleaning and collection\n",
      "(??) as well as multilingual approaches (6.1)\n",
      "result in the best performance in this conditions.\n",
      "Finally the shared task on MT in Dravidian lan-\n",
      "guages (Chakravarthi et al., 2021) features 3 lan-\n",
      "guages paired with English as well as Tamil\n",
      "Telugu. Again, the winning system uses a mul-\n",
      "tilingual approach. The best performing systems\n",
      "use BT (6.3) and BPE word segmentation (2.1).\n",
      "The results from these challenges indicate that\n",
      "the optimal selection and combination of meth-\n",
      "ods differs between cases (i.e., amount of mono-\n",
      "lingual, parallel data, cleanness of data, domain\n",
      "mismatch, linguistic closeness of languages). This\n",
      "implies that data analysis and linguistic knowl-\n",
      "edge are needed to improve a final systems per-\n",
      "formance.\n",
      "A.5 Transfer learning\n",
      "This helps low-resource tasks as a lower amount\n",
      "of data can be used for training. One applicationof transfer learning to MT is the usage of a pre-\n",
      "trained RNN LM (Gulcehre et al., 2015) as the de-\n",
      "coder in an NMT system. Zoph et al. (2016) is the\n",
      "first work that uses pretrained models to improve\n",
      "NMT systems. The authors perform two experi-\n",
      "ments with an RNN encoderdecoder architecture\n",
      "with an attention mechanism: the model is first\n",
      "pretrained on a high-resource language pair This\n",
      "works even better if related languages are used\n",
      "during pretraining (Nguyen and Chiang, 2017).\n",
      "Using pretrained LMs at decoding time and as pri-\n",
      "ors at training time also improves vanilla models\n",
      "(Baziotis et al., 2020).\n",
      "To avoid overfitting, models can be finetuned on\n",
      "both a HRLs pair and a LRLs pair in a multi-task\n",
      "fashion (Neubig and Hu, 2018).\n",
      "However, how can we represent best the vocab-\n",
      "ulary? Zoph et al. (2016) use separate embeddings\n",
      "for the source and the target language. However,\n",
      "using tied embeddings has been shown to yield\n",
      "better results (Press and Wolf, 2017). Edunov et al.\n",
      "(2019) employs ELMO (Peters et al., 2018) repre-\n",
      "sentations as pretrained features in the encoder of\n",
      "a transformer model. Song et al. (2020) shows that\n",
      "it is possible to improve performance by combin-\n",
      "ing monolingual texts from linguistically related\n",
      "languages, performing a script mapping. It is also\n",
      "possible to extract features from a BERT model\n",
      "in the source language and combining these with\n",
      "an NMT system (Zhu et al., 2020b), but using a\n",
      "BERT model pretrained with a mixed sentences\n",
      "from source and target languages lead to even bet-\n",
      "ter results (Xu et al., 2021).\n",
      "Encoder-decoder pretrained models have\n",
      "gained popularity in the last years for low-\n",
      "resource MT. Conneau and Lample (2019)\n",
      "proposes training the encoder and the decoder\n",
      "separately in order to get cross-language rep-\n",
      "resentations (XLM). This idea has further been\n",
      "extended by Song et al. (2019, MASS) to\n",
      "masking a sequence of tokens from the input.\n",
      "Training MASS in a multilingual fashion and\n",
      "using monolingual data for pretraining helps to\n",
      "improve NMT for low-resource languages and\n",
      "zero-shot translation (Siddhant et al., 2020).\n",
      "Another approach is to train the entire transformer\n",
      "model as a denoising autoencoder (BART; Lewis\n",
      "et al., 2019). The multilingual version of BART\n",
      "(mBART) is more suitable for NMT tasks and\n",
      "yields important gains (Liu et al., 2020). It is also\n",
      "possible to pretrain a transformer in a multi-task,text-to-text fashion, where one of the tasks is\n",
      "MT (T5; Raffel et al., 2020). All four models\n",
      "can be finetuned for MT or used in an unsuper-\n",
      "vised fashion. Improvements to BART can be\n",
      "obtained by augmenting the maximum likelihood\n",
      "objective with an additional objective, which is\n",
      "a data-dependent Gaussian prior distribution (Li\n",
      "et al., 2020). Huge LMs can improve zero-shot\n",
      "and few-shot learning even further (Brown et al.,\n",
      "2020), but at a high computational cost. Pursuing\n",
      "another direction, Wang et al. (2019a) develops a\n",
      "hybrid architecture between a transformer and a\n",
      "pointer-generator network. At training time, the\n",
      "authors jointly train the encoder and the decoder\n",
      "in a denoising auto-encoding fashion.\n",
      "One crucial problem for transfer-learning is\n",
      "minimizing catastrophic forgetting (Serra et al.,\n",
      "2018). Chen et al. (2021) show that it is possible\n",
      "to combine a pre-trained multilingual model, with\n",
      "fine-tuining it with one single language pair, to im-\n",
      "prove zero-shot machine translation. Another way\n",
      "to handle this problem is reducing the number of\n",
      "parameter to be updated. Gheini et al. (2021) pro-\n",
      "pose to only update the cross attention parameters.\n",
      "A.6 Unsupervised MT\n",
      "The addition of other components such as masked\n",
      "LMs and denoising auto-encoding has also been\n",
      "tried (Stojanovski et al., 2019). Unsupervised\n",
      "methods are vulnerable to adversarial attacks of\n",
      "word substitution and order change in the input.\n",
      "Adversarial training can improve performance in\n",
      "such situations (Sun et al., 2020). Since the ini-\n",
      "tialization step is crucial for UMT, Ren et al.\n",
      "(2020) aligns semantically similar sentences from\n",
      "two monolingual corpora with the help of cross-\n",
      "lingual embeddings. With these, an SMT system\n",
      "is trained to warm up an NMT system. How-\n",
      "ever, UMT still has to overcome a set of chal-\n",
      "lenges. Sgaard et al. (2018) shows that perfor-\n",
      "mance decays dramatically for languages with dif-\n",
      "ferent typological features, since, in such situa-\n",
      "tions, bilingual word embeddings (Conneau et al.,\n",
      "2017) are far from isomorphic. Vuli c et al. (2020)\n",
      "finds that isomorphism is also less likely if small\n",
      "amounts of monolingual data are used for training\n",
      "bilingual word embeddings. Nooralahzadeh et al.\n",
      "(2020) discovers that performance quickly deteri-\n",
      "orates for a mismatch of source and target domain\n",
      "and that the initialization of word embeddings can\n",
      "affect MT performance. All of this makes UMTfor LRLs or endangered languages challenging.\n",
      "Some of the described issues have been ad-\n",
      "dressed: Liu et al. (2019) proposes to combine\n",
      "word-level and subword-level embeddings to ac-\n",
      "count for morphological complexity. For the prob-\n",
      "lem of distant language pairs, Leng et al. (2019)\n",
      "proposes pivoting (cf. 6.3). Isomorphism of\n",
      "bilingual word-embeddings can be improved with\n",
      "semi-supervised methods (Vuli c et al., 2019).\n",
      "Garcia et al. (2020) introduces multilingual\n",
      "UMT systems. The main idea consists of general-\n",
      "izing UMT by using a multi-way back-translation\n",
      "objective. Recently, pretrained multilingual trans-\n",
      "former networks are used to improve UMT even\n",
      "further (cf. 6.4).\n",
      "B Ethical Considerations\n",
      "Ethical concerns when working on MT for endan-\n",
      "gered languages include a lack of community in-\n",
      "volvement during language documentation, data\n",
      "creation, and development and setup of MT sys-\n",
      "tems. For more information, we refer interested\n",
      "readers to Bird (2020). Finally, we want to men-\n",
      "tion that publicly employing low-quality MT sys-\n",
      "tems for LRLs bears a risk of translating incor-\n",
      "rectly or in biased (e.g., sexist or racist) ways.\n"
     ]
    }
   ],
   "source": [
    "# Removing non-ascii characters, urls from extracted text\n",
    "\n",
    "import re\n",
    "text_without_non_ascii = re.sub(r\"[^\\x00-\\x7F]\", \"\", extracted_text) \n",
    "text_without_non_ascii = re.sub(r\",.-/:\",\"\",text_without_non_ascii)\n",
    "text_without_urls = re.sub(r\"h/t_tps?://[^\\s]+\",\"\",text_without_non_ascii)\n",
    "\n",
    "print(text_without_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abstract', '1 Introduction', '2 Background and Definitions', '3 Challenges and open questions', '4 Available MT datasets for ILA', '5 Low-resource MT', '6 Low-resource MT paradigms', '7 Advances in MT for the indigenous languages of the Americas', '8 Ethical aspects', '9 Conclusion']\n"
     ]
    }
   ],
   "source": [
    "# Format the section headings for extraction of text under each section\n",
    "output = []\n",
    "current_element = ''\n",
    "\n",
    "for i, item in enumerate(new_items):\n",
    "    if i > 0 and item[0].isdigit() and not new_items[i - 1][0].isdigit():\n",
    "\n",
    "        output.append(current_element.strip())\n",
    "        current_element = item[0] \n",
    "    else:\n",
    "        current_element += ' ' + item[0]\n",
    "\n",
    "# Add the last element to the output\n",
    "if current_element:\n",
    "    output.append(current_element.strip())\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abstract',\n",
       " '1 Introduction',\n",
       " '2 Background and Definitions',\n",
       " '3 Challenges and open questions',\n",
       " '4 Available MT datasets for ILA',\n",
       " '5 Low-resource MT',\n",
       " '6 Low-resource MT paradigms',\n",
       " '7 Advances in MT for the indigenous languages of the Americas',\n",
       " '8 Ethical aspects',\n",
       " '9 Conclusion']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_output = []\n",
    "\n",
    "if(len(output[0].split(' '))>1):\n",
    "    words = output[0].split(' ')\n",
    "    new_output.append(words[0])\n",
    "\n",
    "else:\n",
    "    new_output = output\n",
    "            \n",
    "new_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abstract',\n",
       " '1 Introduction',\n",
       " '2 Background and Definitions',\n",
       " '3 Challenges and open questions',\n",
       " '4 Available MT datasets for ILA',\n",
       " '5 Low-resource MT',\n",
       " '6 Low-resource MT paradigms',\n",
       " '7 Advances in MT for the indigenous languages of the Americas',\n",
       " '8 Ethical aspects',\n",
       " '9 Conclusion']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(1,len(output)):\n",
    "  if(output != new_output):\n",
    "    element = output[i].split('.')[0][:-1]\n",
    "    element_new = ''\n",
    "    if(output[i].split('.')[0][-1].isdigit()):\n",
    "        element_new = element\n",
    "    else:\n",
    "        element_new = output[i].split('.')[0]\n",
    "        \n",
    "    new_output.append(element_new.rstrip())\n",
    "  else:\n",
    "     pass\n",
    "\n",
    "new_output\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections = new_output\n",
    "len(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markers found in the text. Abstract\n",
      "Markers found in the text. 1 Introduction\n",
      "Markers found in the text. 2 Background and Definitions\n",
      "Markers found in the text. 3 Challenges and open questions\n",
      "Markers found in the text. 4 Available MT datasets for ILA\n",
      "Markers found in the text. 5 Low-resource MT\n",
      "Markers not found in the text. 6 Low-resource MT paradigms\n",
      "Markers not found in the text. 7 Advances in MT for the indigenous languages of the Americas\n",
      "Markers found in the text. 8 Ethical aspects\n",
      "Markers found in the text. 9 Conclusion\n"
     ]
    }
   ],
   "source": [
    "# Extracting text falling under each sections for summary\n",
    "section_extraction = []\n",
    "\n",
    "for i in range(len(sections)-1):\n",
    "    start_index = text_without_urls.find(sections[i])\n",
    "    end_index = text_without_urls.find(sections[i+1])\n",
    "\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        extraction = text_without_urls[start_index:end_index].strip()\n",
    "        print(\"Markers found in the text.\",sections[i])\n",
    "        section_extraction.append(extraction)\n",
    "    else:\n",
    "        print(\"Markers not found in the text.\",sections[i])\n",
    "\n",
    "# Extract the last section separately\n",
    "last_start_index = text_without_urls.find(sections[-1])\n",
    "if last_start_index != -1:\n",
    "    last_extraction = text_without_urls[last_start_index:].strip()\n",
    "    print(\"Markers found in the text.\", sections[-1])\n",
    "    section_extraction.append(last_extraction)\n",
    "else:\n",
    "    print(\"Markers not found in the text.\", sections[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abstract\\nNeural models have drastically advanced state\\nof the art for machine translation (MT) be-\\ntween high-resource languages. Traditionally,\\nthese models rely on large amounts of train-\\ning data, but many language pairs lack these\\nresources. However, an important part of the\\nlanguages in the world do not have this amount\\nof data. Most languages from the Americas are\\namong them, having a limited amount of par-\\nallel and monolingual data, if any. Here, we\\npresent an introduction to the interested reader\\nto the basic challenges, concepts, and tech-\\nniques that involve the creation of MT systems\\nfor these languages. Finally, we discuss the\\nrecent advances and findings and open ques-\\ntions, product of an increased interest of the\\nNLP community in these languages.',\n",
       " '1 Introduction\\nMore than 7 billion people on Earth communicate\\nin nearly 7000 different languages (Pereltsvaig,\\n2020). Of these, approximately 900 languages\\nare native of the American continent (Campbell,\\n2000). Most of these indigenous languages of the\\nAmericas (ILA) are endangered at some degree\\n(Thomason, 2015). This huge variety in languages\\nis simultaneously a rich treasure for humanity and\\nalso a barrier to communication among people\\nfrom different backgrounds. Human translators\\nhave been important in overcoming language bar-\\nriers. However, trained translators are not acces-\\nsible to everyone on Earth and even scarcer for\\nendangered and minority languages. The need\\nfor translations is even written in the constitutions\\nof several countries like Mexico, Peru, Paraguay,\\nVenezuela, and Bolivia (Zajcov, 2017) to allow\\nnative speakers to have equal language rights re-\\ngarding law.\\nThis is why developing MT is crucial: it helps\\nhumanity overcome language barriers while si-\\nmultaneously allowing people to continue using\\nWork done while at the University of Stuttgart.their native tongue. However, the challenges to\\nachieving these problems are not trivial. It is not\\nonly the amount of available data (a common the-\\nsis among the NLP community) but also a set\\nof challenging issues (dialectical and orthographic\\nvariations, noisy texts, complex morphology, etc.)\\nthat must be addressed.\\nMT has always been an important task within\\nthe larger area of natural language processing\\n(NLP). In 1954, the GeorgetownIBM experiment\\n(Hutchins, 2004) was the first that showed at least\\nsome effectiveness of MT. Further research re-\\nsulted in rule-based systems and statistical models.\\nIn 2023, neural models define state of the art for\\nMT if training data is plentiful  i.e., for so-called\\nhigh-resource languages (HRLs)  and have also\\nachieved impressive results for low-resource lan-\\nguages (LRLs). MT is also the most studied NLP\\ntask for the ILA (Mager et al., 2018b; Littell et al.,\\n2018). The common issue among these languages\\nis the extreme low-resource conditions they are\\nconfronted with. The research interest for these\\nlanguages has increased in the last years, including\\nthe recent AmericasNLP 2021 shared task (Mager\\net al., 2021) on 10 indigenous languages to Span-\\nish, and the WMT (Conference on Machine Trans-\\nlation) shared task for InuktitutEnglish (Barrault\\net al., 2020).\\nIn this work we aim to provide a comprehensive\\nintroduction to the challenges that involve creat-\\ning MT systems for ILA, and the current status\\nof the existing work. We organize this work as\\nfollows: We start by introducing state-of-the-art\\nNMT models (2). Then, we discuss the current\\nchallenges for these languages (3); and we in-\\ntroduce the key concepts related to low-resource\\nNMT and the implications for endangered lan-\\nguages of the Americas(3). This is followed by\\na discussion of available data (4). Afterwards,\\nwe introduce the important concepts for LRL and\\nendangered languages (5); then we introduce thearXiv:2306.06804v1  [cs.CL]  11 Jun 2023main strategies aimed at improving NMT with\\nlimited training data (6); and finally we give an\\noverview of the work done for ILA on MT (7).\\nIn doing so, we provide insights into the follow-\\ning questions: Which systems define the state of\\nthe art on low-resource NMT applied to the ILA?\\nWhat is the route that ahead to improve the trans-\\nlations of the ILA?',\n",
       " '2 Background and Definitions\\nFormally, the task of MT consists of converting\\ntextXin a source language Lxinto text Yin a\\ntarget language Lythat conveys the same mean-\\ning.1Translating text XLxintoYLycan be\\ndescribed as a function (Neubig, 2017):\\nY=MT(X). (1)\\nXandYcan be of variable length, such as\\nphrases, sentences, or even documents.\\nIf other languages are used during the transla-\\ntion process, e.g., as pivots, we denote them as\\nL1, . . . , L n. We refer to a corpus of monolingual\\nsentences in language LiasMLi=S1, ..., S n.\\nProbabilistic Modeling and Data When us-\\ning probabilistic MT models, the goal is to find\\nYLywith the highest conditional probability,\\ngiven XLx. Under the supervised machine\\nlearning paradigm, a parallel corpus Cparallel =\\n(X1, Y1), ...,(Xn, Yn)is used to learn a set of pa-\\nrameters , which define a probability distribution\\nover possible translations. Given Cparallel , the\\ntraining objective of an NMT model is generally\\nto maximize the log-likelihood Lwith respect to\\n:\\nL=X\\n(Xi,Yi)Cparallellogp(Yi|Xi;).(2)\\nWithin this overall framework, there are a num-\\nber of design decisions one has to make, such as\\nwhich model architecture to use, how to generate\\ntranslations, and how to evaluate.\\nDecoding Decoding refers to the generation of\\noutput Y, given the parameters and an input X.\\nOften, decoding is done by approximately solving\\nthe following maximization problem:\\nargmax Yp(Y|X;) (3)\\n1This is an approximation, since it is in general not possi-\\nble to map the meaning of text exactly into another language\\n(Nida, 1945; Sechrest et al., 1972; Baker, 2018).Most NMT systems factorize the probability of\\nY= y1, ...,yTin a left-to-right fashion:\\np(Y) =TY\\nt=1p(yt|y<t, X,  ) (4)\\nThus, the probability of token ytat time step tis\\ncomputed using the previously generated tokens\\ny<t, the source sentence Xand the model param-\\neters . Common algorithms for finding a high-\\nprobability translation are greedy decoding, i.e.,\\npicking the token with the highest probability at\\neach time step, and beam search (Lowerre, 1976).\\n2.1 Input Representations\\nThe texts XandYare input into an NMT sys-\\ntem as sequences of continuous vectors. How-\\never, defining which units should be represented\\nas such vectors is non-trivial. The classic way\\nis to represent each word within XandYas\\na vector (or embedding). However, in a low-\\nresource setting, often not all vocabulary items ap-\\npear in the training data (Jean et al., 2015; Lu-\\nong et al., 2015). This issue especially effects lan-\\nguages with a rich inflectional morphology (Sen-\\nnrich et al., 2016c): as many word forms can\\nrepresent the same lemma, the vocabulary cover-\\nage decreases drastically. Furthermore, for many\\nLRLs, boundaries between words or morphemes\\nare not easy to obtain or not well defined in the\\ncase of languages without a standard orthography.\\nAlternative input units have been explored, such as\\ncharacters (Ling et al., 2015), byte pair encoding\\n(BPE; Sennrich et al., 2016a), morphological rep-\\nresentations (Vania and Lopez, 2017; Ataman and\\nFederico, 2018), syllables (Zhang et al., 2019), or,\\nrecently, a visual representation of rendered text\\n(Salesky et al., 2021). No clear advantage has been\\ndiscovered for using morphological segmentations\\nover BPEs when testing them on LRLs (Saleva and\\nLignos, 2021).\\nInput representations can be pretrained. The\\ntwo most common options are: i) word em-\\nbeddings, where each type is represented by a\\nvector, e.g., Word2Vec (Mikolov et al., 2013),\\nGlove (Pennington et al., 2014), or Fasttext (Bo-\\njanowski et al., 2017)) embeddings, and ii) contex-\\ntualized word representations, where entire sen-\\ntences are being encoded at a time, e.g., ELMo\\n(Peters et al., 2018) or BERT (Devlin et al.,\\n2019). However, training of these methods re-\\nquires large monolingual training corpora, whichmay not be readily available for LRLs. As most\\nILA have rich morphology, this topic has gath-\\nered special interest. The discussion about the us-\\nage of morpholigical segmented input for NMT\\nmodels is recurrent. (Mager et al., 2022) show\\nthat the unsupervised morphologically inspired\\nmodels outperform BPE pre-processing (experi-\\nmented on 4 language pares). Similar experi-\\nments done on QuechuaSpanish and Inuktitut\\nEnlgish (Schwartz et al., 2020), comparing BPEs\\nagainst Morfessor (Smit et al., 2014). Also (Or-\\ntega et al., 2020a) improves the SOTA (state-of-\\nthe-art) for QuechuaSpanish MT using a mor-\\nphological guided BPE algorithm.\\n2.2 Architectures\\nNMT models typically are sequence-to-sequence\\nmodels. They encode a variable-length sequence\\ninto a vector or matrix representation, which they\\nthen decode back into a variable-length sequence\\n(Cho et al., 2014). The two most frequent archi-\\ntectures are: i) recurrent neural networks (RNN),\\nsuch as LSTMs (Hochreiter and Schmidhuber,\\n1997) or GRUs (Cho et al., 2014), and ii) trans-\\nformers (Vaswani et al., 2017), which define the\\ncurrent state of the art in the high-resource setting.\\nAs for most neural network models, training an\\nNMT system on a limited number of instances\\nis challenging (Fernndez-Delgado et al., 2014).\\nThere are common problems that arise from lim-\\nited data in the training set. One major advan-\\ntage of neural models is their ability to learn rep-\\nresentations from raw data, in contrast to manu-\\nally engineered features (Barron, 1993). However,\\nproblems arise when not enough data is provided\\nto enable effective learning of features. Another\\nstrength of neural networks is their generalization\\ncapacity (Kawaguchi et al., 2017). However, train-\\ning a neural network on a small dataset easily leads\\nto overfitting (Rolnick et al., 2017). Recent stud-\\nies, however, show empirically that this does not\\nnecessarily happen if the network is tuned cor-\\nrectly (Olson et al., 2018).\\n2.3 Evaluation\\nAccurately judging translation quality is difficult\\nand, thus, often still done manually: bilingual\\nspeakers assign scores according to provided crite-\\nria such as fluency and adequacy ( Does the output\\nhave the same meaning as the input? ). However,\\nmanual evaluation is expensive and slow. More-over, in the case of endangered languages, bilin-\\ngual speakers can be hard or impossible to find.\\nAutomatic metrics provide an alternative.2\\nThese metrics assign a score to system output,\\ngiven one or more ground truth reference transla-\\ntions. The most widely used metric is BLEU (Pa-\\npineni et al., 2002), which relies on token-level n-\\ngram matches between the translation to be rated\\nand one or more gold-standard translations. For\\nmorphologically rich languages, character-level\\nmetrics, such as chrF (Popovi c, 2017), are often\\nmore suitable, as they allow for more flexibility.\\nIn the AmericasNLP ST (Mager et al., 2021) this\\nmetric was used over BLEU, as it fits better to the\\nrich morphology of many ILA.\\nTo have a concrete example, lets have the fol-\\nlowing Wixarika phrase with an English transla-\\ntion:\\nyu-huta-me ne-p+-we-iwa\\nan-two-ns 1sg:s-asi-2pl:o-brother\\nI have two brothers\\nAs discussed in (Mager et al., 2018c) it is dif-\\nficult to translate back from Spanish (or other Fu-\\nsional language) the morpheme p+as it has not\\nequivalent in these languages. So if we would ig-\\nnore these morpheme at all, BLEU would penal-\\nize the entire word nep+weiwa . In contrast, chrF\\nwould give credit to the translation, even if the p+\\nis missing.\\nOne shortcoming of these evaluation metrics is\\nthat the evaluation is very dependent on the sur-\\nface forms and not on the ultimate goal of seman-\\ntic similarity and fluency. Recent work uses pre-\\ntrained models to evaluate semantic similarity be-\\ntween translations and the gold standard (Zhang\\net al., 2020d), but these methods are limited to lan-\\nguages for which such models are available. This\\nis not possible for the ILA, as the amount of mono-\\nlingual data is not enough to train a reliable pre-\\ntrained language model3.',\n",
       " '3 Challenges and open questions\\nIn an overview of the datasets and recent studies\\nof MT for the ILA, we found the following main\\nissues to be handled.\\n2For a detailed overview of automatic metrics for MT we\\nrefer the interested reader to specialized reviews (Han, 2016;\\nCelikyilmaz et al., 2020; Chatzikoumi, 2020).\\n3One exception to this is Quechua, that has a large enough\\nmonolingual dataset to train a BERT like model (Zevallos\\net al., 2022)Extreme low-resource parallel datasets Even\\nwith the recent advances, the resources available\\nto train MT systems are extremely scarce, hav-\\ning training set between 4k and 20k sentences (see\\n4), with notable exceptions for Inuktitut, Guarani\\nand Quechua (Joanis et al., 2020; Ortega et al.,\\n2020a).\\nLack of monolingual data Most of these lan-\\nguages are mostly used in spoken form. In re-\\ncent years, with the advancement and democra-\\ntization of mobile technologies, indigenous lan-\\nguages have seen a slight increase in massaging\\nsystems and private spheres (Rosales et al.). How-\\never, the usage of these languages on the internet\\nis rather limited. Even Wikipedia has a limited\\namount of these languages (Mager et al., 2018b).\\nLow domain diversity . As most parallel\\ndatasets are scarce, they are restricted to a small\\nnumber of domains, making it challenging to\\nadapt it, or try to aim for general translation mod-\\nels. This has been recognized as a major problem\\nduring the AmericasNLP ST (Mager et al., 2021).\\nRich morphology An important number of\\nthese languages are morphological highly rich. In\\nmany cases, we find polysynthetic, with or highly\\nagglutinative languages (Kann et al., 2018) or even\\nfusional phenomenon (Mager et al., 2020).\\nDistant paired language The most common\\nlanguages that we find that ILA is translated into\\nare Spanish, English, and Portuguese. However,\\nthese languages are distantly related to the ILA,\\nand have completely different linguistically phe-\\nnomenons (Campbell, 2000; Romero et al., 2016).\\nNoisy text environments Monolingual texts, if\\nexist, are found in social media that often use a\\nnon-canonical witting (Rosales et al.).\\nCode-Swithing This phenomenon is strongly\\npresent in ILA, as all of these languages are mi-\\nnority languages in their own countries. The\\nbilingualism among their communities is strong\\n(and CS is a common phenomenon in this setup\\n(etino glu, 2017)). The final result of this phe-\\nnomenon is the inclusion of code-switching on a\\ncommon base (Mager et al., 2019) in their lan-\\nguage.\\nLack of orthographic normalization The us-\\nage of ILA faces the problem of having a unifiedorthographic standard. This is not always possi-\\nble, as the suggestions of linguists and official en-\\ntities do not always match the day-by-day writ-\\ning of the speakers. Moreover, in some cases,\\nspecial symbols present in the orthographic stan-\\ndards are not accessible in English or Spanish key-\\nboard and need to be replaced with other symbols.\\nThe winner of the AmericasNLP ST got important\\nimprovements using orthographic normalizers de-\\nveloped specifically for each American language\\n(Vzquez et al., 2021).\\nDialectal variety The indigenous languages\\nhave a strong dialectal variety, making it hard for\\nnative speakers to understand even speakers from\\nneighboring villages. The linguistic richness of\\nentire regions is so diverse that even a single state\\nlike the Mexican Oaxaca could correspond to the\\ndiversity in the whole Europe (McQuown, 1955).',\n",
       " '4 Available MT datasets for ILA\\nThe parallel datasets available for MT have been\\nincreasing during the last years. At this moment,\\nwe can show in two folds the development of these\\nresources: as shown in table 2 work on specific\\nlanguage has emerged; but also broader datasets\\nhave started to cover the ILA (see table 1).\\nLanguage-specific corpus collection work has\\nbeen done for many languages, where parallel\\ncorpus has been the main component. In re-\\ncent time we have seen CherokeeEnglish (OPUS)\\n(Zhang et al., 2020c), WixarikaSpanish (Mager\\net al., 2018a), ShipioKonibo (Feldman and Coto-\\nSolano, 2020), and others (see table 2). The most\\nprominent of these datasets has been the Inuktitut\\nEnglish parallel data. The last version of this\\ndataset corpora (Joanis et al., 2020) is has medium\\nsize with 1,450,094 sentences. Previous versions\\nof this corpus are (Martin et al., 2003). This data\\nset was used for the WMT 2020 Shared Task on\\nUnsupervised, and Low Resourced MT (Barrault\\net al., 2020).\\nFor wide-spoken languages like Guarani, it is\\neven possible to collect a web crawled dataset,\\nincluding news articles and social media parallel\\naligned data (Chiruzzo et al., 2020; Gngora et al.,\\n2021) This dataset also includes monolingual data.\\nThis is possible as Guaran is one of the most spo-\\nken indigenous languages of the continent.\\nIn contrast to the language-specific datasets,\\nwe find broader approaches (see table 1). The\\nbroadest multilingual dataset, which contains theDataset Paired-languages Authors\\nAmericasNLI Aymara, Ashninka, Bribri, Guaran,\\nNahuatl, Otom, Quechua, Rarmuri,\\nShipibo-Konibo, Wixarika(Ebrahimi et al., 2022)\\nCPML Chol, Maya, Mazatec, Mixtec, Nahu-\\natl and Otomi(Sierra Martnez et al., 2020)\\nOPUS * (Tiedemann, 2016)\\nNew testament Bible * (McCarthy et al., 2020)\\nTable 1: Parallel dataset collections that contain one or more indigenous languages of the Americas\\nLanguage Paried-language ISO Family Sentences Domain Authors\\nAshninka Spanish cni Arawak 3883 (Ortega et al., 2020b)\\nBribri Spanish bzd Chibchan 5923 (Feldman and Coto-\\nSolano, 2020)\\nGuarani Spanish gn Tupi-Guarani News,\\nBlogs(Abdelali et al., 2006)\\nGuarani Spanish gn Tupi-Guarani 14,531 News,\\nBlogs(Chiruzzo et al., 2020)\\nGuarani Spanish gn Tupi-Guarani 14,792 News, So-\\ncial Media(Gngora et al., 2021)\\nGuarani Spanish gn Tupi-Guarani 30855 8 Domains (Chiruzzo et al., 2022)\\nNahuatl Spanish nah Uto-Aztecan 16145 Diverse\\nBooks(Gutierrez-Vasques\\net al., 2016)\\nOtom Spanish oto Oto-Manguean 4889 Diverse\\nBookshttps://\\ntsunkua.elotl.\\nmx\\nRarmuri Spanish tar Uto-Aztecan 14721 Dictionary\\nExamples(Mager et al., 2022)\\nShipibo-Konibo Spanish shp Panoan 14592 Educational,\\nReligious(Galarreta et al., 2017)\\nWixarika Spanish hch Uto-Aztecan 8966 Literature (Mager et al., 2018a)\\nCherokee English chr Uto-Aztecan OPUS (Zhang et al., 2020c)\\nInuktitut English iku EskimoAleut 1,450,094 Legislative (Joanis et al., 2020)\\nAyuuk Spanish mir MixeZoque 7553 Diverse (Zacaras Mrquez and\\nMeza Ruiz, 2021)\\nMazatec Spanish Many Oto-Manguean 9799 Diverse (Tonja et al., 2023)\\nMixtec Spanish Many Oto-Manguean 13235 Diverse (Tonja et al., 2023)\\nTable 2: Parallel datasets that have been released focusing on one indigenous language\\nBibles New Testament, includes about 1600 lan-\\nguages (Mayer and Cysouw, 2014; McCarthy\\net al., 2020) of the 2,508 that have been collected\\nby the Summer Institute of Linguistic (SIL) (An-\\nderson and Anderson, 2012). Another remarkable\\neffort to obtain broad language coverage is the\\nPanLex project (Kamholz et al., 2014), which has\\ngathered lexical translation dictionaries for over\\n5,700 languages. However, for most languages,\\nPanLex contains only a few dozen words. Duan\\net al. (2020) show that such dictionaries can be\\nused to create an NMT system, making bilingual\\ndictionaries relevant for further studies.\\nRecently community-driven research groups\\nhave started the creation of own parallel datasets,\\nsuch as Masakhane (Orife et al., 2020; Nekoto\\net al., 2020) for African languages, and Americ-\\nasNLP for indigenous languages of the Americas(Ebrahimi et al., 2021; Mager et al., 2021). The\\nAmericasNLI dataset is an important effort to have\\na common evaluation benchmark for the 10 in-\\ndigenous languages of the Americas for the MT\\nand NLI tasks.\\nGiven the constitutional rights of indigenous\\nlanguages in many countries of the Americas, it is\\npossible to access this data. Vzquez et al. (2021)\\nmade available this resource during their shared\\ntask system development.\\nFinally, it is important to mention that many\\nof the languages spoken in the Americas have\\nWikipedias set of articles available4.\\n4The available languages in wikipedia can be consulted\\nat:https://es.wikipedia.org/wiki/Portal:\\nLenguas_ind genas_de_Amlrica . Until the\\npublication of this article, there were only entries in Nahu-\\natl, Navajo, Guarani, Aymara, Klaalisut, Esquimal, Inukitut,\\nCherokee, and Cree.Collection of New Data A common way to cre-\\nate parallel data with the help of bilingual speakers\\nis via elicitation (translating the foreign text into\\nanother language). It has the disadvantage of bias-\\ning the created text to forms and topics, culture,\\nand even grammatical forms towards the source\\nlanguage (Lrscher, 2005). A method that avoids\\nthis problem is language documentation, which\\nconsists of storing and annotating commonly used\\nspeech or text (Himmelmann, 2008). However, it\\nis costly and requires specialists. In this process,\\ninvolving the community members that are bilin-\\ngual speakers is important (Bird, 2020).',\n",
       " '5 Low-resource MT\\nFor the purpose of this paper we define LRLs\\nas languages for which standard techniques are\\nunable to create well performing systems, which\\nmakes it necessary to resort to other techniques\\n(cf. Figure 1) such as transfer learning. For MT,\\nthe amount of available resources differs widely\\nacross language pairs: some have less than 10k\\nparallel sentences, while other have more than\\n500k, with some exceptions in the orders of sev-\\neral million.\\nEmulating a low-resource scenario by down-\\nsampling available data for high-resource lan-\\nguages is common and helps understanding a\\nmodels performance across different settings.\\nHowever, further evaluating methods on a diverse\\nset of low-resource languages is crucial, since\\nmany languages exhibit particular linguistic phe-\\nnomena (Mager et al., 2020), that perturb the fi-\\nnal results, especially since most large datasets\\nare from the Indo-European language family, to\\nwhich only 6.16% of the worlds languages belong\\n(Lewis, 2009).\\nImportantly, there is no strong correlation be-\\ntween the number of resources available per lan-\\nguage and the number of speakers: Javanese with\\n95 million speakers and Kannada with 44 million\\nare considered LRLs, while French, with only 64\\nmillion native speakers, is among the most widely\\nstudied languages. Improving models to handle\\nLRLs will extend access to information online as\\nwell as human language technology to all mono-\\nlingual speakers of those languages. In the case\\nof ILA, most languages are endangered at some\\ndegree, but most of them have the same issue:\\nthey are low resourced for parallel and monolin-\\ngual data.Endangered Languages Krauss (1992) esti-\\nmates that 50% of all languages are doomed or\\ndying, and that in this century we will see either\\nthe death or the doom of 90% of all human lan-\\nguages. The current proportion of languages that\\nare already extinct or moribund ranges from 31%\\ndown to 8% depending on the region, with the\\nmost severe cases in the Americas and Australia\\n(Simons and Lewis, 2013). To determine how en-\\ndangered a language is, Lewis and Simons (2010)\\nproposes a classification scale called EGIDS with\\n13 levels. The higher the number on this scale,\\nthe greater the level of disruption of the languages\\ninter-generational transmission.5MT for endan-\\ngered LRLs has the potential to help with doc-\\numentation, promotion and revitalization efforts\\n(Galla, 2016; Mager et al., 2018b). However, as\\nthese languages are commonly spoken by small\\ncommunities, or indigenous people, researchers\\nshould aim for a direct involvement of those com-\\nmunities (Bird, 2020).\\nWhat is polysynthesis? A polysynthetic lan-\\nguage is defined by the following linguistic fea-\\ntures: the verb in a polysynthetic language must\\nhave an agreement with the subject, objects and\\nindirect objects (Baker, 1996); nouns can be in-\\ncorporated into the complex verb morphology\\n(Mithun, 1986); and, therefore, polysynthetic lan-\\nguages have agreement morphemes, pronominal\\naffixes and incorporated roots in the verb (Baker,\\n1996), and also encode their relations and charac-\\nterizations into that verb. The most common word\\norders present in these languages are SOV , VSO,\\nSVO and free order. It is important to notice that\\na polysynthtic language can have a aggutinative6\\nor can have also fusional characteristics, like To-\\ntonaco or Tepehua (Mager et al., 2020).',\n",
       " '8 Ethical aspects\\nWhen working with ILAs are also interacting with\\ncommunities and nations that speak these lan-\\nguages. In most cases, these speakers have been\\nexposed to a colonial past, or to a local oppres-\\nsion, by the majority language and culture. It is\\nimportant to point to best practices and recom-\\nmendations when performing our research. Bird\\n(2020) and Liu et al. (2022) advocate to include\\ncommunity members as co-authors (Liu et al.,\\n2022) as well as considering data and technology\\nsovereignty. This is also aligned with the com-\\nmunity building aimed at by Zhang et al. (2022).\\nMager et al. (2023) summarizes the main aspects\\nthat should be considered as follows: i) Consul-\\ntation, Negotiation and Mutual Understanding . It\\nis important to inform the community about the\\nplanned research, negotiating a possible outcome,\\nand reaching a mutual agreement on the direc-\\ntions and details of the project should happen in\\nall cases. ii) Respect of the local culture and in-\\nvolvement . As each community has its own cul-\\nture and view of the world, researchers should be\\nfamiliar with the history and traditions of the com-\\nmunity. Also, it should be recommended that lo-\\ncal researchers, speakers, or internal governments\\nshould be involved in the project. iii) Sharing and\\ndistribution of data and research . The product\\nof the research should be available for use by the\\ncommunity, so they can take advantage of the gen-\\nerated materials, like papers, books, or data.',\n",
       " '9 Conclusion\\nMachine translation for ILA has gained interest in\\nthe NLP community over the last few years. Here,\\nwe provide an exhaustive overview of the basic\\nMT concepts and the particular challenges for MT\\nfor ILA (in the context of low-resource scenarios\\nand its relation to endangered languages). We ad-\\nditionally survey the current advances of MT for\\nthese languages.\\nLimitations\\nThis papers aim is to give an introduction to re-\\nsearchers, students, of interested community in-\\ndigenous community members to the topic of Ma-\\nchine Translation for Indigenous languages of the\\n8http://turing.iimas.unam.mx/\\namericasnlp/st.htmlAmericas. Therefore, this paper is not an in-depth\\nsurvey of the literature on indigenous languages\\nnor a more technical survey of low-resource ma-\\nchine translation. We would point the reader to\\nmore specific surveys on these aspects (Haddow\\net al., 2022; Mager et al., 2018b).\\nEthical statement\\nWe could not find any specific Ethical issue for\\nthis paper or potential danger. Nevertheless, we\\nwant to point to the reader that working with in-\\ndigenous languages (in this case, MT) implies a\\nset of ethical questions that are important to han-\\ndle. For a deeper understanding of the matter, we\\nsuggest specialized literature to the reader (Mager\\net al., 2023; Bird, 2020; Schwartz, 2022).\\nReferences\\nAhmed Abdelali, James Cowie, Steve Helmreich,\\nWanying Jin, Maria Pilar Milagros, Bill Ogden,\\nMansouri Rad, and Ron Zacharski. 2006. Guarani:\\nA case study in resource development for quick\\nramp-up MT. In Proceedings of the 7th Confer-\\nence of the Association for Machine Translation in\\nthe Americas: Technical Papers , pages 19, Cam-\\nbridge, Massachusetts, USA. Association for Ma-\\nchine Translation in the Americas.\\nIdris Abdulmumin, Bashir Shehu Galadanci, and Aliyu\\nGarba. 2019. Tag-less back-translation. arXiv\\npreprint arXiv:1912.10514 .\\nRoee Aharoni, Melvin Johnson, and Orhan Firat. 2019.\\nMassively multilingual neural machine translation.\\nInProceedings of the 2019 Conference of the North\\nAmerican Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies,\\nVolume 1 (Long and Short Papers) , pages 3874\\n3884.\\nBenyamin Ahmadnia and Bonnie J Dorr. 2019. Aug-\\nmenting neural machine translation through round-\\ntrip training approach. Open Computer Science ,\\n9(1):268278.\\nAntonios Anastasopoulos and David Chiang. 2018.\\nTied multitask learning for neural speech translation.\\nInProceedings of the 2018 Conference of the North\\nAmerican Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies,\\nVolume 1 (Long Papers) , pages 8291, New Orleans,\\nLouisiana. Association for Computational Linguis-\\ntics.\\nStephen R Anderson and Stephen Anderson. 2012.\\nLanguages: A very short introduction , volume 320.\\nOxford University Press.Naveen Arivazhagan, Ankur Bapna, Orhan Firat,\\nRoee Aharoni, Melvin Johnson, and Wolfgang\\nMacherey. 2019a. The missing ingredient in zero-\\nshot neural machine translation. arXiv preprint\\narXiv:1903.07091 .\\nNaveen Arivazhagan, Ankur Bapna, Orhan Firat,\\nDmitry Lepikhin, Melvin Johnson, Maxim Krikun,\\nMia Xu Chen, Yuan Cao, George Foster, Colin\\nCherry, et al. 2019b. Massively multilingual neural\\nmachine translation in the wild: Findings and chal-\\nlenges. arXiv preprint arXiv:1907.05019 .\\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and\\nKyunghyun Cho. 2018. Unsupervised neural ma-\\nchine translation. In 6th International Conference\\non Learning Representations, ICLR 2018 .\\nMikel Artetxe and Holger Schwenk. 2019. Mas-\\nsively multilingual sentence embeddings for zero-\\nshot cross-lingual transfer and beyond. Transac-\\ntions of the Association for Computational Linguis-\\ntics, 7:597610.\\nDuygu Ataman and Marcello Federico. 2018. Compo-\\nsitional representation of morphologically-rich input\\nfor neural machine translation. In Proceedings of\\nthe 56th Annual Meeting of the Association for Com-\\nputational Linguistics (Volume 2: Short Papers) ,\\npages 305311.\\nDuygu Ataman, Matteo Negri, Marco Turchi, and Mar-\\ncello Federico. 2017. Linguistically motivated vo-\\ncabulary reduction for neural machine translation\\nfrom turkish to english.\\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\\nton. 2016. Layer normalization. stat, 1050:21.\\nMark C Baker. 1996. The polysynthesis parameter .\\nOxford University Press.\\nMona Baker. 2018. In other words: A coursebook on\\ntranslation . Routledge.\\nLoc Barrault, Magdalena Biesialska, Ond rej Bojar,\\nMarta R. Costa-juss, Christian Federmann, Yvette\\nGraham, Roman Grundkiewicz, Barry Haddow,\\nMatthias Huck, Eric Joanis, Tom Kocmi, Philipp\\nKoehn, Chi-kiu Lo, Nikola Ljubei c, Christof\\nMonz, Makoto Morishita, Masaaki Nagata, Toshi-\\naki Nakazawa, Santanu Pal, Matt Post, and Marcos\\nZampieri. 2020. Findings of the 2020 conference on\\nmachine translation (WMT20). In Proceedings of\\nthe Fifth Conference on Machine Translation , pages\\n155, Online. Association for Computational Lin-\\nguistics.\\nAndrew R Barron. 1993. Universal approximation\\nbounds for superpositions of a sigmoidal func-\\ntion. IEEE Transactions on Information theory ,\\n39(3):930945.\\nChristos Baziotis, Barry Haddow, and Alexandra\\nBirch. 2020. Language model prior for low-\\nresource neural machine translation. arXiv preprint\\narXiv:2004.14928 .Yonatan Belinkov and Yonatan Bisk. 2018. Synthetic\\nand natural noise both break neural machine transla-\\ntion. In International Conference on Learning Rep-\\nresentations .\\nSteven Bird. 2020. Decolonising speech and lan-\\nguage technology. In Proceedings of the 28th Inter-\\nnational Conference on Computational Linguistics ,\\npages 35043519, Barcelona, Spain (Online). Inter-\\nnational Committee on Computational Linguistics.\\nGraeme Blackwood, Miguel Ballesteros, and Todd\\nWard. 2018. Multilingual neural machine transla-\\ntion with task-specific attention. In Proceedings of\\nthe 27th International Conference on Computational\\nLinguistics , pages 31123122.\\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\\nTomas Mikolov. 2017. Enriching word vectors with\\nsubword information. Transactions of the Associa-\\ntion for Computational Linguistics , 5:135146.\\nMarcel Bollmann, Rahul Aralikatte, Hctor Murri-\\neta Bello, Daniel Hershcovich, Miryam de Lhoneux,\\nand Anders Sgaard. 2021. Moses and the\\ncharacter-based random babbling baseline:\\nCoAStaL at AmericasNLP 2021 shared task.\\nInProceedings of the First Workshop on Natural\\nLanguage Processing for Indigenous Languages of\\nthe Americas , pages 248254, Online. Association\\nfor Computational Linguistics.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\\nGretchen Krueger, Tom Henighan, Rewon Child,\\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\\nClemens Winter, Christopher Hesse, Mark Chen,\\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\\nChess, Jack Clark, Christopher Berner, Sam Mc-\\nCandlish, Alec Radford, Ilya Sutskever, and Dario\\nAmodei. 2020. Language models are few-shot\\nlearners.\\nGina Bustamante, Arturo Oncevay, and Roberto\\nZariquiey. 2020. No data to crawl? monolingual\\ncorpus creation from PDF files of truly low-resource\\nlanguages in Peru. In Proceedings of the 12th Lan-\\nguage Resources and Evaluation Conference , pages\\n29142923, Marseille, France. European Language\\nResources Association.\\nLyle Campbell. 2000. American Indian languages: the\\nhistorical linguistics of Native America , volume 4.\\nOxford University Press on Demand.\\nRich Caruana. 1997. Multitask learning. Machine\\nlearning , 28(1):4175.\\nIsaac Caswell, Ciprian Chelba, and David Grangier.\\n2019. Tagged back-translation. In Proceedings of\\nthe Fourth Conference on Machine Translation (Vol-\\nume 1: Research Papers) , pages 5363.Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao.\\n2020. Evaluation of text generation: A survey.\\narXiv preprint arXiv:2006.14799 .\\nzlem etino glu. 2017. A code-switching corpus of\\nTurkish-German conversations. In Proceedings of\\nthe 11th Linguistic Annotation Workshop , pages 34\\n40, Valencia, Spain. Association for Computational\\nLinguistics.\\nBharathi Raja Chakravarthi, Ruba Priyadharshini,\\nShubhanker Banerjee, Richard Saldanha, John P.\\nMcCrae, Anand Kumar M, Parameswari Krishna-\\nmurthy, and Melvin Johnson. 2021. Findings of the\\nshared task on machine translation in Dravidian lan-\\nguages. In Proceedings of the First Workshop on\\nSpeech and Language Technologies for Dravidian\\nLanguages , pages 119125, Kyiv. Association for\\nComputational Linguistics.\\nEirini Chatzikoumi. 2020. How to evaluate machine\\ntranslation: A review of automated and human met-\\nrics. Natural Language Engineering , 26(2):137\\n161.\\nGuanhua Chen, Shuming Ma, Yun Chen, Li Dong,\\nDongdong Zhang, Jia Pan, Wenping Wang, and Furu\\nWei. 2021. Zero-shot cross-lingual transfer of neu-\\nral machine translation with multilingual pretrained\\nencoders. In Proceedings of the 2021 Conference on\\nEmpirical Methods in Natural Language Process-\\ning, pages 1526, Online and Punta Cana, Domini-\\ncan Republic. Association for Computational Lin-\\nguistics.\\nYong Cheng. 2019. Joint training for pivot-based neu-\\nral machine translation. In Joint Training for Neural\\nMachine Translation , pages 4154. Springer.\\nYong Cheng, Lu Jiang, and Wolfgang Macherey. 2019.\\nRobust neural machine translation with doubly ad-\\nversarial inputs. In Proceedings of the 57th Annual\\nMeeting of the Association for Computational Lin-\\nguistics , pages 43244333.\\nYong Cheng, Lu Jiang, Wolfgang Macherey, and Ja-\\ncob Eisenstein. 2020. AdvAug: Robust adversar-\\nial augmentation for neural machine translation. In\\nProceedings of the 58th Annual Meeting of the Asso-\\nciation for Computational Linguistics , pages 5961\\n5970, Online. Association for Computational Lin-\\nguistics.\\nYong Cheng, Zhaopeng Tu, Fandong Meng, Junjie\\nZhai, and Yang Liu. 2018. Towards robust neural\\nmachine translation. In Proceedings of the 56th An-\\nnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers) , pages 1756\\n1766.\\nLuis Chiruzzo, Pedro Amarilla, Adolfo Ros, and Gus-\\ntavo Gimnez Lugo. 2020. Development of a\\nGuarani - Spanish parallel corpus. In Proceedings of\\nthe 12th Language Resources and Evaluation Con-\\nference , pages 26292633, Marseille, France. Euro-\\npean Language Resources Association.Luis Chiruzzo, Santiago Gngora, Aldo Alvarez, Gus-\\ntavo Gimnez-Lugo, Marvin Agero-Torales, and\\nYliana Rodrguez. 2022. Jojajovai: A parallel\\nguarani-spanish corpus for mt benchmarking. In\\nProceedings of the Thirteenth Language Resources\\nand Evaluation Conference , pages 20982107.\\nKyunghyun Cho, Bart van Merrinboer, Caglar Gul-\\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\\nSchwenk, and Yoshua Bengio. 2014. Learning\\nphrase representations using rnn encoderdecoder\\nfor statistical machine translation. In Proceedings of\\nthe 2014 Conference on Empirical Methods in Nat-\\nural Language Processing (EMNLP) , pages 1724\\n1734.\\nTrevor Cohn and Mirella Lapata. 2007. Machine trans-\\nlation by triangulation: Making effective use of\\nmulti-parallel corpora. In Proceedings of the 45th\\nAnnual Meeting of the Association of Computational\\nLinguistics , pages 728735.\\nAlexis Conneau and Guillaume Lample. 2019. Cross-\\nlingual language model pretraining. In Advances\\nin Neural Information Processing Systems , pages\\n70577067.\\nAlexis Conneau, Guillaume Lample, MarcAurelio\\nRanzato, Ludovic Denoyer, and Herv Jgou. 2017.\\nWord translation without parallel data. arXiv\\npreprint arXiv:1710.04087 .\\nRaj Dabre, Chenhui Chu, and Anoop Kunchukuttan.\\n2019. A survey of multilingual neural machine\\ntranslation. arXiv preprint arXiv:1905.05395 .\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. Bert: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies, Volume 1 (Long and Short Papers) , pages\\n41714186.\\nXiangyu Duan, Baijun Ji, Hao Jia, Min Tan, Min\\nZhang, Boxing Chen, Weihua Luo, and Yue Zhang.\\n2020. Bilingual dictionary based neural machine\\ntranslation without using parallel sentences. In Pro-\\nceedings of the 58th Annual Meeting of the Asso-\\nciation for Computational Linguistics , pages 1570\\n1579.\\nAbteen Ebrahim, Manuel Mager, Pavel Oncevay Ar-\\nturo Danni Liu Koneru Sai Ugan Enes Yavuz\\nWiemerslage, Adam Denisov, Zhaolin Li, Jan\\nNiehues, Monica Romero, Ivan G Torre, Tanel\\nAlume, Jiaming Kong, Sergey Polezhaev, Yury\\nBelousov, Wei-Rui Chen, Peter Sullivan, Ife\\nAdebara, Bashar Talafha, Inciarte Alcides Al-\\ncoba, Muhammad Abdul-Mageed, Luis Chiruzzo,\\nRolando Coto-Solano, Hilaria Cruz, Sofa Flores-\\nSolrzano, Aldo Andrs Alvarez Lpez, Ivan Meza-\\nRuiz, John E. Ortega, Alexis Palmer, Rodolfo Joel\\nZevallos Salazar, Kristine, Thang Vu Stenzel, andKatharina Kann. 2023. Findings of the second amer-\\nicasnlp competition on speech-to-text translation.\\npreprint .\\nAbteen Ebrahimi, Manuel Mager, Arturo Once-\\nvay, Vishrav Chaudhary, Luis Chiruzzo, Angela\\nFan, John Ortega, Ricardo Ramos, Annette Rios,\\nIvan Vladimir Meza Ruiz, Gustavo Gimnez-Lugo,\\nElisabeth Mager, Graham Neubig, Alexis Palmer,\\nRolando Coto-Solano, Thang Vu, and Katharina\\nKann. 2022. AmericasNLI: Evaluating zero-shot\\nnatural language understanding of pretrained multi-\\nlingual models in truly low-resource languages. In\\nProceedings of the 60th Annual Meeting of the As-\\nsociation for Computational Linguistics (Volume 1:\\nLong Papers) , pages 62796299, Dublin, Ireland.\\nAssociation for Computational Linguistics.\\nAbteen Ebrahimi, Manuel Mager, Arturo Oncevay,\\nVishrav Chaudhary, Luis Chiruzzo, Angela Fan,\\nJohn Ortega, Ricardo Ramos, Annette Rios, Ivan\\nVladimir, Gustavo A. Gimnez-Lugo, Elisabeth\\nMager, Graham Neubig, Alexis Palmer, Rolando\\nA. Coto Solano, Ngoc Thang Vu, and Katharina\\nKann. 2021. Americasnli: Evaluating zero-shot nat-\\nural language understanding of pretrained multilin-\\ngual models in truly low-resource languages.\\nAbteen Ebrahimi, Manuel Mager, Arturo Oncevay,\\nEnora Rice, Cynthia Montao, John Ortega, Shruti\\nRijhwani, Alexis Palmer, Rolando Coto-Solano, Hi-\\nlaria Cruz, and Katharina Kann. 2023. Findings\\nof the AmericasNLP 2023 shared task on machine\\ntranslation into indigenous languages. In Proceed-\\nings of the Third Workshop on Natural Language\\nProcessing for Indigenous Languages of the Amer-\\nicas. Association for Computational Linguistics.\\nJavid Ebrahimi, Daniel Lowd, and Dejing Dou. 2018.\\nOn adversarial examples for character-level neural\\nmachine translation. In Proceedings of the 27th In-\\nternational Conference on Computational Linguis-\\ntics, pages 653663, Santa Fe, New Mexico, USA.\\nAssociation for Computational Linguistics.\\nSergey Edunov, Alexei Baevski, and Michael Auli.\\n2019. Pre-trained language model representations\\nfor language generation. In Proceedings of the 2019\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long and Short\\nPapers) , pages 40524059.\\nSergey Edunov, Myle Ott, Michael Auli, and David\\nGrangier. 2018. Understanding back-translation at\\nscale. In Proceedings of the 2018 Conference on\\nEmpirical Methods in Natural Language Process-\\ning, pages 489500.\\nMarzieh Fadaee, Arianna Bisazza, and Christof Monz.\\n2017. Data augmentation for low-resource neural\\nmachine translation. In Proceedings of the 55th An-\\nnual Meeting of the Association for Computational\\nLinguistics (Volume 2: Short Papers) , pages 567\\n573.Isaac Feldman and Rolando Coto-Solano. 2020. Neu-\\nral machine translation models with back-translation\\nfor the extremely low-resource indigenous language\\nBribri. In Proceedings of the 28th International\\nConference on Computational Linguistics , pages\\n39653976, Barcelona, Spain (Online). Interna-\\ntional Committee on Computational Linguistics.\\nManuel Fernndez-Delgado, Eva Cernadas, Senn\\nBarro, and Dinani Amorim. 2014. Do we need hun-\\ndreds of classifiers to solve real world classification\\nproblems? The journal of machine learning re-\\nsearch , 15(1):31333181.\\nAlexander Fraser. 2020. Findings of the WMT 2020\\nshared tasks in unsupervised MT and very low re-\\nsource supervised MT. In Proceedings of the Fifth\\nConference on Machine Translation , pages 765\\n771, Online. Association for Computational Lin-\\nguistics.\\nAna-Paula Galarreta, Andrs Melgar, and Arturo On-\\ncevay. 2017. Corpus creation and initial SMT ex-\\nperiments between Spanish and Shipibo-konibo. In\\nProceedings of the International Conference Recent\\nAdvances in Natural Language Processing, RANLP\\n2017 , pages 238244, Varna, Bulgaria. INCOMA\\nLtd.\\nCandace Kaleimamoowahinekapu Galla. 2016. Indige-\\nnous language revitalization, promotion, and educa-\\ntion: Function of digital technology. Computer As-\\nsisted Language Learning , 29(7):11371151.\\nXavier Garcia, Pierre Foret, Thibault Sellam, and\\nAnkur P Parikh. 2020. A multilingual view of\\nunsupervised machine translation. arXiv preprint\\narXiv:2002.02955 .\\nMozhdeh Gheini, Xiang Ren, and Jonathan May. 2021.\\nCross-attention is all you need: Adapting pretrained\\nTransformers for machine translation. In Proceed-\\nings of the 2021 Conference on Empirical Methods\\nin Natural Language Processing , pages 17541765,\\nOnline and Punta Cana, Dominican Republic. Asso-\\nciation for Computational Linguistics.\\nSantiago Gngora, Nicols Giossa, and Luis Chiruzzo.\\n2021. Experiments on a Guarani corpus of news\\nand social media. In Proceedings of the First Work-\\nshop on Natural Language Processing for Indige-\\nnous Languages of the Americas , pages 153158,\\nOnline. Association for Computational Linguistics.\\nSantiago Gngora, Nicols Giossa, and Luis Chiruzzo.\\n2022. Can we use word embeddings for enhancing\\nGuarani-Spanish machine translation? In Proceed-\\nings of the Fifth Workshop on the Use of Compu-\\ntational Methods in the Study of Endangered Lan-\\nguages , pages 127132, Dublin, Ireland. Associa-\\ntion for Computational Linguistics.\\nYvette Graham, Barry Haddow, and Philipp Koehn.\\n2020. Statistical power and translationese in ma-\\nchine translation evaluation. In Proceedings of the2020 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP) , pages 7281, On-\\nline. Association for Computational Linguistics.\\nStig-Arne Grnroos, Sami Virpioja, Peter Smit, and\\nMikko Kurimo. 2014. Morfessor flatcat: An hmm-\\nbased method for unsupervised and semi-supervised\\nlearning of morphology. In Proceedings of COLING\\n2014, the 25th International Conference on Compu-\\ntational Linguistics: Technical Papers , pages 1177\\n1185.\\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Vic-\\ntor OK Li. 2019. Improved zero-shot neural ma-\\nchine translation via ignoring spurious correlations.\\nInProceedings of the 57th Annual Meeting of the\\nAssociation for Computational Linguistics , pages\\n12581268.\\nCaglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun\\nCho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,\\nHolger Schwenk, and Yoshua Bengio. 2015. On us-\\ning monolingual corpora in neural machine transla-\\ntion. arXiv preprint arXiv:1503.03535 .\\nXimena Gutierrez-Vasques, Gerardo Sierra, and\\nIsaac Hernandez Pompa. 2016. Axolotl: a web\\naccessible parallel corpus for Spanish-Nahuatl. In\\nProceedings of the Tenth International Conference\\non Language Resources and Evaluation (LREC16) ,\\npages 42104214, Portoro, Slovenia. European\\nLanguage Resources Association (ELRA).\\nBarry Haddow, Rachel Bawden, Antonio Valerio\\nMiceli Barone, Jind rich Helcl, and Alexandra Birch.\\n2022. Survey of low-resource machine translation.\\nComputational Linguistics , pages 167.\\nLifeng Han. 2016. Machine translation evaluation re-\\nsources and methods: A survey. arXiv preprint\\narXiv:1605.04515 .\\nFranois Hernandez and Vincent Nguyen. 2020. The\\nubiqus English-Inuktitut system for WMT20. In\\nProceedings of the Fifth Conference on Machine\\nTranslation , pages 213217, Online. Association for\\nComputational Linguistics.\\nNikolaus P Himmelmann. 2008. Language documen-\\ntation: What is it and what is it good for? In Es-\\nsentials of language documentation , pages 130. De\\nGruyter Mouton.\\nVu Cong Duy Hoang, Philipp Koehn, Gholamreza\\nHaffari, and Trevor Cohn. 2018. Iterative back-\\ntranslation for neural machine translation. In Pro-\\nceedings of the 2nd Workshop on Neural Machine\\nTranslation and Generation , pages 1824.\\nSepp Hochreiter and Jrgen Schmidhuber. 1997.\\nLong short-term memory. Neural computation ,\\n9(8):17351780.\\nJ Edward Hu, Huda Khayrallah, Ryan Culkin, Patrick\\nXia, Tongfei Chen, Matt Post, and Benjamin\\nVan Durme. 2019. Improved lexically constraineddecoding for translation and monolingual rewriting.\\nInProceedings of the 2019 Conference of the North\\nAmerican Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies,\\nVolume 1 (Long and Short Papers) , pages 839850.\\nW John Hutchins. 2004. The georgetown-ibm experi-\\nment demonstrated in january 1954. In Conference\\nof the Association for Machine Translation in the\\nAmericas , pages 102114. Springer.\\nSbastien Jean, Kyunghyun Cho, Roland Memisevic,\\nand Yoshua Bengio. 2015. On using very large\\ntarget vocabulary for neural machine translation.\\nInProceedings of the 53rd Annual Meeting of the\\nAssociation for Computational Linguistics and the\\n7th International Joint Conference on Natural Lan-\\nguage Processing (Volume 1: Long Papers) , pages\\n110, Beijing, China. Association for Computa-\\ntional Linguistics.\\nEric Joanis, Rebecca Knowles, Roland Kuhn, Samuel\\nLarkin, Patrick Littell, Chi-kiu Lo, Darlene Stewart,\\nand Jeffrey Micher. 2020. The Nunavut Hansard\\nInuktitutEnglish parallel corpus 3.0 with prelimi-\\nnary machine translation results. In Proceedings of\\nthe 12th Language Resources and Evaluation Con-\\nference , pages 25622572, Marseille, France. Euro-\\npean Language Resources Association.\\nMelvin Johnson, Mike Schuster, Quoc V Le, Maxim\\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\\nFernanda Vigas, Martin Wattenberg, Greg Corrado,\\net al. 2017. Googles multilingual neural machine\\ntranslation system: Enabling zero-shot translation.\\nTransactions of the Association for Computational\\nLinguistics , 5:339351.\\nDavid Kamholz, Jonathan Pool, and Susan M Colow-\\nick. 2014. Panlex: Building a resource for panlin-\\ngual lexical translation. In LREC , pages 31453150.\\nKatharina Kann, Jesus Manuel Mager Hois,\\nIvan Vladimir Meza-Ruiz, and Hinrich Schtze.\\n2018. Fortification of neural morphological\\nsegmentation models for polysynthetic minimal-\\nresource languages. In Proceedings of the 2018\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long Papers) ,\\npages 4757, New Orleans, Louisiana. Association\\nfor Computational Linguistics.\\nKenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua\\nBengio. 2017. Generalization in deep learning.\\narXiv preprint arXiv:1710.05468 .\\nHuda Khayrallah, Brian Thompson, Matt Post, and\\nPhilipp Koehn. 2020. Simulated multiple reference\\ntraining improves low-resource machine translation.\\narXiv preprint arXiv:2004.14524 .\\nRebecca Knowles, Darlene Stewart, Samuel Larkin,\\nand Patrick Littell. 2020. NRC systems for the 2020\\nInuktitut-English news translation task. In Proceed-\\nings of the Fifth Conference on Machine Translation ,pages 156170, Online. Association for Computa-\\ntional Linguistics.\\nRebecca Knowles, Darlene Stewart, Samuel Larkin,\\nand Patrick Littell. 2021. NRC-CNRC machine\\ntranslation systems for the 2021 AmericasNLP\\nshared task. In Proceedings of the First Workshop on\\nNatural Language Processing for Indigenous Lan-\\nguages of the Americas , pages 224233, Online. As-\\nsociation for Computational Linguistics.\\nSosuke Kobayashi. 2018. Contextual augmentation:\\nData augmentation by words with paradigmatic re-\\nlations. In Proceedings of the 2018 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies, Volume 2 (Short Papers) , pages 452457.\\nTom Kocmi. 2020. CUNI submission for the Inuk-\\ntitut language in WMT news 2020. In Proceed-\\nings of the Fifth Conference on Machine Translation ,\\npages 171174, Online. Association for Computa-\\ntional Linguistics.\\nMichael Krauss. 1992. The worlds languages in crisis.\\nLanguage , 68(1):410.\\nMateusz Krubi nski, Marcin Chochowski, Bartomiej\\nBoczek, Mikoaj Koszowski, Adam Dobrowolski,\\nMarcin Szyma nski, and Pawe Przybysz. 2020.\\nSamsung R&D institute Poland submission to\\nWMT20 news translation task. In Proceedings of\\nthe Fifth Conference on Machine Translation , pages\\n181190, Online. Association for Computational\\nLinguistics.\\nSurafel M Lakew, Quintino F Lotito, Matteo Negri,\\nMarco Turchi, and Marcello Federico. 2018. Im-\\nproving zero-shot translation of low-resource lan-\\nguages. In Proceedings of the 14h IWSLT , pages\\n113119.\\nGuillaume Lample, Alexis Conneau, Ludovic Denoyer,\\nand MarcAurelio Ranzato. 2017. Unsupervised\\nmachine translation using monolingual corpora only.\\narXiv preprint arXiv:1711.00043 .\\nSahinur Rahman Laskar, Abdullah Faiz Ur Rah-\\nman Khilji, Partha Pakray, and Sivaji Bandyopad-\\nhyay. 2020. Zero-shot neural machine translation:\\nRussian-Hindi @LoResMT 2020. In Proceedings\\nof the 3rd Workshop on Technologies for MT of Low\\nResource Languages , pages 3842, Suzhou, China.\\nAssociation for Computational Linguistics.\\nYichong Leng, Xu Tan, Tao Qin, Xiang-Yang Li, and\\nTie-Yan Liu. 2019. Unsupervised pivot translation\\nfor distant languages. In Proceedings of the 57th An-\\nnual Meeting of the Association for Computational\\nLinguistics , pages 175183.\\nM Paul Lewis. 2009. Ethnologue: Languages of the\\nworld . SIL international.\\nM Paul Lewis and Gary F Simons. 2010. Assessing\\nendangerment: expanding fishmans gids. Revue\\nroumaine de linguistique , 55(2):103120.Mike Lewis, Yinhan Liu, Naman Goyal, Mar-\\njan Ghazvininejad, Abdelrahman Mohamed, Omer\\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\\nBart: Denoising sequence-to-sequence pre-training\\nfor natural language generation, translation, and\\ncomprehension. arXiv preprint arXiv:1910.13461 .\\nZuchao Li, Rui Wang, Kehai Chen, Masso Utiyama,\\nEiichiro Sumita, Zhuosheng Zhang, and Hai Zhao.\\n2020. Data-dependent gaussian prior objective for\\nlanguage generation. In International Conference\\non Learning Representations .\\nZehui Lin, Xiao Pan, Mingxuan Wang, Xipeng Qiu,\\nJiangtao Feng, Hao Zhou, and Lei Li. 2020. Pre-\\ntraining multilingual neural machine translation by\\nleveraging alignment information. In Proceed-\\nings of the 2020 Conference on Empirical Methods\\nin Natural Language Processing (EMNLP) , pages\\n26492663, Online. Association for Computational\\nLinguistics.\\nWang Ling, Isabel Trancoso, Chris Dyer, and Alan W\\nBlack. 2015. Character-based neural machine trans-\\nlation. arXiv preprint arXiv:1511.04586 .\\nPatrick Littell, Anna Kazantseva, Roland Kuhn, Aidan\\nPine, Antti Arppe, Christopher Cox, and Marie-\\nOdile Junker. 2018. Indigenous language technolo-\\ngies in Canada: Assessment, challenges, and suc-\\ncesses. In Proceedings of the 27th International\\nConference on Computational Linguistics , pages\\n26202632, Santa Fe, New Mexico, USA. Associ-\\nation for Computational Linguistics.\\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\\nLuke Zettlemoyer. 2020. Multilingual denoising\\npre-training for neural machine translation. arXiv\\npreprint arXiv:2001.08210 .\\nZihan Liu, Yan Xu, Genta Indra Winata, and Pascale\\nFung. 2019. Incorporating word and subword units\\nin unsupervised machine translation using language\\nmodel rescoring. In Proceedings of the Fourth Con-\\nference on Machine Translation (Volume 2: Shared\\nTask Papers, Day 1) , pages 275282.\\nZoey Liu, Crystal Richardson, Richard Hatcher Jr, and\\nEmily Prudhommeaux. 2022. Not always about\\nyou: Prioritizing community needs when develop-\\ning endangered language technology. arXiv preprint\\narXiv:2204.05541 .\\nChi-kiu Lo. 2020. Extended study on using pretrained\\nlanguage models and YiSi-1 for machine transla-\\ntion evaluation. In Proceedings of the Fifth Confer-\\nence on Machine Translation , pages 895902, On-\\nline. Association for Computational Linguistics.\\nWolfgang Lrscher. 2005. The translation process:\\nMethods and problems of its investigation. Meta:\\njournal des traducteurs/Meta: Translators Journal ,\\n50(2):597608.Bruce T Lowerre. 1976. The harpy speech recognition\\nsystem. Technical report, CARNEGIE-MELLON\\nUNIV PITTSBURGH PA DEPT OF COMPUTER\\nSCIENCE.\\nYichao Lu, Phillip Keung, Faisal Ladhak, Vikas Bhard-\\nwaj, Shaonan Zhang, and Jason Sun. 2018. A neu-\\nral interlingua for multilingual machine translation.\\nInProceedings of the Third Conference on Machine\\nTranslation: Research Papers , pages 8492.\\nThang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals,\\nand Wojciech Zaremba. 2015. Addressing the rare\\nword problem in neural machine translation. In Pro-\\nceedings of the 53rd Annual Meeting of the Associ-\\nation for Computational Linguistics and the 7th In-\\nternational Joint Conference on Natural Language\\nProcessing (Volume 1: Long Papers) , pages 1119,\\nBeijing, China. Association for Computational Lin-\\nguistics.\\nManuel Mager, Dinico Carrillo, and Ivan Meza.\\n2018a. Probabilistic finite-state morphological seg-\\nmenter for wixarika (huichol) language. Journal of\\nIntelligent & Fuzzy Systems , 34(5):30813087.\\nManuel Mager, zlem etino glu, and Katharina Kann.\\n2019. Subword-level language identification for\\nintra-word code-switching. In Proceedings of the\\n2019 Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Hu-\\nman Language Technologies, Volume 1 (Long and\\nShort Papers) , pages 20052011, Minneapolis, Min-\\nnesota. Association for Computational Linguistics.\\nManuel Mager, zlem etino glu, and Katharina\\nKann. 2020. Tackling the low-resource chal-\\nlenge for canonical segmentation. arXiv preprint\\narXiv:2010.02804 .\\nManuel Mager, Ximena Gutierrez-Vasques, Gerardo\\nSierra, and Ivan Meza-Ruiz. 2018b. Challenges of\\nlanguage technologies for the indigenous languages\\nof the Americas. In Proceedings of the 27th Inter-\\nnational Conference on Computational Linguistics ,\\npages 5569, Santa Fe, New Mexico, USA. Associ-\\nation for Computational Linguistics.\\nManuel Mager, Elisabeth Mager, Katharina Kann,\\nand Ngoc Thang Vu. 2023. Ethical considerations\\nfor machine translation of indigenous languages:\\nGiving a voice to the speakers. arXiv preprint\\narXiv:2305.19474 .\\nManuel Mager, Elisabeth Mager, Alfonso Medina-\\nUrrea, Ivan Vladimir Meza Ruiz, and Katharina\\nKann. 2018c. Lost in translation: Analysis of in-\\nformation loss during machine translation between\\npolysynthetic and fusional languages. In Proceed-\\nings of the Workshop on Computational Modeling\\nof Polysynthetic Languages , pages 7383, Santa Fe,\\nNew Mexico, USA. Association for Computational\\nLinguistics.Manuel Mager and Ivan Meza. 2021. Retos en con-\\nstruccin de traductores automticos para lenguas\\nindgenas de Mxico. Digital Scholarship in the Hu-\\nmanities , 36.\\nManuel Mager, Arturo Oncevay, Abteen Ebrahimi,\\nJohn Ortega, Annette Rios, Angela Fan, Xi-\\nmena Gutierrez-Vasques, Luis Chiruzzo, Gustavo\\nGimnez-Lugo, Ricardo Ramos, Anna Currey,\\nVishrav Chaudhary, Ivan Vladimir Meza Ruiz,\\nRolando Coto-Solano, Alexis Palmer, Elisabeth\\nMager, Ngoc Thang Vu, Graham Neubig, and\\nKatharina Kann. 2021. Findings of the Americas-\\nNLP 2021 Shared Task on Open Machine Transla-\\ntion for Indigenous Languages of the Americas. In\\nProceedings of theThe First Workshop on NLP for\\nIndigenous Languages of the Americas , Online. As-\\nsociation for Computational Linguistics.\\nManuel Mager, Arturo Oncevay, Elisabeth Mager,\\nKatharina Kann, and Thang Vu. 2022. BPE vs. mor-\\nphological segmentation: A case study on machine\\ntranslation of four polysynthetic languages. In Find-\\nings of the Association for Computational Linguis-\\ntics: ACL 2022 , pages 961971, Dublin, Ireland.\\nAssociation for Computational Linguistics.\\nChaitanya Malaviya, Graham Neubig, and Patrick Lit-\\ntell. 2017. Learning language representations for ty-\\npology prediction. In Proceedings of the 2017 Con-\\nference on Empirical Methods in Natural Language\\nProcessing , pages 25292535.\\nBenjamin Marie, Raphael Rubino, and Atsushi Fujita.\\n2020. Tagged back-translation revisited: Why does\\nit really work? In Proceedings of the 58th Annual\\nMeeting of the Association for Computational Lin-\\nguistics , pages 59905997, Online. Association for\\nComputational Linguistics.\\nJoel Martin, Howard Johnson, Benoit Farley, and Anna\\nMaclachlan. 2003. Aligning and using an english-\\ninuktitut parallel corpus. In Proceedings of the HLT-\\nNAACL 2003 Workshop on Building and using par-\\nallel texts: data driven machine translation and\\nbeyond-Volume 3 , pages 115118. Association for\\nComputational Linguistics.\\nThomas Mayer and Michael Cysouw. 2014. Creat-\\ning a massively parallel bible corpus. Oceania ,\\n135(273):40.\\nArya D. McCarthy, Rachel Wicks, Dylan Lewis,\\nAaron Mueller, Winston Wu, Oliver Adams, Gar-\\nrett Nicolai, Matt Post, and David Yarowsky. 2020.\\nThe johns hopkins university bible corpus: 1600+\\ntongues for typological exploration. In Proceedings\\nof The 12th Language Resources and Evaluation\\nConference , pages 28842892, Marseille, France.\\nEuropean Language Resources Association.\\nNorman A McQuown. 1955. The indigenous lan-\\nguages of latin america. American Anthropologist ,\\n57(3):501570.Antonio Valerio Miceli-Barone, Jind rich Helcl, Rico\\nSennrich, Barry Haddow, and Alexandra Birch.\\n2017. Deep architectures for neural machine trans-\\nlation. In Proceedings of the Second Conference on\\nMachine Translation , pages 99107.\\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\\nfrey Dean. 2013. Efficient estimation of word\\nrepresentations in vector space. arXiv preprint\\narXiv:1301.3781 .\\nMarianne Mithun. 1986. On the nature of noun incor-\\nporation. Language , 62(1):3237.\\nOscar Moreno. 2021. The REPU CS Spanish\\nQuechua submission to the AmericasNLP 2021\\nshared task on open machine translation. In Pro-\\nceedings of the First Workshop on Natural Language\\nProcessing for Indigenous Languages of the Ameri-\\ncas, pages 241247, Online. Association for Com-\\nputational Linguistics.\\nEl Moatez Billah Nagoudi, Wei-Rui Chen, Muham-\\nmad Abdul-Mageed, and Hasan Cavusoglu. 2021.\\nIndT5: A text-to-text transformer for 10 indigenous\\nlanguages. In Proceedings of the First Workshop on\\nNatural Language Processing for Indigenous Lan-\\nguages of the Americas , pages 265271, Online. As-\\nsociation for Computational Linguistics.\\nWilhelmina Nekoto, Vukosi Marivate, Tshinondiwa\\nMatsila, Timi Fasubaa, Taiwo Fagbohungbe,\\nSolomon Oluwole Akinola, Shamsuddeen Muham-\\nmad, Salomon Kabongo Kabenamualu, Salomey\\nOsei, Freshia Sackey, Rubungo Andre Niyongabo,\\nRicky Macharm, Perez Ogayo, Orevaoghene Ahia,\\nMusie Meressa Berhe, Mofetoluwa Adeyemi,\\nMasabata Mokgesi-Selinga, Lawrence Okegbemi,\\nLaura Martinus, Kolawole Tajudeen, Kevin Degila,\\nKelechi Ogueji, Kathleen Siminyu, Julia Kreutzer,\\nJason Webster, Jamiil Toure Ali, Jade Abbott,\\nIroro Orife, Ignatius Ezeani, Idris Abdulkadir\\nDangana, Herman Kamper, Hady Elsahar, Good-\\nness Duru, Ghollah Kioko, Murhabazi Espoir,\\nElan van Biljon, Daniel Whitenack, Christopher\\nOnyefuluchi, Chris Chinenye Emezue, Bonaventure\\nF. P. Dossou, Blessing Sibanda, Blessing Bassey,\\nAyodele Olabiyi, Arshath Ramkilowan, Alp ktem,\\nAdewale Akinfaderin, and Abdallah Bashir. 2020.\\nParticipatory research for low-resourced machine\\ntranslation: A case study in African languages.\\nInFindings of the Association for Computational\\nLinguistics: EMNLP 2020 , pages 21442160,\\nOnline. Association for Computational Linguistics.\\nGraham Neubig. 2017. Neural machine translation\\nand sequence-to-sequence models: A tutorial. arXiv\\npreprint arXiv:1703.01619 .\\nGraham Neubig and Junjie Hu. 2018. Rapid adapta-\\ntion of neural machine translation to new languages.\\nInProceedings of the 2018 Conference on Empiri-\\ncal Methods in Natural Language Processing , pages\\n875880.Tan Ngoc Le and Fatiha Sadat. 2020. Revitalization\\nof indigenous languages through pre-processing and\\nneural machine translation: The case of Inuktitut.\\nInProceedings of the 28th International Conference\\non Computational Linguistics , pages 46614666,\\nBarcelona, Spain (Online). International Committee\\non Computational Linguistics.\\nDat Quoc Nguyen, Kairit Sirts, and Mark Johnson.\\n2015. Improving topic coherence with latent fea-\\nture word representations in MAP estimation for\\ntopic modeling. In Proceedings of the Australasian\\nLanguage Technology Association Workshop 2015 ,\\npages 116121, Parramatta, Australia.\\nToan Q Nguyen and David Chiang. 2017. Trans-\\nfer learning across low-resource, related languages\\nfor neural machine translation. In Proceedings of\\nthe Eighth International Joint Conference on Natu-\\nral Language Processing (Volume 2: Short Papers) ,\\npages 296301.\\nEugene Nida. 1945. Linguistics and ethnology in\\ntranslation-problems. Word , 1(2):194208.\\nJan Niehues and Eunah Cho. 2017. Exploiting linguis-\\ntic resources for neural machine translation using\\nmulti-task learning. In Proceedings of the Second\\nConference on Machine Translation , pages 8089,\\nCopenhagen, Denmark. Association for Computa-\\ntional Linguistics.\\nJan Niehues, Eunah Cho, Thanh-Le Ha, and Alex\\nWaibel. 2016. Pre-translation for neural machine\\ntranslation. In Proceedings of COLING 2016, the\\n26th International Conference on Computational\\nLinguistics: Technical Papers , pages 18281836,\\nOsaka, Japan. The COLING 2016 Organizing Com-\\nmittee.\\nFarhad Nooralahzadeh, Giannis Bekoulis, Johannes\\nBjerva, and Isabelle Augenstein. 2020. Zero-shot\\ncross-lingual transfer with meta learning. arXiv\\npreprint arXiv:2003.02739 .\\nAtul Kr. Ojha, Valentin Malykh, Alina Karakanta, and\\nChao-Hong Liu. 2020. Findings of the LoResMT\\n2020 shared task on zero-shot for low-resource lan-\\nguages. In Proceedings of the 3rd Workshop on\\nTechnologies for MT of Low Resource Languages ,\\npages 3337, Suzhou, China. Association for Com-\\nputational Linguistics.\\nMatthew Olson, Abraham Wyner, and Richard Berk.\\n2018. Modern neural networks generalize on small\\ndata sets. In Advances in Neural Information Pro-\\ncessing Systems , pages 36193628.\\nArturo Oncevay. 2021. Peru is multilingual, its ma-\\nchine translation should be too? In Proceedings\\nof the First Workshop on Natural Language Pro-\\ncessing for Indigenous Languages of the Americas ,\\npages 194201, Online. Association for Computa-\\ntional Linguistics.Iroro Orife, Julia Kreutzer, Blessing Sibanda, Daniel\\nWhitenack, Kathleen Siminyu, Laura Martinus,\\nJamiil Toure Ali, Jade Abbott, Vukosi Marivate,\\nSalomon Kabongo, et al. 2020. Masakhane\\nmachine translation for africa. arXiv preprint\\narXiv:2003.11529 .\\nJohn E Ortega, Richard Castro Mamani, and\\nKyunghyun Cho. 2020a. Neural machine translation\\nwith a polysynthetic low resource language. Ma-\\nchine Translation , 34(4):325346.\\nJohn E Ortega, Richard Alexander Castro-Mamani, and\\nJaime Rafael Montoya Samame. 2020b. Overcom-\\ning resistance: The normalization of an Amazonian\\ntribal language. In Proceedings of the 3rd Work-\\nshop on Technologies for MT of Low Resource Lan-\\nguages , pages 113, Suzhou, China. Association for\\nComputational Linguistics.\\nYirong Pan, Xiao Li, Yating Yang, and Rui Dong. 2020.\\nMulti-task neural model for agglutinative language\\ntranslation. In Proceedings of the 58th Annual Meet-\\ning of the Association for Computational Linguis-\\ntics: Student Research Workshop , pages 103110,\\nOnline. Association for Computational Linguistics.\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\\nJing Zhu. 2002. Bleu: a method for automatic eval-\\nuation of machine translation. In Proceedings of the\\n40th Annual Meeting of the Association for Com-\\nputational Linguistics , pages 311318, Philadelphia,\\nPennsylvania, USA. Association for Computational\\nLinguistics.\\nShantipriya Parida, Subhadarshi Panda, Amulya Dash,\\nEsau Villatoro-Tello, A. Seza Do gruz, Rosa M.\\nOrtega-Mendoza, Amadeo Hernndez, Yashvardhan\\nSharma, and Petr Motlicek. 2021. Open machine\\ntranslation for low resource South American lan-\\nguages (AmericasNLP 2021 shared task contribu-\\ntion). In Proceedings of the First Workshop on Natu-\\nral Language Processing for Indigenous Languages\\nof the Americas , pages 218223, Online. Associa-\\ntion for Computational Linguistics.\\nJeffrey Pennington, Richard Socher, and Christopher\\nManning. 2014. GloVe: Global vectors for word\\nrepresentation. In Proceedings of the 2014 Con-\\nference on Empirical Methods in Natural Language\\nProcessing (EMNLP) , pages 15321543, Doha,\\nQatar. Association for Computational Linguistics.\\nAsya Pereltsvaig. 2020. Languages of the World .\\nCambridge University Press.\\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\\nGardner, Christopher Clark, Kenton Lee, and Luke\\nZettlemoyer. 2018. Deep contextualized word rep-\\nresentations. In Proceedings of the 2018 Confer-\\nence of the North American Chapter of the Associ-\\nation for Computational Linguistics: Human Lan-\\nguage Technologies, Volume 1 (Long Papers) , pages\\n22272237.Alberto Poncelas, Maja Popovi c, Dimitar Shterionov,\\nGideon Maillette de Buy Wenniger, and Andy Way.\\n2019. Combining pbsmt and nmt back-translated\\ndata for efficient nmt. In Proceedings of the Inter-\\nnational Conference on Recent Advances in Natural\\nLanguage Processing (RANLP 2019) , pages 922\\n931.\\nMaja Popovi c. 2017. chrf++: words helping character\\nn-grams. In Proceedings of the second conference\\non machine translation , pages 612618.\\nNima Pourdamghani and Kevin Knight. 2019. Neigh-\\nbors helping the poor: improving low-resource ma-\\nchine translation using related languages. Machine\\nTranslation , 33(3):239258.\\nOfir Press and Lior Wolf. 2017. Using the output em-\\nbedding to improve language models. In Proceed-\\nings of the 15th Conference of the European Chap-\\nter of the Association for Computational Linguistics:\\nVolume 2, Short Papers , pages 157163.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J Liu. 2020. Exploring the limits\\nof transfer learning with a unified text-to-text trans-\\nformer. Journal of Machine Learning Research ,\\n21:167.\\nAlessandro Raganato, Ral Vzquez, Mathias Creutz,\\nand Jrg Tiedemann. 2021. An empirical investi-\\ngation of word alignment supervision for zero-shot\\nmultilingual neural machine translation. In Pro-\\nceedings of the 2021 Conference on Empirical Meth-\\nods in Natural Language Processing , pages 8449\\n8456, Online and Punta Cana, Dominican Republic.\\nAssociation for Computational Linguistics.\\nSurangika Ranathunga, En-Shiun Annie Lee, Mar-\\njana Prifti Skenduli, Ravi Shekhar, Mehreen Alam,\\nand Rishemjit Kaur. 2021. Neural machine trans-\\nlation for low-resource languages: A survey. arXiv\\npreprint arXiv:2106.15115 .\\nShuo Ren, Yu Wu, Shujie Liu, Ming Zhou, and\\nShuai Ma. 2020. A retrieve-and-rewrite initializa-\\ntion method for unsupervised machine translation.\\nInProceedings of the 58th Annual Meeting of the\\nAssociation for Computational Linguistics , pages\\n34983504, Online. Association for Computational\\nLinguistics.\\nParker Riley, Isaac Caswell, Markus Freitag, and David\\nGrangier. 2020. Translationese as a language in\\nmultilingual NMT. In Proceedings of the 58th An-\\nnual Meeting of the Association for Computational\\nLinguistics , pages 77377746, Online. Association\\nfor Computational Linguistics.\\nChristian Roest, Lukas Edman, Gosse Minnema, Kevin\\nKelly, Jennifer Spenader, and Antonio Toral. 2020.\\nMachine translation for EnglishInuktitut with seg-\\nmentation, data acquisition and pre-training. In\\nProceedings of the Fifth Conference on MachineTranslation , pages 274281, Online. Association for\\nComputational Linguistics.\\nDavid Rolnick, Andreas Veit, Serge Belongie, and Nir\\nShavit. 2017. Deep learning is robust to massive la-\\nbel noise. arXiv preprint arXiv:1705.10694 .\\nCarlos Barron Romero, Jess Manuel Mager Hois, and\\nFernando Reyes Avils. 2016. Richard feynman, los\\nalfabetos y los lenguajes. Relingstica aplicada ,\\n(19):2.\\nMnica Jasso Rosales, Manuel Mager, and Ivan\\nVladimir Meza Ruz. Towards a twitter corpus of\\nthe indigenous languages of the americas.\\nDevendra Singh Sachan and Graham Neubig. 2018.\\nParameter sharing methods for multilingual self-\\nattentional translation models. arXiv preprint\\narXiv:1809.00252 .\\nElizabeth Salesky, David Etter, and Matt Post. 2021.\\nRobust open-vocabulary translation from visual text\\nrepresentations. arXiv preprint arXiv:2104.08211 .\\nJonne Saleva and Constantine Lignos. 2021. The effec-\\ntiveness of morphology-aware segmentation in low-\\nresource neural machine translation. In Proceedings\\nof the 16th Conference of the European Chapter of\\nthe Association for Computational Linguistics: Stu-\\ndent Research Workshop , pages 164174, Online.\\nAssociation for Computational Linguistics.\\nMotoki Sano, Jun Suzuki, and Shun Kiyono. 2019. Ef-\\nfective adversarial regularization for neural machine\\ntranslation. In Proceedings of the 57th Annual Meet-\\ning of the Association for Computational Linguis-\\ntics, pages 204210.\\nYves Scherrer, Stig-Arne Grnroos, and Sami Virpi-\\noja. 2020. The University of Helsinki and aalto\\nuniversity submissions to the WMT 2020 news and\\nlow-resource translation tasks. In Proceedings of\\nthe Fifth Conference on Machine Translation , pages\\n11291138, Online. Association for Computational\\nLinguistics.\\nLane Schwartz. 2022. Primum non nocere: Before\\nworking with indigenous data, the acl must confront\\nongoing colonialism. In Proceedings of the 60th An-\\nnual Meeting of the Association for Computational\\nLinguistics (Volume 2: Short Papers) , pages 724\\n731.\\nLane Schwartz, Francis Tyers, Lori Levin, Christo\\nKirov, Patrick Littell, Chi-kiu Lo, Emily\\nPrudhommeaux, Hyunji Hayley Park, Ken-\\nneth Steimel, Rebecca Knowles, et al. 2020. Neural\\npolysynthetic language modelling. arXiv preprint\\narXiv:2005.05477 .\\nLee Sechrest, Todd L Fay, and SM Hafeez Zaidi. 1972.\\nProblems of translation in cross-cultural research.\\nJournal of cross-cultural psychology , 3(1):4156.Rico Sennrich, Barry Haddow, and Alexandra Birch.\\n2016a. Improving neural machine translation mod-\\nels with monolingual data. In Proceedings of the\\n54th Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers) , pages\\n8696, Berlin, Germany. Association for Computa-\\ntional Linguistics.\\nRico Sennrich, Barry Haddow, and Alexandra Birch.\\n2016b. Improving neural machine translation mod-\\nels with monolingual data. In Proceedings of the\\n54th Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers) , pages\\n8696.\\nRico Sennrich, Barry Haddow, and Alexandra Birch.\\n2016c. Neural machine translation of rare words\\nwith subword units. In Proceedings of the 54th An-\\nnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers) , pages 1715\\n1725, Berlin, Germany. Association for Computa-\\ntional Linguistics.\\nJoan Serra, Didac Suris, Marius Miron, and Alexan-\\ndros Karatzoglou. 2018. Overcoming catastrophic\\nforgetting with hard attention to the task. In In-\\nternational Conference on Machine Learning , pages\\n45484557. PMLR.\\nAditya Siddhant, Ankur Bapna, Yuan Cao, Orhan Fi-\\nrat, Mia Chen, Sneha Kudugunta, Naveen Arivazha-\\ngan, and Yonghui Wu. 2020. Leveraging mono-\\nlingual data with self-supervision for multilingual\\nneural machine translation. In Proceedings of the\\n58th Annual Meeting of the Association for Compu-\\ntational Linguistics , pages 28272835, Online. As-\\nsociation for Computational Linguistics.\\nGerardo Sierra Martnez, Cynthia Montao, Gemma\\nBel-Enguix, Diego Crdova, and Margarita\\nMota Montoya. 2020. CPLM, a parallel corpus for\\nMexican languages: Development and interface.\\nInProceedings of the 12th Language Resources\\nand Evaluation Conference , pages 29472952,\\nMarseille, France. European Language Resources\\nAssociation.\\nGary F Simons and M Paul Lewis. 2013. The worlds\\nlanguages in crisis. Responses to language endan-\\ngerment: In honor of Mickey Noonan. New direc-\\ntions in language documentation and language revi-\\ntalization , 3:20.\\nPeter Smit, Sami Virpioja, Stig-Arne Grnroos, Mikko\\nKurimo, et al. 2014. Morfessor 2.0: Toolkit for sta-\\ntistical morphological segmentation. In The 14th\\nConference of the European Chapter of the Associa-\\ntion for Computational Linguistics (EACL), Gothen-\\nburg, Sweden, April 26-30, 2014 . Aalto University.\\nAnders Sgaard, Sebastian Ruder, and Ivan Vuli c.\\n2018. On the limitations of unsupervised bilingual\\ndictionary induction. In Proceedings of the 56th An-\\nnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers) , pages 778\\n788.Haiyue Song, Raj Dabre, Zhuoyuan Mao, Fei Cheng,\\nSadao Kurohashi, and Eiichiro Sumita. 2020. Pre-\\ntraining via leveraging assisting languages and data\\nselection for neural machine translation. arXiv\\npreprint arXiv:2001.08353 .\\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\\nYan Liu. 2019. Mass: Masked sequence to se-\\nquence pre-training for language generation. In In-\\nternational Conference on Machine Learning , pages\\n59265936.\\nXabier Soto, Dimitar Shterionov, Alberto Poncelas,\\nand Andy Way. 2020. Selecting backtranslated data\\nfrom multiple sources for improved neural machine\\ntranslation. In Proceedings of the 58th Annual\\nMeeting of the Association for Computational Lin-\\nguistics , pages 38983908, Online. Association for\\nComputational Linguistics.\\nTejas Srinivasan, Ramon Sanabria, and Florian Metze.\\n2019. Multitask learning for different subword seg-\\nmentations in neural machine translation. arXiv\\npreprint arXiv:1910.12368 .\\nDario Stojanovski, Viktor Hangya, Matthias Huck, and\\nAlexander Fraser. 2019. The lmu munich unsuper-\\nvised machine translation system for wmt19. In\\nProceedings of the Fourth Conference on Machine\\nTranslation (Volume 2: Shared Task Papers, Day 1) ,\\npages 393399.\\nHaipeng Sun, Rui Wang, Kehai Chen, Masao Utiyama,\\nEiichiro Sumita, and Tiejun Zhao. 2020. Robust un-\\nsupervised neural machine translation with adversar-\\nial training. arXiv preprint arXiv:2002.12549 .\\nXu Tan, Yichong Leng, Jiale Chen, Yi Ren, Tao\\nQin, and Tie-Yan Liu. 2019. A study of multi-\\nlingual neural machine translation. arXiv preprint\\narXiv:1912.11625 .\\nSarah G Thomason. 2015. Endangered languages .\\nCambridge University Press.\\nJrg Tiedemann. 2016. Opusparallel corpora for ev-\\neryone. Baltic Journal of Modern Computing , page\\n384.\\nJrg Tiedemann. 2018. Emerging language spaces\\nlearned from massively multilingual corpora. arXiv\\npreprint arXiv:1802.00273 .\\nAtnafu Lambebo Tonja, Christian Maldonado-\\nSifuentes, David Alejandro Mendoza Castillo, Olga\\nKolesnikova, No Castro-Snchez, Grigori Sidorov,\\nand Alexander Gelbukh. 2023. Parallel corpus\\nfor indigenous language translation: Spanish-\\nmazatec and spanish-mixtec. arXiv preprint\\narXiv:2305.17404 .\\nAntonio Toral, Sheila Castilho, Ke Hu, and Andy\\nWay. 2018. Attaining the unattainable? reassess-\\ning claims of human parity in neural machine trans-\\nlation. In Proceedings of the Third Conference on\\nMachine Translation: Research Papers , pages 113\\n123.Hai-Long Trieu, Duc-Vu Tran, Ashwin Ittoo, and Le-\\nMinh Nguyen. 2019. Leveraging additional re-\\nsources for improving statistical machine translation\\non asian low-resource languages. ACM Trans. Asian\\nLow-Resour. Lang. Inf. Process. , 18(3).\\nMasao Utiyama and Hitoshi Isahara. 2007. A com-\\nparison of pivot methods for phrase-based statistical\\nmachine translation. In Human Language Technolo-\\ngies 2007: The Conference of the North American\\nChapter of the Association for Computational Lin-\\nguistics; Proceedings of the Main Conference , pages\\n484491.\\nClara Vania and Adam Lopez. 2017. From characters\\nto words to in between: Do we capture morphol-\\nogy? In Proceedings of the 55th Annual Meeting of\\nthe Association for Computational Linguistics (Vol-\\nume 1: Long Papers) , pages 20162027, Vancouver,\\nCanada. Association for Computational Linguistics.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, ukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Advances in neural information pro-\\ncessing systems , pages 59986008.\\nRal Vzquez, Yves Scherrer, Sami Virpioja, and Jrg\\nTiedemann. 2021. The Helsinki submission to the\\nAmericasNLP shared task. In Proceedings of the\\nFirst Workshop on Natural Language Processing for\\nIndigenous Languages of the Americas , pages 255\\n264, Online. Association for Computational Lin-\\nguistics.\\nIvan Vuli c, Goran Glava, Roi Reichart, and Anna Ko-\\nrhonen. 2019. Do we really need fully unsuper-\\nvised cross-lingual embeddings? In Proceedings of\\nthe 2019 Conference on Empirical Methods in Nat-\\nural Language Processing and the 9th International\\nJoint Conference on Natural Language Processing\\n(EMNLP-IJCNLP) , pages 44074418, Hong Kong,\\nChina. Association for Computational Linguistics.\\nIvan Vuli c, Sebastian Ruder, and Anders Sgaard.\\n2020. Are all good word vector spaces isomorphic?\\narXiv preprint arXiv:2004.04070 .\\nLiang Wang, Wei Zhao, Ruoyu Jia, Sujian Li, and\\nJingming Liu. 2019a. Denoising based sequence-\\nto-sequence pre-training for text generation. In Pro-\\nceedings of the 2019 Conference on Empirical Meth-\\nods in Natural Language Processing and the 9th In-\\nternational Joint Conference on Natural Language\\nProcessing (EMNLP-IJCNLP) , pages 39944006.\\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu,\\nChangliang Li, Derek F. Wong, and Lidia S. Chao.\\n2019b. Learning deep transformer models for ma-\\nchine translation. In Proceedings of the 57th Annual\\nMeeting of the Association for Computational Lin-\\nguistics , pages 18101822, Florence, Italy. Associa-\\ntion for Computational Linguistics.Rui Wang, Xu Tan, Renqian Luo, Tao Qin, and Tie-\\nYan Liu. 2021. A survey on low-resource neural\\nmachine translation. In Proceedings of the Thirtieth\\nInternational Joint Conference on Artificial Intel-\\nligence, IJCAI-21 , pages 46364643. International\\nJoint Conferences on Artificial Intelligence Organi-\\nzation. Survey Track.\\nXinyi Wang, Hieu Pham, Philip Arthur, and Gra-\\nham Neubig. 2019c. Multilingual neural machine\\ntranslation with soft decoupled encoding. In Inter-\\nnational Conference on Learning Representations\\n(ICLR) , New Orleans, LA, USA.\\nXinyi Wang, Yulia Tsvetkov, and Graham Neubig.\\n2020. Balancing training for multilingual neural\\nmachine translation. In Proceedings of the 58th An-\\nnual Meeting of the Association for Computational\\nLinguistics , pages 85268537, Online. Association\\nfor Computational Linguistics.\\nKarl Weiss, Taghi M Khoshgoftaar, and DingDing\\nWang. 2016. A survey of transfer learning. Jour-\\nnal of Big Data , 3(1):9.\\nJohn Wieting, Taylor Berg-Kirkpatrick, Kevin Gimpel,\\nand Graham Neubig. 2019. Beyond bleu: Train-\\ning neural machine translation with semantic sim-\\nilarity. In Proceedings of the 57th Annual Meet-\\ning of the Association for Computational Linguis-\\ntics, pages 43444355.\\nHua Wu and Haifeng Wang. 2007. Pivot language ap-\\nproach for phrase-based statistical machine transla-\\ntion. Machine Translation , 21(3):165181.\\nXing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han,\\nand Songlin Hu. 2019. Conditional bert contextual\\naugmentation. In International Conference on Com-\\nputational Science , pages 8495. Springer.\\nHaoran Xu, Benjamin Van Durme, and Kenton Murray.\\n2021. BERT, mBERT, or BiBERT? a study on con-\\ntextualized embeddings for neural machine transla-\\ntion. In Proceedings of the 2021 Conference on Em-\\npirical Methods in Natural Language Processing ,\\npages 66636675, Online and Punta Cana, Domini-\\ncan Republic. Association for Computational Lin-\\nguistics.\\nLinting Xue, Noah Constant, Adam Roberts, Mi-\\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\\nBarua, and Colin Raffel. 2021. mt5: A massively\\nmultilingual pre-trained text-to-text transformer. In\\nProceedings of the 2021 Conference of the North\\nAmerican Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies ,\\npages 483498.\\nDelfino Zacaras Mrquez and Ivan Vladimir\\nMeza Ruiz. 2021. Ayuuk-Spanish neural ma-\\nchine translator. In Proceedings of the First\\nWorkshop on Natural Language Processing for\\nIndigenous Languages of the Americas , pages\\n168172, Online. Association for Computational\\nLinguistics.Lenka Zajcov. 2017. Lenguas indgenas en\\nla legislacin de los pases hispanoamericanos.\\nOnomzein , (NE III):171203.\\nPoorya Zaremoodi, Wray Buntine, and Gholamreza\\nHaffari. 2018. Adaptive knowledge sharing in\\nmulti-task learning: Improving low-resource neural\\nmachine translation. In Proceedings of the 56th An-\\nnual Meeting of the Association for Computational\\nLinguistics (Volume 2: Short Papers) , pages 656\\n661.\\nRodolfo Zevallos, John Ortega, William Chen, Richard\\nCastro, Nria Bel, Cesar Toshio, Renzo Venturas,\\nAradiel, and Hilario Nelsi Melgarejo. 2022. Intro-\\nducing QuBERT: A large monolingual corpus and\\nBERT model for Southern Quechua. In Proceed-\\nings of the Third Workshop on Deep Learning for\\nLow-Resource Natural Language Processing , pages\\n113, Hybrid. Association for Computational Lin-\\nguistics.\\nBiao Zhang, Philip Williams, Ivan Titov, and Rico\\nSennrich. 2020a. Improving massively multilingual\\nneural machine translation and zero-shot translation.\\narXiv preprint arXiv:2004.11867 .\\nBiao Zhang, Philip Williams, Ivan Titov, and Rico\\nSennrich. 2020b. Improving massively multilingual\\nneural machine translation and zero-shot translation.\\nInProceedings of the 58th Annual Meeting of the\\nAssociation for Computational Linguistics , pages\\n16281639, Online. Association for Computational\\nLinguistics.\\nShiyue Zhang, Ben Frey, and Mohit Bansal. 2022.\\nHow can nlp help revitalize endangered languages?\\na case study and roadmap for the cherokee language.\\narXiv preprint arXiv:2204.11909 .\\nShiyue Zhang, Benjamin Frey, and Mohit Bansal.\\n2020c. ChrEn: Cherokee-English machine transla-\\ntion for endangered language revitalization. In Pro-\\nceedings of the 2020 Conference on Empirical Meth-\\nods in Natural Language Processing (EMNLP) ,\\npages 577595, Online. Association for Computa-\\ntional Linguistics.\\nShiyue Zhang, Benjamin Frey, and Mohit Bansal.\\n2021. ChrEnTranslate: Cherokee-English machine\\ntranslation demo with quality estimation and correc-\\ntive feedback. In Proceedings of the 59th Annual\\nMeeting of the Association for Computational Lin-\\nguistics and the 11th International Joint Conference\\non Natural Language Processing: System Demon-\\nstrations , pages 272279, Online. Association for\\nComputational Linguistics.\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\\nWeinberger, and Yoav Artzi. 2020d. Bertscore:\\nEvaluating text generation with bert. In ICLR .\\nYuhao Zhang, Ziyang Wang, Runzhe Cao, Binghao\\nWei, Weiqiao Shan, Shuhan Zhou, Abudurexiti Re-\\nheman, Tao Zhou, Xin Zeng, Laohu Wang, YongyuMu, Jingnan Zhang, Xiaoqian Liu, Xuanjun Zhou,\\nYinqiao Li, Bei Li, Tong Xiao, and Jingbo Zhu.\\n2020e. The NiuTrans machine translation systems\\nfor WMT20. In Proceedings of the Fifth Confer-\\nence on Machine Translation , pages 338345, On-\\nline. Association for Computational Linguistics.\\nZhuosheng Zhang, Yafang Huang, and Hai Zhao. 2019.\\nOpen vocabulary learning for neural chinese pinyin\\nime. In Proceedings of the 57th Annual Meeting\\nof the Association for Computational Linguistics ,\\npages 15841594.\\nFrancis Zheng, Machel Reid, Edison Marrese-Taylor,\\nand Yutaka Matsuo. 2021. Low-resource machine\\ntranslation using cross-lingual language model pre-\\ntraining. In Proceedings of the First Workshop on\\nNatural Language Processing for Indigenous Lan-\\nguages of the Americas , pages 234240, Online. As-\\nsociation for Computational Linguistics.\\nHao Zheng, Yong Cheng, and Yang Liu. 2017.\\nMaximum expected likelihood estimation for zero-\\nresource neural machine translation. In IJCAI ,\\npages 42514257.\\nShuyan Zhou, Xiangkai Zeng, Yingqi Zhou, Antonios\\nAnastasopoulos, and Graham Neubig. 2019. Im-\\nproving robustness of neural machine translation\\nwith multi-task learning. In Proceedings of the\\nFourth Conference on Machine Translation (Volume\\n2: Shared Task Papers, Day 1) , pages 565571, Flo-\\nrence, Italy. Association for Computational Linguis-\\ntics.\\nChangfeng Zhu, Heng Yu, Shanbo Cheng, and Weihua\\nLuo. 2020a. Language-aware interlingua for multi-\\nlingual neural machine translation. In Proceedings\\nof the 58th Annual Meeting of the Association for\\nComputational Linguistics , pages 16501655, On-\\nline. Association for Computational Linguistics.\\nJinhua Zhu, Fei Gao, Lijun Wu, Yingce Xia, Tao\\nQin, Wengang Zhou, Xueqi Cheng, and Tie-Yan\\nLiu. 2019. Soft contextual data augmentation\\nfor neural machine translation. arXiv preprint\\narXiv:1905.10523 .\\nJinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,\\nWengang Zhou, Houqiang Li, and Tie-Yan Liu.\\n2020b. Incorporating bert into neural machine trans-\\nlation. arXiv preprint arXiv:2002.06823 .\\nBarret Zoph, Deniz Yuret, Jonathan May, and\\nKevin Knight. 2016. Transfer learning for low-\\nresource neural machine translation. arXiv preprint\\narXiv:1604.02201 .A Appendix\\nIn this appendix we expand the information re-\\ngarding current work on MT for LRL.\\nA.1 Expanded LR work on Multilingual\\nsupervised training\\nArivazhagan et al. (2019a) introduce a represen-\\ntational invariance training objective across lan-\\nguages that achieves comparable results with piv-\\noting methods. Promising results of multilingual\\nmodels have encouraged experiments with models\\ntrained on a massive amount of language pairs, re-\\nsulting in large multilingual models: Aharoni et al.\\n(2019) train a single model on 102 languages to\\nand from English in contrast to the 58 languages\\nused by Neubig and Hu (2018).\\nThe negative aspect of this approach is the size\\nof the network. Arivazhagan et al. (2019b) per-\\nform an extensive study on 102 language pairs\\nto explore different settings and training setups\\nand achieve good results for LRLs, while main-\\ntaining good performance for high-resource lan-\\nguages. Related massively multilingual NMT\\nsystems have been trained for analytic proposes\\n(Tiedemann, 2018; Malaviya et al., 2017) and\\ngeneral zero-shot transfer learning (Artetxe and\\nSchwenk, 2019). mRASP (Lin et al., 2020) use\\nfor pretraining of the multilingual model and add\\na randomly aligned substitution loss that aims to\\nbring words and phrases closer in the cross-lingual\\nspace.\\nZhang et al. (2020a) explores the main problems\\nthat arise for such models: multilingual NMT usu-\\nally underperforms bilingual models (Arivazha-\\ngan et al., 2019b), the larger the number of lan-\\nguages gets the more the performance drops (Aha-\\nroni et al., 2019), languages in datasets used for\\nmultilingual training are unbalanced in size, and\\npoor zero-shot performance compared to pivot\\nmodels (cf. 6.3). Zhang et al. (2020a) ad-\\ndresses these problems with a language-aware in-\\nput layer, a deep transformer architecture (Wang\\net al., 2019b), and an online back-translation\\napproach. These modifications boost zero-shot\\ntranslation performance for multilingual models.\\nTo improve the problem of imbalanced and lin-\\nguistically diverse training data, mostly heuristic\\nmethods have been proposed: Arivazhagan et al.\\n(2019b) samples training data from different lan-\\nguages based on a data size scaled by temperature\\nterm. These heuristics have an impact on perfor-mance, and ignore other factors that are not size.\\nOversampling of data is used by Johnson et al.\\n(2017); Neubig and Hu (2018); Conneau and Lam-\\nple (2019). Wang et al. (2020) proposes a differ-\\nentiable data selection method that automatically\\nlearns to weight training data, optimizing transla-\\ntion on all languages.\\nMultilingual modeling Sharing all parameters\\nexcept for the attention mechanism shows im-\\nprovements compared with sharing everything in\\nan RNN NMT model (Blackwood et al., 2018).\\nSachan and Neubig (2018) explores parameter\\nsharing in the transformer architecture for the de-\\ncoder in the one-to-many translation setting and\\nshows that transformers are more suitable than\\nRNNs for this task. Also, parameter sharing in\\nthe decoder and embedding layer further improves\\nperformance. Lu et al. (2018) proposes a shared\\nlayer intended to capture the interlingua knowl-\\nedge and an extension to the typical RNN network\\nwith multiple blocks along with a trainable routing\\nnetwork. The routing network enables adaptive\\ncollaboration by dynamic sharing of blocks condi-\\ntioned on the task at hand, input, and model state\\n(Zaremoodi et al., 2018). Zhang et al. (2020a) pro-\\nposes a language-aware layer to improve such ar-\\nchitectures further. With a similar idea, Zhu et al.\\n(2020a) incorporates two special language embed-\\ndings into the self-attention mechanism. The first\\nencodes the unique characteristics of each lan-\\nguage, while the second captures common seman-\\ntics across languages.\\nOne problem in multilingual NMT systems is\\nthe translation into the wrong language. To ad-\\ndress this problem, Zhang et al. (2020b) add\\na language-aware layer normalization and a lin-\\near transformation that is inserted between the\\nencoder and the decoder to induce a language-\\nspecific translation. Raganato et al. (2021) explore\\nto weight the target language label with jointly\\ntraining one cross attention head with word align-\\nments.\\nOther modifications of NMT model archi-\\ntectures to improve their performance on low-\\nresource languages include: deep RNNs (Miceli-\\nBarone et al., 2017), normalization layers (Ba\\net al., 2016), direct lexical connections (Nguyen\\net al., 2015), word embedding layers conducive to\\nlexical sharing (Wang et al., 2019c).A.2 Extended Multi-task training\\nZhou et al. (2019) uses this approach, but extends\\nit with a cascade architecture: the first decoder\\nreads the encoder, and the second decoder reads\\nthe encoder and the first decoder (Niehues et al.,\\n2016; Anastasopoulos and Chiang, 2018). The\\nauxiliary task (first decoder) is a denoising de-\\ncoder. With RNN NMT architectures, one can\\nfurther decide if the attention mechanism should\\nbe shared among tasks (Niehues and Cho, 2017).\\nThe authors compare all architectures and find that\\nthey perform similarly, with only sharing the en-\\ncoder being slightly better.\\nUsing linguistic information as an auxiliary task\\nhas not yet been explored exhaustively. Niehues\\nand Cho (2017) studies the usage of part-of-speech\\n(POS) and named entity (NE) tags, finding that\\ntraining on named entity recognition (NER), POS\\ntagging and MT together improves performance\\nthe most. For agglutinative languages, morpho-\\nlogical auxiliary tasks can be beneficial: Pan et al.\\n(2020) uses stemming with fully shared parame-\\nters.\\nAs an alternative to linguistically informed aux-\\niliary tasks Srinivasan et al. (2019) uses multiple\\nBPE vocabulary sizes to generate different seg-\\nmentations. Each segmentation is treated as an in-\\ndividual task.\\nA.3 Data augmentation\\nBack-translation Caswell et al. (2019) shows\\nthat adding a special tag to the synthetic data im-\\nproves performance. A technique that exploits this\\nidea is training an initial translation model with\\nsynthetic data generated via BT and then finetune\\nit with gold data (Abdulmumin et al., 2019). This\\nsimple yet effective training algorithm improves\\nNMT for LRLs; however, it can also degrade per-\\nformance on HRLs if trained without a tagging\\nstrategy (Marie et al., 2020).\\nMultiple improvements of BT have been pro-\\nposed. Edunov et al. (2018) shows that sampling\\nor noisy beam search can generate more effective\\npseudo-parallel data. However, for LRLs an op-\\ntimal beam search and greedy decoding are bet-\\nter. A factor that influences BTs effectiveness\\nis the quality of the initial MT systems (Hoang\\net al., 2018). Using back-translated data from mul-\\ntiple sources (Poncelas et al., 2019) or optimizing\\nthe ranking of back-translated data yields further\\ngains (Soto et al., 2020).BT results in gains when the parallel corpora are\\nnaturally occurring text and not translationese, as\\nthe latter would only improve automatic n metrics\\n(Toral et al., 2018; Graham et al., 2020). ?shows\\nthat BT produces more fluent text and is preferred\\nby humans. Additionally, translationese and origi-\\nnal data can be modeled as separate languages in a\\nmultilingual model (Riley et al., 2020). BT is also\\na central part of unsupervised MT (UMT; cf. 6.4)\\nand zero-shot MT (Gu et al., 2019).\\nSentence modification Zhu et al. (2019) pro-\\nposes to replace a randomly chosen word in a sen-\\ntence with a soft-word . That means that, instead\\nof sampling a word from the lexical distribution\\nof a LM like Kobayashi (2018), the authors use\\nthe hidden state vector of the LM directly. Wu\\net al. (2019) substitutes the RNN LMs from pre-\\nvious work and use BERT (Devlin et al., 2019)\\n a transformer trained with a masked language\\nmodeling objective  instead. The authors finetune\\nBERT with a conditional masked language mod-\\neling objective that tries to avoid the prediction of\\nwords that do not correspond to the original sen-\\ntence meaning.\\nAnother way to augmented MT data is by para-\\nphrasing. If a good paraphrase system exists, this\\ncan increase the number of training instances (Hu\\net al., 2019). Paraphrasing can also be used at\\ntraining time by sampling paraphrases of the refer-\\nence sentence from a paraphraser and training the\\nMT model to predict the distribution of the para-\\nphraser (Khayrallah et al., 2020). This helps the\\nmodel to generalize. Wieting et al. (2019) propose\\na similar approach, using minimum risk training to\\noptimize BLEU. To avoid BLEUs constraints to a\\nspecific reference, they use paraphrasing to diver-\\nsify the given reference.\\nFinally, existing data can be augmented by\\nadding noise. This noise can be continuous or dis-\\ncrete. In the case of applying continuous noise,\\nnoise vectors are added to the word embeddings\\n(Cheng et al., 2018; Sano et al., 2019). Discrete\\nnoise is realized by inserting, deleting, or replac-\\ning words, BPE tokens, or characters to expand\\nthe training set in an adversarial fashion (Belinkov\\nand Bisk, 2018; Ebrahimi et al., 2018; ?; Cheng\\net al., 2019, 2020).\\nPivoting While it is simple to implement and\\neffective, pivot-based approaches suffer from er-\\nror propagation. To overcome that for NMT, jointtraining Zheng et al. (2017); Cheng (2019) and\\nround-trip training (Ahmadnia and Dorr, 2019)\\nhave been proposed.\\nPivoting with NMT systems has been used\\nfor translating Japanese, Indonesian, and Malay\\ninto Vietnamese (Trieu et al., 2019), translation\\nof related languages (Pourdamghani and Knight,\\n2019), multilingual zero-shot MT (Lakew et al.,\\n2018), and UMT (cf. 6.4) between distant lan-\\nguage pairs (Leng et al., 2019).\\nA.4 Recent low-resource Shared Tasks\\nFirst, the LoResMT 2020 shared task (Ojha\\net al., 2020) explores the case of language pairs\\nwhich have no parallel data between them (Hindi\\nBhojpuri, HindiMagahi, and RussianHindi).\\nThe winning system (Laskar et al., 2020) uses\\na MASS model in a zero-shot fashion with ad-\\nditional monolingual data (see 6.4). Second,\\nthe WMT 2020 shared tasks on UMT and very\\nlow-resource supervised MT (Fraser, 2020) pro-\\nvide text and 60k aligned phrases for German\\nUpper Sorbian., The most important technique in\\nall tracks is transfer learning, achieving surpris-\\ningly good results. For the AmericasNLP 2021\\nshared task on open MT (Mager et al., 2021), 10\\nindigenous language languages were paired with\\nSpanish, resulting in an extreme low-resource set-\\nting (4k to 125k paired sentences), with challenges\\nout as domain, dialectical, and orthographic mis-\\nmatches between splits and datasets. The best\\nsystems shows that data cleaning and collection\\n(??) as well as multilingual approaches (6.1)\\nresult in the best performance in this conditions.\\nFinally the shared task on MT in Dravidian lan-\\nguages (Chakravarthi et al., 2021) features 3 lan-\\nguages paired with English as well as Tamil\\nTelugu. Again, the winning system uses a mul-\\ntilingual approach. The best performing systems\\nuse BT (6.3) and BPE word segmentation (2.1).\\nThe results from these challenges indicate that\\nthe optimal selection and combination of meth-\\nods differs between cases (i.e., amount of mono-\\nlingual, parallel data, cleanness of data, domain\\nmismatch, linguistic closeness of languages). This\\nimplies that data analysis and linguistic knowl-\\nedge are needed to improve a final systems per-\\nformance.\\nA.5 Transfer learning\\nThis helps low-resource tasks as a lower amount\\nof data can be used for training. One applicationof transfer learning to MT is the usage of a pre-\\ntrained RNN LM (Gulcehre et al., 2015) as the de-\\ncoder in an NMT system. Zoph et al. (2016) is the\\nfirst work that uses pretrained models to improve\\nNMT systems. The authors perform two experi-\\nments with an RNN encoderdecoder architecture\\nwith an attention mechanism: the model is first\\npretrained on a high-resource language pair This\\nworks even better if related languages are used\\nduring pretraining (Nguyen and Chiang, 2017).\\nUsing pretrained LMs at decoding time and as pri-\\nors at training time also improves vanilla models\\n(Baziotis et al., 2020).\\nTo avoid overfitting, models can be finetuned on\\nboth a HRLs pair and a LRLs pair in a multi-task\\nfashion (Neubig and Hu, 2018).\\nHowever, how can we represent best the vocab-\\nulary? Zoph et al. (2016) use separate embeddings\\nfor the source and the target language. However,\\nusing tied embeddings has been shown to yield\\nbetter results (Press and Wolf, 2017). Edunov et al.\\n(2019) employs ELMO (Peters et al., 2018) repre-\\nsentations as pretrained features in the encoder of\\na transformer model. Song et al. (2020) shows that\\nit is possible to improve performance by combin-\\ning monolingual texts from linguistically related\\nlanguages, performing a script mapping. It is also\\npossible to extract features from a BERT model\\nin the source language and combining these with\\nan NMT system (Zhu et al., 2020b), but using a\\nBERT model pretrained with a mixed sentences\\nfrom source and target languages lead to even bet-\\nter results (Xu et al., 2021).\\nEncoder-decoder pretrained models have\\ngained popularity in the last years for low-\\nresource MT. Conneau and Lample (2019)\\nproposes training the encoder and the decoder\\nseparately in order to get cross-language rep-\\nresentations (XLM). This idea has further been\\nextended by Song et al. (2019, MASS) to\\nmasking a sequence of tokens from the input.\\nTraining MASS in a multilingual fashion and\\nusing monolingual data for pretraining helps to\\nimprove NMT for low-resource languages and\\nzero-shot translation (Siddhant et al., 2020).\\nAnother approach is to train the entire transformer\\nmodel as a denoising autoencoder (BART; Lewis\\net al., 2019). The multilingual version of BART\\n(mBART) is more suitable for NMT tasks and\\nyields important gains (Liu et al., 2020). It is also\\npossible to pretrain a transformer in a multi-task,text-to-text fashion, where one of the tasks is\\nMT (T5; Raffel et al., 2020). All four models\\ncan be finetuned for MT or used in an unsuper-\\nvised fashion. Improvements to BART can be\\nobtained by augmenting the maximum likelihood\\nobjective with an additional objective, which is\\na data-dependent Gaussian prior distribution (Li\\net al., 2020). Huge LMs can improve zero-shot\\nand few-shot learning even further (Brown et al.,\\n2020), but at a high computational cost. Pursuing\\nanother direction, Wang et al. (2019a) develops a\\nhybrid architecture between a transformer and a\\npointer-generator network. At training time, the\\nauthors jointly train the encoder and the decoder\\nin a denoising auto-encoding fashion.\\nOne crucial problem for transfer-learning is\\nminimizing catastrophic forgetting (Serra et al.,\\n2018). Chen et al. (2021) show that it is possible\\nto combine a pre-trained multilingual model, with\\nfine-tuining it with one single language pair, to im-\\nprove zero-shot machine translation. Another way\\nto handle this problem is reducing the number of\\nparameter to be updated. Gheini et al. (2021) pro-\\npose to only update the cross attention parameters.\\nA.6 Unsupervised MT\\nThe addition of other components such as masked\\nLMs and denoising auto-encoding has also been\\ntried (Stojanovski et al., 2019). Unsupervised\\nmethods are vulnerable to adversarial attacks of\\nword substitution and order change in the input.\\nAdversarial training can improve performance in\\nsuch situations (Sun et al., 2020). Since the ini-\\ntialization step is crucial for UMT, Ren et al.\\n(2020) aligns semantically similar sentences from\\ntwo monolingual corpora with the help of cross-\\nlingual embeddings. With these, an SMT system\\nis trained to warm up an NMT system. How-\\never, UMT still has to overcome a set of chal-\\nlenges. Sgaard et al. (2018) shows that perfor-\\nmance decays dramatically for languages with dif-\\nferent typological features, since, in such situa-\\ntions, bilingual word embeddings (Conneau et al.,\\n2017) are far from isomorphic. Vuli c et al. (2020)\\nfinds that isomorphism is also less likely if small\\namounts of monolingual data are used for training\\nbilingual word embeddings. Nooralahzadeh et al.\\n(2020) discovers that performance quickly deteri-\\norates for a mismatch of source and target domain\\nand that the initialization of word embeddings can\\naffect MT performance. All of this makes UMTfor LRLs or endangered languages challenging.\\nSome of the described issues have been ad-\\ndressed: Liu et al. (2019) proposes to combine\\nword-level and subword-level embeddings to ac-\\ncount for morphological complexity. For the prob-\\nlem of distant language pairs, Leng et al. (2019)\\nproposes pivoting (cf. 6.3). Isomorphism of\\nbilingual word-embeddings can be improved with\\nsemi-supervised methods (Vuli c et al., 2019).\\nGarcia et al. (2020) introduces multilingual\\nUMT systems. The main idea consists of general-\\nizing UMT by using a multi-way back-translation\\nobjective. Recently, pretrained multilingual trans-\\nformer networks are used to improve UMT even\\nfurther (cf. 6.4).\\nB Ethical Considerations\\nEthical concerns when working on MT for endan-\\ngered languages include a lack of community in-\\nvolvement during language documentation, data\\ncreation, and development and setup of MT sys-\\ntems. For more information, we refer interested\\nreaders to Bird (2020). Finally, we want to men-\\ntion that publicly employing low-quality MT sys-\\ntems for LRLs bears a risk of translating incor-\\nrectly or in biased (e.g., sexist or racist) ways.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying LSA for summarizing the section extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing the extracted sentences\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(section_extraction)\n",
    "\n",
    "# 3. Create a TruncatedSVD object for LSA \n",
    "lsa = TruncatedSVD(n_components = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.70072576  0.54734714  0.0060138  -0.19535958 -0.35470592  0.03044179\n",
      "  -0.16467642]\n",
      " [ 0.83094813  0.20731302 -0.01527954 -0.11648635  0.060489    0.01919389\n",
      "   0.20940075]\n",
      " [ 0.78959341 -0.21578694 -0.07652416 -0.05654078  0.15016734  0.50406263\n",
      "  -0.2103922 ]\n",
      " [ 0.82983254 -0.11145451 -0.11039983 -0.18104609  0.03488119  0.01990331\n",
      "   0.4042745 ]\n",
      " [ 0.71021895 -0.46393653 -0.31428962 -0.09523508 -0.26755229 -0.26675063\n",
      "  -0.14988971]\n",
      " [ 0.77979426  0.08682592  0.1763532  -0.12868583  0.44622058 -0.30494595\n",
      "  -0.19454685]\n",
      " [ 0.67136877 -0.22502615  0.63553244  0.22458846 -0.20738742  0.00132172\n",
      "   0.03534088]\n",
      " [ 0.6782175   0.17683355 -0.26608363  0.65727339  0.04727918 -0.03751354\n",
      "   0.00954011]]\n"
     ]
    }
   ],
   "source": [
    "# 4. Perform LSA on the TF-IDF matrix\n",
    "lsa_matrix = lsa.fit_transform(tfidf_matrix)\n",
    "print(lsa_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-ranked sentences for Section Abstract:\n",
      "Abstract\n",
      "Neural models have drastically advanced state\n",
      "of the art for machine translation (MT) be-\n",
      "tween high-resource languages. Traditionally,\n",
      "these models rely on large amounts of train-\n",
      "ing data, but many language pairs lack these\n",
      "resources. However, an important part of the\n",
      "languages in the world do not have this amount\n",
      "of data. Most languages from the Americas are\n",
      "among them, having a limited amount of par-\n",
      "allel and monolingual data, if any. Here, we\n",
      "present an introduction to the interested reader\n",
      "to the basic challenges, concepts, and tech-\n",
      "niques that involve the creation of MT systems\n",
      "for these languages. Finally, we discuss the\n",
      "recent advances and findings and open ques-\n",
      "tions, product of an increased interest of the\n",
      "NLP community in these languages.\n",
      "\n",
      "\n",
      "Top-ranked sentences for Section 1 Introduction:\n",
      "1 Introduction\n",
      "More than 7 billion people on Earth communicate\n",
      "in nearly 7000 different languages (Pereltsvaig,\n",
      "2020). Of these, approximately 900 languages\n",
      "are native of the American continent (Campbell,\n",
      "2000). Most of these indigenous languages of the\n",
      "Americas (ILA) are endangered at some degree\n",
      "(Thomason, 2015). This huge variety in languages\n",
      "is simultaneously a rich treasure for humanity and\n",
      "also a barrier to communication among people\n",
      "from different backgrounds. Human translators\n",
      "have been important in overcoming language bar-\n",
      "riers. However, trained translators are not acces-\n",
      "sible to everyone on Earth and even scarcer for\n",
      "endangered and minority languages. The need\n",
      "for translations is even written in the constitutions\n",
      "of several countries like Mexico, Peru, Paraguay,\n",
      "Venezuela, and Bolivia (Zajcov, 2017) to allow\n",
      "native speakers to have equal language rights re-\n",
      "garding law.\n",
      "This is why developing MT is crucial: it helps\n",
      "humanity overcome language barriers while si-\n",
      "multaneously allowing people to continue using\n",
      "Work done while at the University of Stuttgart.their native tongue. However, the challenges to\n",
      "achieving these problems are not trivial. It is not\n",
      "only the amount of available data (a common the-\n",
      "sis among the NLP community) but also a set\n",
      "of challenging issues (dialectical and orthographic\n",
      "variations, noisy texts, complex morphology, etc.)\n",
      "that must be addressed.\n",
      "MT has always been an important task within\n",
      "the larger area of natural language processing\n",
      "(NLP). In 1954, the GeorgetownIBM experiment\n",
      "(Hutchins, 2004) was the first that showed at least\n",
      "some effectiveness of MT. Further research re-\n",
      "sulted in rule-based systems and statistical models.\n",
      "In 2023, neural models define state of the art for\n",
      "MT if training data is plentiful  i.e., for so-called\n",
      "high-resource languages (HRLs)  and have also\n",
      "achieved impressive results for low-resource lan-\n",
      "guages (LRLs). MT is also the most studied NLP\n",
      "task for the ILA (Mager et al., 2018b; Littell et al.,\n",
      "2018). The common issue among these languages\n",
      "is the extreme low-resource conditions they are\n",
      "confronted with. The research interest for these\n",
      "languages has increased in the last years, including\n",
      "the recent AmericasNLP 2021 shared task (Mager\n",
      "et al., 2021) on 10 indigenous languages to Span-\n",
      "ish, and the WMT (Conference on Machine Trans-\n",
      "lation) shared task for InuktitutEnglish (Barrault\n",
      "et al., 2020).\n",
      "In this work we aim to provide a comprehensive\n",
      "introduction to the challenges that involve creat-\n",
      "ing MT systems for ILA, and the current status\n",
      "of the existing work. We organize this work as\n",
      "follows: We start by introducing state-of-the-art\n",
      "NMT models (2). Then, we discuss the current\n",
      "challenges for these languages (3); and we in-\n",
      "troduce the key concepts related to low-resource\n",
      "NMT and the implications for endangered lan-\n",
      "guages of the Americas(3). This is followed by\n",
      "a discussion of available data (4). Afterwards,\n",
      "we introduce the important concepts for LRL and\n",
      "endangered languages (5); then we introduce thearXiv:2306.06804v1  [cs.CL]  11 Jun 2023main strategies aimed at improving NMT with\n",
      "limited training data (6); and finally we give an\n",
      "overview of the work done for ILA on MT (7).\n",
      "In doing so, we provide insights into the follow-\n",
      "ing questions: Which systems define the state of\n",
      "the art on low-resource NMT applied to the ILA?\n",
      "What is the route that ahead to improve the trans-\n",
      "lations of the ILA?\n",
      "\n",
      "\n",
      "Top-ranked sentences for Section 2 Background and Definitions:\n",
      "2 Background and Definitions\n",
      "Formally, the task of MT consists of converting\n",
      "textXin a source language Lxinto text Yin a\n",
      "target language Lythat conveys the same mean-\n",
      "ing.1Translating text XLxintoYLycan be\n",
      "described as a function (Neubig, 2017):\n",
      "Y=MT(X). (1)\n",
      "XandYcan be of variable length, such as\n",
      "phrases, sentences, or even documents.\n",
      "If other languages are used during the transla-\n",
      "tion process, e.g., as pivots, we denote them as\n",
      "L1, . . . , L n. We refer to a corpus of monolingual\n",
      "sentences in language LiasMLi=S1, ..., S n.\n",
      "Probabilistic Modeling and Data When us-\n",
      "ing probabilistic MT models, the goal is to find\n",
      "YLywith the highest conditional probability,\n",
      "given XLx. Under the supervised machine\n",
      "learning paradigm, a parallel corpus Cparallel =\n",
      "(X1, Y1), ...,(Xn, Yn)is used to learn a set of pa-\n",
      "rameters , which define a probability distribution\n",
      "over possible translations. Given Cparallel , the\n",
      "training objective of an NMT model is generally\n",
      "to maximize the log-likelihood Lwith respect to\n",
      ":\n",
      "L=X\n",
      "(Xi,Yi)Cparallellogp(Yi|Xi;).(2)\n",
      "Within this overall framework, there are a num-\n",
      "ber of design decisions one has to make, such as\n",
      "which model architecture to use, how to generate\n",
      "translations, and how to evaluate.\n",
      "Decoding Decoding refers to the generation of\n",
      "output Y, given the parameters and an input X.\n",
      "Often, decoding is done by approximately solving\n",
      "the following maximization problem:\n",
      "argmax Yp(Y|X;) (3)\n",
      "1This is an approximation, since it is in general not possi-\n",
      "ble to map the meaning of text exactly into another language\n",
      "(Nida, 1945; Sechrest et al., 1972; Baker, 2018).Most NMT systems factorize the probability of\n",
      "Y= y1, ...,yTin a left-to-right fashion:\n",
      "p(Y) =TY\n",
      "t=1p(yt|y<t, X,  ) (4)\n",
      "Thus, the probability of token ytat time step tis\n",
      "computed using the previously generated tokens\n",
      "y<t, the source sentence Xand the model param-\n",
      "eters . Common algorithms for finding a high-\n",
      "probability translation are greedy decoding, i.e.,\n",
      "picking the token with the highest probability at\n",
      "each time step, and beam search (Lowerre, 1976).\n",
      "2.1 Input Representations\n",
      "The texts XandYare input into an NMT sys-\n",
      "tem as sequences of continuous vectors. How-\n",
      "ever, defining which units should be represented\n",
      "as such vectors is non-trivial. The classic way\n",
      "is to represent each word within XandYas\n",
      "a vector (or embedding). However, in a low-\n",
      "resource setting, often not all vocabulary items ap-\n",
      "pear in the training data (Jean et al., 2015; Lu-\n",
      "ong et al., 2015). This issue especially effects lan-\n",
      "guages with a rich inflectional morphology (Sen-\n",
      "nrich et al., 2016c): as many word forms can\n",
      "represent the same lemma, the vocabulary cover-\n",
      "age decreases drastically. Furthermore, for many\n",
      "LRLs, boundaries between words or morphemes\n",
      "are not easy to obtain or not well defined in the\n",
      "case of languages without a standard orthography.\n",
      "Alternative input units have been explored, such as\n",
      "characters (Ling et al., 2015), byte pair encoding\n",
      "(BPE; Sennrich et al., 2016a), morphological rep-\n",
      "resentations (Vania and Lopez, 2017; Ataman and\n",
      "Federico, 2018), syllables (Zhang et al., 2019), or,\n",
      "recently, a visual representation of rendered text\n",
      "(Salesky et al., 2021). No clear advantage has been\n",
      "discovered for using morphological segmentations\n",
      "over BPEs when testing them on LRLs (Saleva and\n",
      "Lignos, 2021).\n",
      "Input representations can be pretrained. The\n",
      "two most common options are: i) word em-\n",
      "beddings, where each type is represented by a\n",
      "vector, e.g., Word2Vec (Mikolov et al., 2013),\n",
      "Glove (Pennington et al., 2014), or Fasttext (Bo-\n",
      "janowski et al., 2017)) embeddings, and ii) contex-\n",
      "tualized word representations, where entire sen-\n",
      "tences are being encoded at a time, e.g., ELMo\n",
      "(Peters et al., 2018) or BERT (Devlin et al.,\n",
      "2019). However, training of these methods re-\n",
      "quires large monolingual training corpora, whichmay not be readily available for LRLs. As most\n",
      "ILA have rich morphology, this topic has gath-\n",
      "ered special interest. The discussion about the us-\n",
      "age of morpholigical segmented input for NMT\n",
      "models is recurrent. (Mager et al., 2022) show\n",
      "that the unsupervised morphologically inspired\n",
      "models outperform BPE pre-processing (experi-\n",
      "mented on 4 language pares). Similar experi-\n",
      "ments done on QuechuaSpanish and Inuktitut\n",
      "Enlgish (Schwartz et al., 2020), comparing BPEs\n",
      "against Morfessor (Smit et al., 2014). Also (Or-\n",
      "tega et al., 2020a) improves the SOTA (state-of-\n",
      "the-art) for QuechuaSpanish MT using a mor-\n",
      "phological guided BPE algorithm.\n",
      "2.2 Architectures\n",
      "NMT models typically are sequence-to-sequence\n",
      "models. They encode a variable-length sequence\n",
      "into a vector or matrix representation, which they\n",
      "then decode back into a variable-length sequence\n",
      "(Cho et al., 2014). The two most frequent archi-\n",
      "tectures are: i) recurrent neural networks (RNN),\n",
      "such as LSTMs (Hochreiter and Schmidhuber,\n",
      "1997) or GRUs (Cho et al., 2014), and ii) trans-\n",
      "formers (Vaswani et al., 2017), which define the\n",
      "current state of the art in the high-resource setting.\n",
      "As for most neural network models, training an\n",
      "NMT system on a limited number of instances\n",
      "is challenging (Fernndez-Delgado et al., 2014).\n",
      "There are common problems that arise from lim-\n",
      "ited data in the training set. One major advan-\n",
      "tage of neural models is their ability to learn rep-\n",
      "resentations from raw data, in contrast to manu-\n",
      "ally engineered features (Barron, 1993). However,\n",
      "problems arise when not enough data is provided\n",
      "to enable effective learning of features. Another\n",
      "strength of neural networks is their generalization\n",
      "capacity (Kawaguchi et al., 2017). However, train-\n",
      "ing a neural network on a small dataset easily leads\n",
      "to overfitting (Rolnick et al., 2017). Recent stud-\n",
      "ies, however, show empirically that this does not\n",
      "necessarily happen if the network is tuned cor-\n",
      "rectly (Olson et al., 2018).\n",
      "2.3 Evaluation\n",
      "Accurately judging translation quality is difficult\n",
      "and, thus, often still done manually: bilingual\n",
      "speakers assign scores according to provided crite-\n",
      "ria such as fluency and adequacy ( Does the output\n",
      "have the same meaning as the input? ). However,\n",
      "manual evaluation is expensive and slow. More-over, in the case of endangered languages, bilin-\n",
      "gual speakers can be hard or impossible to find.\n",
      "Automatic metrics provide an alternative.2\n",
      "These metrics assign a score to system output,\n",
      "given one or more ground truth reference transla-\n",
      "tions. The most widely used metric is BLEU (Pa-\n",
      "pineni et al., 2002), which relies on token-level n-\n",
      "gram matches between the translation to be rated\n",
      "and one or more gold-standard translations. For\n",
      "morphologically rich languages, character-level\n",
      "metrics, such as chrF (Popovi c, 2017), are often\n",
      "more suitable, as they allow for more flexibility.\n",
      "In the AmericasNLP ST (Mager et al., 2021) this\n",
      "metric was used over BLEU, as it fits better to the\n",
      "rich morphology of many ILA.\n",
      "To have a concrete example, lets have the fol-\n",
      "lowing Wixarika phrase with an English transla-\n",
      "tion:\n",
      "yu-huta-me ne-p+-we-iwa\n",
      "an-two-ns 1sg:s-asi-2pl:o-brother\n",
      "I have two brothers\n",
      "As discussed in (Mager et al., 2018c) it is dif-\n",
      "ficult to translate back from Spanish (or other Fu-\n",
      "sional language) the morpheme p+as it has not\n",
      "equivalent in these languages. So if we would ig-\n",
      "nore these morpheme at all, BLEU would penal-\n",
      "ize the entire word nep+weiwa . In contrast, chrF\n",
      "would give credit to the translation, even if the p+\n",
      "is missing.\n",
      "One shortcoming of these evaluation metrics is\n",
      "that the evaluation is very dependent on the sur-\n",
      "face forms and not on the ultimate goal of seman-\n",
      "tic similarity and fluency. Recent work uses pre-\n",
      "trained models to evaluate semantic similarity be-\n",
      "tween translations and the gold standard (Zhang\n",
      "et al., 2020d), but these methods are limited to lan-\n",
      "guages for which such models are available. This\n",
      "is not possible for the ILA, as the amount of mono-\n",
      "lingual data is not enough to train a reliable pre-\n",
      "trained language model3.\n",
      "\n",
      "\n",
      "Top-ranked sentences for Section 3 Challenges and open questions:\n",
      "3 Challenges and open questions\n",
      "In an overview of the datasets and recent studies\n",
      "of MT for the ILA, we found the following main\n",
      "issues to be handled.\n",
      "2For a detailed overview of automatic metrics for MT we\n",
      "refer the interested reader to specialized reviews (Han, 2016;\n",
      "Celikyilmaz et al., 2020; Chatzikoumi, 2020).\n",
      "3One exception to this is Quechua, that has a large enough\n",
      "monolingual dataset to train a BERT like model (Zevallos\n",
      "et al., 2022)Extreme low-resource parallel datasets Even\n",
      "with the recent advances, the resources available\n",
      "to train MT systems are extremely scarce, hav-\n",
      "ing training set between 4k and 20k sentences (see\n",
      "4), with notable exceptions for Inuktitut, Guarani\n",
      "and Quechua (Joanis et al., 2020; Ortega et al.,\n",
      "2020a).\n",
      "Lack of monolingual data Most of these lan-\n",
      "guages are mostly used in spoken form. In re-\n",
      "cent years, with the advancement and democra-\n",
      "tization of mobile technologies, indigenous lan-\n",
      "guages have seen a slight increase in massaging\n",
      "systems and private spheres (Rosales et al.). How-\n",
      "ever, the usage of these languages on the internet\n",
      "is rather limited. Even Wikipedia has a limited\n",
      "amount of these languages (Mager et al., 2018b).\n",
      "Low domain diversity . As most parallel\n",
      "datasets are scarce, they are restricted to a small\n",
      "number of domains, making it challenging to\n",
      "adapt it, or try to aim for general translation mod-\n",
      "els. This has been recognized as a major problem\n",
      "during the AmericasNLP ST (Mager et al., 2021).\n",
      "Rich morphology An important number of\n",
      "these languages are morphological highly rich. In\n",
      "many cases, we find polysynthetic, with or highly\n",
      "agglutinative languages (Kann et al., 2018) or even\n",
      "fusional phenomenon (Mager et al., 2020).\n",
      "Distant paired language The most common\n",
      "languages that we find that ILA is translated into\n",
      "are Spanish, English, and Portuguese. However,\n",
      "these languages are distantly related to the ILA,\n",
      "and have completely different linguistically phe-\n",
      "nomenons (Campbell, 2000; Romero et al., 2016).\n",
      "Noisy text environments Monolingual texts, if\n",
      "exist, are found in social media that often use a\n",
      "non-canonical witting (Rosales et al.).\n",
      "Code-Swithing This phenomenon is strongly\n",
      "present in ILA, as all of these languages are mi-\n",
      "nority languages in their own countries. The\n",
      "bilingualism among their communities is strong\n",
      "(and CS is a common phenomenon in this setup\n",
      "(etino glu, 2017)). The final result of this phe-\n",
      "nomenon is the inclusion of code-switching on a\n",
      "common base (Mager et al., 2019) in their lan-\n",
      "guage.\n",
      "Lack of orthographic normalization The us-\n",
      "age of ILA faces the problem of having a unifiedorthographic standard. This is not always possi-\n",
      "ble, as the suggestions of linguists and official en-\n",
      "tities do not always match the day-by-day writ-\n",
      "ing of the speakers. Moreover, in some cases,\n",
      "special symbols present in the orthographic stan-\n",
      "dards are not accessible in English or Spanish key-\n",
      "board and need to be replaced with other symbols.\n",
      "The winner of the AmericasNLP ST got important\n",
      "improvements using orthographic normalizers de-\n",
      "veloped specifically for each American language\n",
      "(Vzquez et al., 2021).\n",
      "Dialectal variety The indigenous languages\n",
      "have a strong dialectal variety, making it hard for\n",
      "native speakers to understand even speakers from\n",
      "neighboring villages. The linguistic richness of\n",
      "entire regions is so diverse that even a single state\n",
      "like the Mexican Oaxaca could correspond to the\n",
      "diversity in the whole Europe (McQuown, 1955).\n",
      "\n",
      "\n",
      "Top-ranked sentences for Section 4 Available MT datasets for ILA:\n",
      "4 Available MT datasets for ILA\n",
      "The parallel datasets available for MT have been\n",
      "increasing during the last years. At this moment,\n",
      "we can show in two folds the development of these\n",
      "resources: as shown in table 2 work on specific\n",
      "language has emerged; but also broader datasets\n",
      "have started to cover the ILA (see table 1).\n",
      "Language-specific corpus collection work has\n",
      "been done for many languages, where parallel\n",
      "corpus has been the main component. In re-\n",
      "cent time we have seen CherokeeEnglish (OPUS)\n",
      "(Zhang et al., 2020c), WixarikaSpanish (Mager\n",
      "et al., 2018a), ShipioKonibo (Feldman and Coto-\n",
      "Solano, 2020), and others (see table 2). The most\n",
      "prominent of these datasets has been the Inuktitut\n",
      "English parallel data. The last version of this\n",
      "dataset corpora (Joanis et al., 2020) is has medium\n",
      "size with 1,450,094 sentences. Previous versions\n",
      "of this corpus are (Martin et al., 2003). This data\n",
      "set was used for the WMT 2020 Shared Task on\n",
      "Unsupervised, and Low Resourced MT (Barrault\n",
      "et al., 2020).\n",
      "For wide-spoken languages like Guarani, it is\n",
      "even possible to collect a web crawled dataset,\n",
      "including news articles and social media parallel\n",
      "aligned data (Chiruzzo et al., 2020; Gngora et al.,\n",
      "2021) This dataset also includes monolingual data.\n",
      "This is possible as Guaran is one of the most spo-\n",
      "ken indigenous languages of the continent.\n",
      "In contrast to the language-specific datasets,\n",
      "we find broader approaches (see table 1). The\n",
      "broadest multilingual dataset, which contains theDataset Paired-languages Authors\n",
      "AmericasNLI Aymara, Ashninka, Bribri, Guaran,\n",
      "Nahuatl, Otom, Quechua, Rarmuri,\n",
      "Shipibo-Konibo, Wixarika(Ebrahimi et al., 2022)\n",
      "CPML Chol, Maya, Mazatec, Mixtec, Nahu-\n",
      "atl and Otomi(Sierra Martnez et al., 2020)\n",
      "OPUS * (Tiedemann, 2016)\n",
      "New testament Bible * (McCarthy et al., 2020)\n",
      "Table 1: Parallel dataset collections that contain one or more indigenous languages of the Americas\n",
      "Language Paried-language ISO Family Sentences Domain Authors\n",
      "Ashninka Spanish cni Arawak 3883 (Ortega et al., 2020b)\n",
      "Bribri Spanish bzd Chibchan 5923 (Feldman and Coto-\n",
      "Solano, 2020)\n",
      "Guarani Spanish gn Tupi-Guarani News,\n",
      "Blogs(Abdelali et al., 2006)\n",
      "Guarani Spanish gn Tupi-Guarani 14,531 News,\n",
      "Blogs(Chiruzzo et al., 2020)\n",
      "Guarani Spanish gn Tupi-Guarani 14,792 News, So-\n",
      "cial Media(Gngora et al., 2021)\n",
      "Guarani Spanish gn Tupi-Guarani 30855 8 Domains (Chiruzzo et al., 2022)\n",
      "Nahuatl Spanish nah Uto-Aztecan 16145 Diverse\n",
      "Books(Gutierrez-Vasques\n",
      "et al., 2016)\n",
      "Otom Spanish oto Oto-Manguean 4889 Diverse\n",
      "Bookshttps://\n",
      "tsunkua.elotl.\n",
      "mx\n",
      "Rarmuri Spanish tar Uto-Aztecan 14721 Dictionary\n",
      "Examples(Mager et al., 2022)\n",
      "Shipibo-Konibo Spanish shp Panoan 14592 Educational,\n",
      "Religious(Galarreta et al., 2017)\n",
      "Wixarika Spanish hch Uto-Aztecan 8966 Literature (Mager et al., 2018a)\n",
      "Cherokee English chr Uto-Aztecan OPUS (Zhang et al., 2020c)\n",
      "Inuktitut English iku EskimoAleut 1,450,094 Legislative (Joanis et al., 2020)\n",
      "Ayuuk Spanish mir MixeZoque 7553 Diverse (Zacaras Mrquez and\n",
      "Meza Ruiz, 2021)\n",
      "Mazatec Spanish Many Oto-Manguean 9799 Diverse (Tonja et al., 2023)\n",
      "Mixtec Spanish Many Oto-Manguean 13235 Diverse (Tonja et al., 2023)\n",
      "Table 2: Parallel datasets that have been released focusing on one indigenous language\n",
      "Bibles New Testament, includes about 1600 lan-\n",
      "guages (Mayer and Cysouw, 2014; McCarthy\n",
      "et al., 2020) of the 2,508 that have been collected\n",
      "by the Summer Institute of Linguistic (SIL) (An-\n",
      "derson and Anderson, 2012). Another remarkable\n",
      "effort to obtain broad language coverage is the\n",
      "PanLex project (Kamholz et al., 2014), which has\n",
      "gathered lexical translation dictionaries for over\n",
      "5,700 languages. However, for most languages,\n",
      "PanLex contains only a few dozen words. Duan\n",
      "et al. (2020) show that such dictionaries can be\n",
      "used to create an NMT system, making bilingual\n",
      "dictionaries relevant for further studies.\n",
      "Recently community-driven research groups\n",
      "have started the creation of own parallel datasets,\n",
      "such as Masakhane (Orife et al., 2020; Nekoto\n",
      "et al., 2020) for African languages, and Americ-\n",
      "asNLP for indigenous languages of the Americas(Ebrahimi et al., 2021; Mager et al., 2021). The\n",
      "AmericasNLI dataset is an important effort to have\n",
      "a common evaluation benchmark for the 10 in-\n",
      "digenous languages of the Americas for the MT\n",
      "and NLI tasks.\n",
      "Given the constitutional rights of indigenous\n",
      "languages in many countries of the Americas, it is\n",
      "possible to access this data. Vzquez et al. (2021)\n",
      "made available this resource during their shared\n",
      "task system development.\n",
      "Finally, it is important to mention that many\n",
      "of the languages spoken in the Americas have\n",
      "Wikipedias set of articles available4.\n",
      "4The available languages in wikipedia can be consulted\n",
      "at:https://es.wikipedia.org/wiki/Portal:\n",
      "Lenguas_ind genas_de_Amlrica . Until the\n",
      "publication of this article, there were only entries in Nahu-\n",
      "atl, Navajo, Guarani, Aymara, Klaalisut, Esquimal, Inukitut,\n",
      "Cherokee, and Cree.Collection of New Data A common way to cre-\n",
      "ate parallel data with the help of bilingual speakers\n",
      "is via elicitation (translating the foreign text into\n",
      "another language). It has the disadvantage of bias-\n",
      "ing the created text to forms and topics, culture,\n",
      "and even grammatical forms towards the source\n",
      "language (Lrscher, 2005). A method that avoids\n",
      "this problem is language documentation, which\n",
      "consists of storing and annotating commonly used\n",
      "speech or text (Himmelmann, 2008). However, it\n",
      "is costly and requires specialists. In this process,\n",
      "involving the community members that are bilin-\n",
      "gual speakers is important (Bird, 2020).\n",
      "\n",
      "\n",
      "Top-ranked sentences for Section 5 Low-resource MT:\n",
      "5 Low-resource MT\n",
      "For the purpose of this paper we define LRLs\n",
      "as languages for which standard techniques are\n",
      "unable to create well performing systems, which\n",
      "makes it necessary to resort to other techniques\n",
      "(cf. Figure 1) such as transfer learning. For MT,\n",
      "the amount of available resources differs widely\n",
      "across language pairs: some have less than 10k\n",
      "parallel sentences, while other have more than\n",
      "500k, with some exceptions in the orders of sev-\n",
      "eral million.\n",
      "Emulating a low-resource scenario by down-\n",
      "sampling available data for high-resource lan-\n",
      "guages is common and helps understanding a\n",
      "models performance across different settings.\n",
      "However, further evaluating methods on a diverse\n",
      "set of low-resource languages is crucial, since\n",
      "many languages exhibit particular linguistic phe-\n",
      "nomena (Mager et al., 2020), that perturb the fi-\n",
      "nal results, especially since most large datasets\n",
      "are from the Indo-European language family, to\n",
      "which only 6.16% of the worlds languages belong\n",
      "(Lewis, 2009).\n",
      "Importantly, there is no strong correlation be-\n",
      "tween the number of resources available per lan-\n",
      "guage and the number of speakers: Javanese with\n",
      "95 million speakers and Kannada with 44 million\n",
      "are considered LRLs, while French, with only 64\n",
      "million native speakers, is among the most widely\n",
      "studied languages. Improving models to handle\n",
      "LRLs will extend access to information online as\n",
      "well as human language technology to all mono-\n",
      "lingual speakers of those languages. In the case\n",
      "of ILA, most languages are endangered at some\n",
      "degree, but most of them have the same issue:\n",
      "they are low resourced for parallel and monolin-\n",
      "gual data.Endangered Languages Krauss (1992) esti-\n",
      "mates that 50% of all languages are doomed or\n",
      "dying, and that in this century we will see either\n",
      "the death or the doom of 90% of all human lan-\n",
      "guages. The current proportion of languages that\n",
      "are already extinct or moribund ranges from 31%\n",
      "down to 8% depending on the region, with the\n",
      "most severe cases in the Americas and Australia\n",
      "(Simons and Lewis, 2013). To determine how en-\n",
      "dangered a language is, Lewis and Simons (2010)\n",
      "proposes a classification scale called EGIDS with\n",
      "13 levels. The higher the number on this scale,\n",
      "the greater the level of disruption of the languages\n",
      "inter-generational transmission.5MT for endan-\n",
      "gered LRLs has the potential to help with doc-\n",
      "umentation, promotion and revitalization efforts\n",
      "(Galla, 2016; Mager et al., 2018b). However, as\n",
      "these languages are commonly spoken by small\n",
      "communities, or indigenous people, researchers\n",
      "should aim for a direct involvement of those com-\n",
      "munities (Bird, 2020).\n",
      "What is polysynthesis? A polysynthetic lan-\n",
      "guage is defined by the following linguistic fea-\n",
      "tures: the verb in a polysynthetic language must\n",
      "have an agreement with the subject, objects and\n",
      "indirect objects (Baker, 1996); nouns can be in-\n",
      "corporated into the complex verb morphology\n",
      "(Mithun, 1986); and, therefore, polysynthetic lan-\n",
      "guages have agreement morphemes, pronominal\n",
      "affixes and incorporated roots in the verb (Baker,\n",
      "1996), and also encode their relations and charac-\n",
      "terizations into that verb. The most common word\n",
      "orders present in these languages are SOV , VSO,\n",
      "SVO and free order. It is important to notice that\n",
      "a polysynthtic language can have a aggutinative6\n",
      "or can have also fusional characteristics, like To-\n",
      "tonaco or Tepehua (Mager et al., 2020).\n",
      "\n",
      "\n",
      "Top-ranked sentences for Section 6 Low-resource MT paradigms:\n",
      "8 Ethical aspects\n",
      "When working with ILAs are also interacting with\n",
      "communities and nations that speak these lan-\n",
      "guages. In most cases, these speakers have been\n",
      "exposed to a colonial past, or to a local oppres-\n",
      "sion, by the majority language and culture. It is\n",
      "important to point to best practices and recom-\n",
      "mendations when performing our research. Bird\n",
      "(2020) and Liu et al. (2022) advocate to include\n",
      "community members as co-authors (Liu et al.,\n",
      "2022) as well as considering data and technology\n",
      "sovereignty. This is also aligned with the com-\n",
      "munity building aimed at by Zhang et al. (2022).\n",
      "Mager et al. (2023) summarizes the main aspects\n",
      "that should be considered as follows: i) Consul-\n",
      "tation, Negotiation and Mutual Understanding . It\n",
      "is important to inform the community about the\n",
      "planned research, negotiating a possible outcome,\n",
      "and reaching a mutual agreement on the direc-\n",
      "tions and details of the project should happen in\n",
      "all cases. ii) Respect of the local culture and in-\n",
      "volvement . As each community has its own cul-\n",
      "ture and view of the world, researchers should be\n",
      "familiar with the history and traditions of the com-\n",
      "munity. Also, it should be recommended that lo-\n",
      "cal researchers, speakers, or internal governments\n",
      "should be involved in the project. iii) Sharing and\n",
      "distribution of data and research . The product\n",
      "of the research should be available for use by the\n",
      "community, so they can take advantage of the gen-\n",
      "erated materials, like papers, books, or data.\n",
      "\n",
      "\n",
      "Top-ranked sentences for Section 7 Advances in MT for the indigenous languages of the Americas:\n",
      "9 Conclusion\n",
      "Machine translation for ILA has gained interest in\n",
      "the NLP community over the last few years. Here,\n",
      "we provide an exhaustive overview of the basic\n",
      "MT concepts and the particular challenges for MT\n",
      "for ILA (in the context of low-resource scenarios\n",
      "and its relation to endangered languages). We ad-\n",
      "ditionally survey the current advances of MT for\n",
      "these languages.\n",
      "Limitations\n",
      "This papers aim is to give an introduction to re-\n",
      "searchers, students, of interested community in-\n",
      "digenous community members to the topic of Ma-\n",
      "chine Translation for Indigenous languages of the\n",
      "8http://turing.iimas.unam.mx/\n",
      "americasnlp/st.htmlAmericas. Therefore, this paper is not an in-depth\n",
      "survey of the literature on indigenous languages\n",
      "nor a more technical survey of low-resource ma-\n",
      "chine translation. We would point the reader to\n",
      "more specific surveys on these aspects (Haddow\n",
      "et al., 2022; Mager et al., 2018b).\n",
      "Ethical statement\n",
      "We could not find any specific Ethical issue for\n",
      "this paper or potential danger. Nevertheless, we\n",
      "want to point to the reader that working with in-\n",
      "digenous languages (in this case, MT) implies a\n",
      "set of ethical questions that are important to han-\n",
      "dle. For a deeper understanding of the matter, we\n",
      "suggest specialized literature to the reader (Mager\n",
      "et al., 2023; Bird, 2020; Schwartz, 2022).\n",
      "References\n",
      "Ahmed Abdelali, James Cowie, Steve Helmreich,\n",
      "Wanying Jin, Maria Pilar Milagros, Bill Ogden,\n",
      "Mansouri Rad, and Ron Zacharski. 2006. Guarani:\n",
      "A case study in resource development for quick\n",
      "ramp-up MT. In Proceedings of the 7th Confer-\n",
      "ence of the Association for Machine Translation in\n",
      "the Americas: Technical Papers , pages 19, Cam-\n",
      "bridge, Massachusetts, USA. Association for Ma-\n",
      "chine Translation in the Americas.\n",
      "Idris Abdulmumin, Bashir Shehu Galadanci, and Aliyu\n",
      "Garba. 2019. Tag-less back-translation. arXiv\n",
      "preprint arXiv:1912.10514 .\n",
      "Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019.\n",
      "Massively multilingual neural machine translation.\n",
      "InProceedings of the 2019 Conference of the North\n",
      "American Chapter of the Association for Computa-\n",
      "tional Linguistics: Human Language Technologies,\n",
      "Volume 1 (Long and Short Papers) , pages 3874\n",
      "3884.\n",
      "Benyamin Ahmadnia and Bonnie J Dorr. 2019. Aug-\n",
      "menting neural machine translation through round-\n",
      "trip training approach. Open Computer Science ,\n",
      "9(1):268278.\n",
      "Antonios Anastasopoulos and David Chiang. 2018.\n",
      "Tied multitask learning for neural speech translation.\n",
      "InProceedings of the 2018 Conference of the North\n",
      "American Chapter of the Association for Computa-\n",
      "tional Linguistics: Human Language Technologies,\n",
      "Volume 1 (Long Papers) , pages 8291, New Orleans,\n",
      "Louisiana. Association for Computational Linguis-\n",
      "tics.\n",
      "Stephen R Anderson and Stephen Anderson. 2012.\n",
      "Languages: A very short introduction , volume 320.\n",
      "Oxford University Press.Naveen Arivazhagan, Ankur Bapna, Orhan Firat,\n",
      "Roee Aharoni, Melvin Johnson, and Wolfgang\n",
      "Macherey. 2019a. The missing ingredient in zero-\n",
      "shot neural machine translation. arXiv preprint\n",
      "arXiv:1903.07091 .\n",
      "Naveen Arivazhagan, Ankur Bapna, Orhan Firat,\n",
      "Dmitry Lepikhin, Melvin Johnson, Maxim Krikun,\n",
      "Mia Xu Chen, Yuan Cao, George Foster, Colin\n",
      "Cherry, et al. 2019b. Massively multilingual neural\n",
      "machine translation in the wild: Findings and chal-\n",
      "lenges. arXiv preprint arXiv:1907.05019 .\n",
      "Mikel Artetxe, Gorka Labaka, Eneko Agirre, and\n",
      "Kyunghyun Cho. 2018. Unsupervised neural ma-\n",
      "chine translation. In 6th International Conference\n",
      "on Learning Representations, ICLR 2018 .\n",
      "Mikel Artetxe and Holger Schwenk. 2019. Mas-\n",
      "sively multilingual sentence embeddings for zero-\n",
      "shot cross-lingual transfer and beyond. Transac-\n",
      "tions of the Association for Computational Linguis-\n",
      "tics, 7:597610.\n",
      "Duygu Ataman and Marcello Federico. 2018. Compo-\n",
      "sitional representation of morphologically-rich input\n",
      "for neural machine translation. In Proceedings of\n",
      "the 56th Annual Meeting of the Association for Com-\n",
      "putational Linguistics (Volume 2: Short Papers) ,\n",
      "pages 305311.\n",
      "Duygu Ataman, Matteo Negri, Marco Turchi, and Mar-\n",
      "cello Federico. 2017. Linguistically motivated vo-\n",
      "cabulary reduction for neural machine translation\n",
      "from turkish to english.\n",
      "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\n",
      "ton. 2016. Layer normalization. stat, 1050:21.\n",
      "Mark C Baker. 1996. The polysynthesis parameter .\n",
      "Oxford University Press.\n",
      "Mona Baker. 2018. In other words: A coursebook on\n",
      "translation . Routledge.\n",
      "Loc Barrault, Magdalena Biesialska, Ond rej Bojar,\n",
      "Marta R. Costa-juss, Christian Federmann, Yvette\n",
      "Graham, Roman Grundkiewicz, Barry Haddow,\n",
      "Matthias Huck, Eric Joanis, Tom Kocmi, Philipp\n",
      "Koehn, Chi-kiu Lo, Nikola Ljubei c, Christof\n",
      "Monz, Makoto Morishita, Masaaki Nagata, Toshi-\n",
      "aki Nakazawa, Santanu Pal, Matt Post, and Marcos\n",
      "Zampieri. 2020. Findings of the 2020 conference on\n",
      "machine translation (WMT20). In Proceedings of\n",
      "the Fifth Conference on Machine Translation , pages\n",
      "155, Online. Association for Computational Lin-\n",
      "guistics.\n",
      "Andrew R Barron. 1993. Universal approximation\n",
      "bounds for superpositions of a sigmoidal func-\n",
      "tion. IEEE Transactions on Information theory ,\n",
      "39(3):930945.\n",
      "Christos Baziotis, Barry Haddow, and Alexandra\n",
      "Birch. 2020. Language model prior for low-\n",
      "resource neural machine translation. arXiv preprint\n",
      "arXiv:2004.14928 .Yonatan Belinkov and Yonatan Bisk. 2018. Synthetic\n",
      "and natural noise both break neural machine transla-\n",
      "tion. In International Conference on Learning Rep-\n",
      "resentations .\n",
      "Steven Bird. 2020. Decolonising speech and lan-\n",
      "guage technology. In Proceedings of the 28th Inter-\n",
      "national Conference on Computational Linguistics ,\n",
      "pages 35043519, Barcelona, Spain (Online). Inter-\n",
      "national Committee on Computational Linguistics.\n",
      "Graeme Blackwood, Miguel Ballesteros, and Todd\n",
      "Ward. 2018. Multilingual neural machine transla-\n",
      "tion with task-specific attention. In Proceedings of\n",
      "the 27th International Conference on Computational\n",
      "Linguistics , pages 31123122.\n",
      "Piotr Bojanowski, Edouard Grave, Armand Joulin, and\n",
      "Tomas Mikolov. 2017. Enriching word vectors with\n",
      "subword information. Transactions of the Associa-\n",
      "tion for Computational Linguistics , 5:135146.\n",
      "Marcel Bollmann, Rahul Aralikatte, Hctor Murri-\n",
      "eta Bello, Daniel Hershcovich, Miryam de Lhoneux,\n",
      "and Anders Sgaard. 2021. Moses and the\n",
      "character-based random babbling baseline:\n",
      "CoAStaL at AmericasNLP 2021 shared task.\n",
      "InProceedings of the First Workshop on Natural\n",
      "Language Processing for Indigenous Languages of\n",
      "the Americas , pages 248254, Online. Association\n",
      "for Computational Linguistics.\n",
      "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\n",
      "Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\n",
      "Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n",
      "Askell, Sandhini Agarwal, Ariel Herbert-V oss,\n",
      "Gretchen Krueger, Tom Henighan, Rewon Child,\n",
      "Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\n",
      "Clemens Winter, Christopher Hesse, Mark Chen,\n",
      "Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\n",
      "Chess, Jack Clark, Christopher Berner, Sam Mc-\n",
      "Candlish, Alec Radford, Ilya Sutskever, and Dario\n",
      "Amodei. 2020. Language models are few-shot\n",
      "learners.\n",
      "Gina Bustamante, Arturo Oncevay, and Roberto\n",
      "Zariquiey. 2020. No data to crawl? monolingual\n",
      "corpus creation from PDF files of truly low-resource\n",
      "languages in Peru. In Proceedings of the 12th Lan-\n",
      "guage Resources and Evaluation Conference , pages\n",
      "29142923, Marseille, France. European Language\n",
      "Resources Association.\n",
      "Lyle Campbell. 2000. American Indian languages: the\n",
      "historical linguistics of Native America , volume 4.\n",
      "Oxford University Press on Demand.\n",
      "Rich Caruana. 1997. Multitask learning. Machine\n",
      "learning , 28(1):4175.\n",
      "Isaac Caswell, Ciprian Chelba, and David Grangier.\n",
      "2019. Tagged back-translation. In Proceedings of\n",
      "the Fourth Conference on Machine Translation (Vol-\n",
      "ume 1: Research Papers) , pages 5363.Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao.\n",
      "2020. Evaluation of text generation: A survey.\n",
      "arXiv preprint arXiv:2006.14799 .\n",
      "zlem etino glu. 2017. A code-switching corpus of\n",
      "Turkish-German conversations. In Proceedings of\n",
      "the 11th Linguistic Annotation Workshop , pages 34\n",
      "40, Valencia, Spain. Association for Computational\n",
      "Linguistics.\n",
      "Bharathi Raja Chakravarthi, Ruba Priyadharshini,\n",
      "Shubhanker Banerjee, Richard Saldanha, John P.\n",
      "McCrae, Anand Kumar M, Parameswari Krishna-\n",
      "murthy, and Melvin Johnson. 2021. Findings of the\n",
      "shared task on machine translation in Dravidian lan-\n",
      "guages. In Proceedings of the First Workshop on\n",
      "Speech and Language Technologies for Dravidian\n",
      "Languages , pages 119125, Kyiv. Association for\n",
      "Computational Linguistics.\n",
      "Eirini Chatzikoumi. 2020. How to evaluate machine\n",
      "translation: A review of automated and human met-\n",
      "rics. Natural Language Engineering , 26(2):137\n",
      "161.\n",
      "Guanhua Chen, Shuming Ma, Yun Chen, Li Dong,\n",
      "Dongdong Zhang, Jia Pan, Wenping Wang, and Furu\n",
      "Wei. 2021. Zero-shot cross-lingual transfer of neu-\n",
      "ral machine translation with multilingual pretrained\n",
      "encoders. In Proceedings of the 2021 Conference on\n",
      "Empirical Methods in Natural Language Process-\n",
      "ing, pages 1526, Online and Punta Cana, Domini-\n",
      "can Republic. Association for Computational Lin-\n",
      "guistics.\n",
      "Yong Cheng. 2019. Joint training for pivot-based neu-\n",
      "ral machine translation. In Joint Training for Neural\n",
      "Machine Translation , pages 4154. Springer.\n",
      "Yong Cheng, Lu Jiang, and Wolfgang Macherey. 2019.\n",
      "Robust neural machine translation with doubly ad-\n",
      "versarial inputs. In Proceedings of the 57th Annual\n",
      "Meeting of the Association for Computational Lin-\n",
      "guistics , pages 43244333.\n",
      "Yong Cheng, Lu Jiang, Wolfgang Macherey, and Ja-\n",
      "cob Eisenstein. 2020. AdvAug: Robust adversar-\n",
      "ial augmentation for neural machine translation. In\n",
      "Proceedings of the 58th Annual Meeting of the Asso-\n",
      "ciation for Computational Linguistics , pages 5961\n",
      "5970, Online. Association for Computational Lin-\n",
      "guistics.\n",
      "Yong Cheng, Zhaopeng Tu, Fandong Meng, Junjie\n",
      "Zhai, and Yang Liu. 2018. Towards robust neural\n",
      "machine translation. In Proceedings of the 56th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics (Volume 1: Long Papers) , pages 1756\n",
      "1766.\n",
      "Luis Chiruzzo, Pedro Amarilla, Adolfo Ros, and Gus-\n",
      "tavo Gimnez Lugo. 2020. Development of a\n",
      "Guarani - Spanish parallel corpus. In Proceedings of\n",
      "the 12th Language Resources and Evaluation Con-\n",
      "ference , pages 26292633, Marseille, France. Euro-\n",
      "pean Language Resources Association.Luis Chiruzzo, Santiago Gngora, Aldo Alvarez, Gus-\n",
      "tavo Gimnez-Lugo, Marvin Agero-Torales, and\n",
      "Yliana Rodrguez. 2022. Jojajovai: A parallel\n",
      "guarani-spanish corpus for mt benchmarking. In\n",
      "Proceedings of the Thirteenth Language Resources\n",
      "and Evaluation Conference , pages 20982107.\n",
      "Kyunghyun Cho, Bart van Merrinboer, Caglar Gul-\n",
      "cehre, Dzmitry Bahdanau, Fethi Bougares, Holger\n",
      "Schwenk, and Yoshua Bengio. 2014. Learning\n",
      "phrase representations using rnn encoderdecoder\n",
      "for statistical machine translation. In Proceedings of\n",
      "the 2014 Conference on Empirical Methods in Nat-\n",
      "ural Language Processing (EMNLP) , pages 1724\n",
      "1734.\n",
      "Trevor Cohn and Mirella Lapata. 2007. Machine trans-\n",
      "lation by triangulation: Making effective use of\n",
      "multi-parallel corpora. In Proceedings of the 45th\n",
      "Annual Meeting of the Association of Computational\n",
      "Linguistics , pages 728735.\n",
      "Alexis Conneau and Guillaume Lample. 2019. Cross-\n",
      "lingual language model pretraining. In Advances\n",
      "in Neural Information Processing Systems , pages\n",
      "70577067.\n",
      "Alexis Conneau, Guillaume Lample, MarcAurelio\n",
      "Ranzato, Ludovic Denoyer, and Herv Jgou. 2017.\n",
      "Word translation without parallel data. arXiv\n",
      "preprint arXiv:1710.04087 .\n",
      "Raj Dabre, Chenhui Chu, and Anoop Kunchukuttan.\n",
      "2019. A survey of multilingual neural machine\n",
      "translation. arXiv preprint arXiv:1905.05395 .\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\n",
      "Kristina Toutanova. 2019. Bert: Pre-training of\n",
      "deep bidirectional transformers for language under-\n",
      "standing. In Proceedings of the 2019 Conference of\n",
      "the North American Chapter of the Association for\n",
      "Computational Linguistics: Human Language Tech-\n",
      "nologies, Volume 1 (Long and Short Papers) , pages\n",
      "41714186.\n",
      "Xiangyu Duan, Baijun Ji, Hao Jia, Min Tan, Min\n",
      "Zhang, Boxing Chen, Weihua Luo, and Yue Zhang.\n",
      "2020. Bilingual dictionary based neural machine\n",
      "translation without using parallel sentences. In Pro-\n",
      "ceedings of the 58th Annual Meeting of the Asso-\n",
      "ciation for Computational Linguistics , pages 1570\n",
      "1579.\n",
      "Abteen Ebrahim, Manuel Mager, Pavel Oncevay Ar-\n",
      "turo Danni Liu Koneru Sai Ugan Enes Yavuz\n",
      "Wiemerslage, Adam Denisov, Zhaolin Li, Jan\n",
      "Niehues, Monica Romero, Ivan G Torre, Tanel\n",
      "Alume, Jiaming Kong, Sergey Polezhaev, Yury\n",
      "Belousov, Wei-Rui Chen, Peter Sullivan, Ife\n",
      "Adebara, Bashar Talafha, Inciarte Alcides Al-\n",
      "coba, Muhammad Abdul-Mageed, Luis Chiruzzo,\n",
      "Rolando Coto-Solano, Hilaria Cruz, Sofa Flores-\n",
      "Solrzano, Aldo Andrs Alvarez Lpez, Ivan Meza-\n",
      "Ruiz, John E. Ortega, Alexis Palmer, Rodolfo Joel\n",
      "Zevallos Salazar, Kristine, Thang Vu Stenzel, andKatharina Kann. 2023. Findings of the second amer-\n",
      "icasnlp competition on speech-to-text translation.\n",
      "preprint .\n",
      "Abteen Ebrahimi, Manuel Mager, Arturo Once-\n",
      "vay, Vishrav Chaudhary, Luis Chiruzzo, Angela\n",
      "Fan, John Ortega, Ricardo Ramos, Annette Rios,\n",
      "Ivan Vladimir Meza Ruiz, Gustavo Gimnez-Lugo,\n",
      "Elisabeth Mager, Graham Neubig, Alexis Palmer,\n",
      "Rolando Coto-Solano, Thang Vu, and Katharina\n",
      "Kann. 2022. AmericasNLI: Evaluating zero-shot\n",
      "natural language understanding of pretrained multi-\n",
      "lingual models in truly low-resource languages. In\n",
      "Proceedings of the 60th Annual Meeting of the As-\n",
      "sociation for Computational Linguistics (Volume 1:\n",
      "Long Papers) , pages 62796299, Dublin, Ireland.\n",
      "Association for Computational Linguistics.\n",
      "Abteen Ebrahimi, Manuel Mager, Arturo Oncevay,\n",
      "Vishrav Chaudhary, Luis Chiruzzo, Angela Fan,\n",
      "John Ortega, Ricardo Ramos, Annette Rios, Ivan\n",
      "Vladimir, Gustavo A. Gimnez-Lugo, Elisabeth\n",
      "Mager, Graham Neubig, Alexis Palmer, Rolando\n",
      "A. Coto Solano, Ngoc Thang Vu, and Katharina\n",
      "Kann. 2021. Americasnli: Evaluating zero-shot nat-\n",
      "ural language understanding of pretrained multilin-\n",
      "gual models in truly low-resource languages.\n",
      "Abteen Ebrahimi, Manuel Mager, Arturo Oncevay,\n",
      "Enora Rice, Cynthia Montao, John Ortega, Shruti\n",
      "Rijhwani, Alexis Palmer, Rolando Coto-Solano, Hi-\n",
      "laria Cruz, and Katharina Kann. 2023. Findings\n",
      "of the AmericasNLP 2023 shared task on machine\n",
      "translation into indigenous languages. In Proceed-\n",
      "ings of the Third Workshop on Natural Language\n",
      "Processing for Indigenous Languages of the Amer-\n",
      "icas. Association for Computational Linguistics.\n",
      "Javid Ebrahimi, Daniel Lowd, and Dejing Dou. 2018.\n",
      "On adversarial examples for character-level neural\n",
      "machine translation. In Proceedings of the 27th In-\n",
      "ternational Conference on Computational Linguis-\n",
      "tics, pages 653663, Santa Fe, New Mexico, USA.\n",
      "Association for Computational Linguistics.\n",
      "Sergey Edunov, Alexei Baevski, and Michael Auli.\n",
      "2019. Pre-trained language model representations\n",
      "for language generation. In Proceedings of the 2019\n",
      "Conference of the North American Chapter of the\n",
      "Association for Computational Linguistics: Human\n",
      "Language Technologies, Volume 1 (Long and Short\n",
      "Papers) , pages 40524059.\n",
      "Sergey Edunov, Myle Ott, Michael Auli, and David\n",
      "Grangier. 2018. Understanding back-translation at\n",
      "scale. In Proceedings of the 2018 Conference on\n",
      "Empirical Methods in Natural Language Process-\n",
      "ing, pages 489500.\n",
      "Marzieh Fadaee, Arianna Bisazza, and Christof Monz.\n",
      "2017. Data augmentation for low-resource neural\n",
      "machine translation. In Proceedings of the 55th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics (Volume 2: Short Papers) , pages 567\n",
      "573.Isaac Feldman and Rolando Coto-Solano. 2020. Neu-\n",
      "ral machine translation models with back-translation\n",
      "for the extremely low-resource indigenous language\n",
      "Bribri. In Proceedings of the 28th International\n",
      "Conference on Computational Linguistics , pages\n",
      "39653976, Barcelona, Spain (Online). Interna-\n",
      "tional Committee on Computational Linguistics.\n",
      "Manuel Fernndez-Delgado, Eva Cernadas, Senn\n",
      "Barro, and Dinani Amorim. 2014. Do we need hun-\n",
      "dreds of classifiers to solve real world classification\n",
      "problems? The journal of machine learning re-\n",
      "search , 15(1):31333181.\n",
      "Alexander Fraser. 2020. Findings of the WMT 2020\n",
      "shared tasks in unsupervised MT and very low re-\n",
      "source supervised MT. In Proceedings of the Fifth\n",
      "Conference on Machine Translation , pages 765\n",
      "771, Online. Association for Computational Lin-\n",
      "guistics.\n",
      "Ana-Paula Galarreta, Andrs Melgar, and Arturo On-\n",
      "cevay. 2017. Corpus creation and initial SMT ex-\n",
      "periments between Spanish and Shipibo-konibo. In\n",
      "Proceedings of the International Conference Recent\n",
      "Advances in Natural Language Processing, RANLP\n",
      "2017 , pages 238244, Varna, Bulgaria. INCOMA\n",
      "Ltd.\n",
      "Candace Kaleimamoowahinekapu Galla. 2016. Indige-\n",
      "nous language revitalization, promotion, and educa-\n",
      "tion: Function of digital technology. Computer As-\n",
      "sisted Language Learning , 29(7):11371151.\n",
      "Xavier Garcia, Pierre Foret, Thibault Sellam, and\n",
      "Ankur P Parikh. 2020. A multilingual view of\n",
      "unsupervised machine translation. arXiv preprint\n",
      "arXiv:2002.02955 .\n",
      "Mozhdeh Gheini, Xiang Ren, and Jonathan May. 2021.\n",
      "Cross-attention is all you need: Adapting pretrained\n",
      "Transformers for machine translation. In Proceed-\n",
      "ings of the 2021 Conference on Empirical Methods\n",
      "in Natural Language Processing , pages 17541765,\n",
      "Online and Punta Cana, Dominican Republic. Asso-\n",
      "ciation for Computational Linguistics.\n",
      "Santiago Gngora, Nicols Giossa, and Luis Chiruzzo.\n",
      "2021. Experiments on a Guarani corpus of news\n",
      "and social media. In Proceedings of the First Work-\n",
      "shop on Natural Language Processing for Indige-\n",
      "nous Languages of the Americas , pages 153158,\n",
      "Online. Association for Computational Linguistics.\n",
      "Santiago Gngora, Nicols Giossa, and Luis Chiruzzo.\n",
      "2022. Can we use word embeddings for enhancing\n",
      "Guarani-Spanish machine translation? In Proceed-\n",
      "ings of the Fifth Workshop on the Use of Compu-\n",
      "tational Methods in the Study of Endangered Lan-\n",
      "guages , pages 127132, Dublin, Ireland. Associa-\n",
      "tion for Computational Linguistics.\n",
      "Yvette Graham, Barry Haddow, and Philipp Koehn.\n",
      "2020. Statistical power and translationese in ma-\n",
      "chine translation evaluation. In Proceedings of the2020 Conference on Empirical Methods in Natural\n",
      "Language Processing (EMNLP) , pages 7281, On-\n",
      "line. Association for Computational Linguistics.\n",
      "Stig-Arne Grnroos, Sami Virpioja, Peter Smit, and\n",
      "Mikko Kurimo. 2014. Morfessor flatcat: An hmm-\n",
      "based method for unsupervised and semi-supervised\n",
      "learning of morphology. In Proceedings of COLING\n",
      "2014, the 25th International Conference on Compu-\n",
      "tational Linguistics: Technical Papers , pages 1177\n",
      "1185.\n",
      "Jiatao Gu, Yong Wang, Kyunghyun Cho, and Vic-\n",
      "tor OK Li. 2019. Improved zero-shot neural ma-\n",
      "chine translation via ignoring spurious correlations.\n",
      "InProceedings of the 57th Annual Meeting of the\n",
      "Association for Computational Linguistics , pages\n",
      "12581268.\n",
      "Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun\n",
      "Cho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,\n",
      "Holger Schwenk, and Yoshua Bengio. 2015. On us-\n",
      "ing monolingual corpora in neural machine transla-\n",
      "tion. arXiv preprint arXiv:1503.03535 .\n",
      "Ximena Gutierrez-Vasques, Gerardo Sierra, and\n",
      "Isaac Hernandez Pompa. 2016. Axolotl: a web\n",
      "accessible parallel corpus for Spanish-Nahuatl. In\n",
      "Proceedings of the Tenth International Conference\n",
      "on Language Resources and Evaluation (LREC16) ,\n",
      "pages 42104214, Portoro, Slovenia. European\n",
      "Language Resources Association (ELRA).\n",
      "Barry Haddow, Rachel Bawden, Antonio Valerio\n",
      "Miceli Barone, Jind rich Helcl, and Alexandra Birch.\n",
      "2022. Survey of low-resource machine translation.\n",
      "Computational Linguistics , pages 167.\n",
      "Lifeng Han. 2016. Machine translation evaluation re-\n",
      "sources and methods: A survey. arXiv preprint\n",
      "arXiv:1605.04515 .\n",
      "Franois Hernandez and Vincent Nguyen. 2020. The\n",
      "ubiqus English-Inuktitut system for WMT20. In\n",
      "Proceedings of the Fifth Conference on Machine\n",
      "Translation , pages 213217, Online. Association for\n",
      "Computational Linguistics.\n",
      "Nikolaus P Himmelmann. 2008. Language documen-\n",
      "tation: What is it and what is it good for? In Es-\n",
      "sentials of language documentation , pages 130. De\n",
      "Gruyter Mouton.\n",
      "Vu Cong Duy Hoang, Philipp Koehn, Gholamreza\n",
      "Haffari, and Trevor Cohn. 2018. Iterative back-\n",
      "translation for neural machine translation. In Pro-\n",
      "ceedings of the 2nd Workshop on Neural Machine\n",
      "Translation and Generation , pages 1824.\n",
      "Sepp Hochreiter and Jrgen Schmidhuber. 1997.\n",
      "Long short-term memory. Neural computation ,\n",
      "9(8):17351780.\n",
      "J Edward Hu, Huda Khayrallah, Ryan Culkin, Patrick\n",
      "Xia, Tongfei Chen, Matt Post, and Benjamin\n",
      "Van Durme. 2019. Improved lexically constraineddecoding for translation and monolingual rewriting.\n",
      "InProceedings of the 2019 Conference of the North\n",
      "American Chapter of the Association for Computa-\n",
      "tional Linguistics: Human Language Technologies,\n",
      "Volume 1 (Long and Short Papers) , pages 839850.\n",
      "W John Hutchins. 2004. The georgetown-ibm experi-\n",
      "ment demonstrated in january 1954. In Conference\n",
      "of the Association for Machine Translation in the\n",
      "Americas , pages 102114. Springer.\n",
      "Sbastien Jean, Kyunghyun Cho, Roland Memisevic,\n",
      "and Yoshua Bengio. 2015. On using very large\n",
      "target vocabulary for neural machine translation.\n",
      "InProceedings of the 53rd Annual Meeting of the\n",
      "Association for Computational Linguistics and the\n",
      "7th International Joint Conference on Natural Lan-\n",
      "guage Processing (Volume 1: Long Papers) , pages\n",
      "110, Beijing, China. Association for Computa-\n",
      "tional Linguistics.\n",
      "Eric Joanis, Rebecca Knowles, Roland Kuhn, Samuel\n",
      "Larkin, Patrick Littell, Chi-kiu Lo, Darlene Stewart,\n",
      "and Jeffrey Micher. 2020. The Nunavut Hansard\n",
      "InuktitutEnglish parallel corpus 3.0 with prelimi-\n",
      "nary machine translation results. In Proceedings of\n",
      "the 12th Language Resources and Evaluation Con-\n",
      "ference , pages 25622572, Marseille, France. Euro-\n",
      "pean Language Resources Association.\n",
      "Melvin Johnson, Mike Schuster, Quoc V Le, Maxim\n",
      "Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\n",
      "Fernanda Vigas, Martin Wattenberg, Greg Corrado,\n",
      "et al. 2017. Googles multilingual neural machine\n",
      "translation system: Enabling zero-shot translation.\n",
      "Transactions of the Association for Computational\n",
      "Linguistics , 5:339351.\n",
      "David Kamholz, Jonathan Pool, and Susan M Colow-\n",
      "ick. 2014. Panlex: Building a resource for panlin-\n",
      "gual lexical translation. In LREC , pages 31453150.\n",
      "Katharina Kann, Jesus Manuel Mager Hois,\n",
      "Ivan Vladimir Meza-Ruiz, and Hinrich Schtze.\n",
      "2018. Fortification of neural morphological\n",
      "segmentation models for polysynthetic minimal-\n",
      "resource languages. In Proceedings of the 2018\n",
      "Conference of the North American Chapter of the\n",
      "Association for Computational Linguistics: Human\n",
      "Language Technologies, Volume 1 (Long Papers) ,\n",
      "pages 4757, New Orleans, Louisiana. Association\n",
      "for Computational Linguistics.\n",
      "Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua\n",
      "Bengio. 2017. Generalization in deep learning.\n",
      "arXiv preprint arXiv:1710.05468 .\n",
      "Huda Khayrallah, Brian Thompson, Matt Post, and\n",
      "Philipp Koehn. 2020. Simulated multiple reference\n",
      "training improves low-resource machine translation.\n",
      "arXiv preprint arXiv:2004.14524 .\n",
      "Rebecca Knowles, Darlene Stewart, Samuel Larkin,\n",
      "and Patrick Littell. 2020. NRC systems for the 2020\n",
      "Inuktitut-English news translation task. In Proceed-\n",
      "ings of the Fifth Conference on Machine Translation ,pages 156170, Online. Association for Computa-\n",
      "tional Linguistics.\n",
      "Rebecca Knowles, Darlene Stewart, Samuel Larkin,\n",
      "and Patrick Littell. 2021. NRC-CNRC machine\n",
      "translation systems for the 2021 AmericasNLP\n",
      "shared task. In Proceedings of the First Workshop on\n",
      "Natural Language Processing for Indigenous Lan-\n",
      "guages of the Americas , pages 224233, Online. As-\n",
      "sociation for Computational Linguistics.\n",
      "Sosuke Kobayashi. 2018. Contextual augmentation:\n",
      "Data augmentation by words with paradigmatic re-\n",
      "lations. In Proceedings of the 2018 Conference of\n",
      "the North American Chapter of the Association for\n",
      "Computational Linguistics: Human Language Tech-\n",
      "nologies, Volume 2 (Short Papers) , pages 452457.\n",
      "Tom Kocmi. 2020. CUNI submission for the Inuk-\n",
      "titut language in WMT news 2020. In Proceed-\n",
      "ings of the Fifth Conference on Machine Translation ,\n",
      "pages 171174, Online. Association for Computa-\n",
      "tional Linguistics.\n",
      "Michael Krauss. 1992. The worlds languages in crisis.\n",
      "Language , 68(1):410.\n",
      "Mateusz Krubi nski, Marcin Chochowski, Bartomiej\n",
      "Boczek, Mikoaj Koszowski, Adam Dobrowolski,\n",
      "Marcin Szyma nski, and Pawe Przybysz. 2020.\n",
      "Samsung R&D institute Poland submission to\n",
      "WMT20 news translation task. In Proceedings of\n",
      "the Fifth Conference on Machine Translation , pages\n",
      "181190, Online. Association for Computational\n",
      "Linguistics.\n",
      "Surafel M Lakew, Quintino F Lotito, Matteo Negri,\n",
      "Marco Turchi, and Marcello Federico. 2018. Im-\n",
      "proving zero-shot translation of low-resource lan-\n",
      "guages. In Proceedings of the 14h IWSLT , pages\n",
      "113119.\n",
      "Guillaume Lample, Alexis Conneau, Ludovic Denoyer,\n",
      "and MarcAurelio Ranzato. 2017. Unsupervised\n",
      "machine translation using monolingual corpora only.\n",
      "arXiv preprint arXiv:1711.00043 .\n",
      "Sahinur Rahman Laskar, Abdullah Faiz Ur Rah-\n",
      "man Khilji, Partha Pakray, and Sivaji Bandyopad-\n",
      "hyay. 2020. Zero-shot neural machine translation:\n",
      "Russian-Hindi @LoResMT 2020. In Proceedings\n",
      "of the 3rd Workshop on Technologies for MT of Low\n",
      "Resource Languages , pages 3842, Suzhou, China.\n",
      "Association for Computational Linguistics.\n",
      "Yichong Leng, Xu Tan, Tao Qin, Xiang-Yang Li, and\n",
      "Tie-Yan Liu. 2019. Unsupervised pivot translation\n",
      "for distant languages. In Proceedings of the 57th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics , pages 175183.\n",
      "M Paul Lewis. 2009. Ethnologue: Languages of the\n",
      "world . SIL international.\n",
      "M Paul Lewis and Gary F Simons. 2010. Assessing\n",
      "endangerment: expanding fishmans gids. Revue\n",
      "roumaine de linguistique , 55(2):103120.Mike Lewis, Yinhan Liu, Naman Goyal, Mar-\n",
      "jan Ghazvininejad, Abdelrahman Mohamed, Omer\n",
      "Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\n",
      "Bart: Denoising sequence-to-sequence pre-training\n",
      "for natural language generation, translation, and\n",
      "comprehension. arXiv preprint arXiv:1910.13461 .\n",
      "Zuchao Li, Rui Wang, Kehai Chen, Masso Utiyama,\n",
      "Eiichiro Sumita, Zhuosheng Zhang, and Hai Zhao.\n",
      "2020. Data-dependent gaussian prior objective for\n",
      "language generation. In International Conference\n",
      "on Learning Representations .\n",
      "Zehui Lin, Xiao Pan, Mingxuan Wang, Xipeng Qiu,\n",
      "Jiangtao Feng, Hao Zhou, and Lei Li. 2020. Pre-\n",
      "training multilingual neural machine translation by\n",
      "leveraging alignment information. In Proceed-\n",
      "ings of the 2020 Conference on Empirical Methods\n",
      "in Natural Language Processing (EMNLP) , pages\n",
      "26492663, Online. Association for Computational\n",
      "Linguistics.\n",
      "Wang Ling, Isabel Trancoso, Chris Dyer, and Alan W\n",
      "Black. 2015. Character-based neural machine trans-\n",
      "lation. arXiv preprint arXiv:1511.04586 .\n",
      "Patrick Littell, Anna Kazantseva, Roland Kuhn, Aidan\n",
      "Pine, Antti Arppe, Christopher Cox, and Marie-\n",
      "Odile Junker. 2018. Indigenous language technolo-\n",
      "gies in Canada: Assessment, challenges, and suc-\n",
      "cesses. In Proceedings of the 27th International\n",
      "Conference on Computational Linguistics , pages\n",
      "26202632, Santa Fe, New Mexico, USA. Associ-\n",
      "ation for Computational Linguistics.\n",
      "Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\n",
      "Edunov, Marjan Ghazvininejad, Mike Lewis, and\n",
      "Luke Zettlemoyer. 2020. Multilingual denoising\n",
      "pre-training for neural machine translation. arXiv\n",
      "preprint arXiv:2001.08210 .\n",
      "Zihan Liu, Yan Xu, Genta Indra Winata, and Pascale\n",
      "Fung. 2019. Incorporating word and subword units\n",
      "in unsupervised machine translation using language\n",
      "model rescoring. In Proceedings of the Fourth Con-\n",
      "ference on Machine Translation (Volume 2: Shared\n",
      "Task Papers, Day 1) , pages 275282.\n",
      "Zoey Liu, Crystal Richardson, Richard Hatcher Jr, and\n",
      "Emily Prudhommeaux. 2022. Not always about\n",
      "you: Prioritizing community needs when develop-\n",
      "ing endangered language technology. arXiv preprint\n",
      "arXiv:2204.05541 .\n",
      "Chi-kiu Lo. 2020. Extended study on using pretrained\n",
      "language models and YiSi-1 for machine transla-\n",
      "tion evaluation. In Proceedings of the Fifth Confer-\n",
      "ence on Machine Translation , pages 895902, On-\n",
      "line. Association for Computational Linguistics.\n",
      "Wolfgang Lrscher. 2005. The translation process:\n",
      "Methods and problems of its investigation. Meta:\n",
      "journal des traducteurs/Meta: Translators Journal ,\n",
      "50(2):597608.Bruce T Lowerre. 1976. The harpy speech recognition\n",
      "system. Technical report, CARNEGIE-MELLON\n",
      "UNIV PITTSBURGH PA DEPT OF COMPUTER\n",
      "SCIENCE.\n",
      "Yichao Lu, Phillip Keung, Faisal Ladhak, Vikas Bhard-\n",
      "waj, Shaonan Zhang, and Jason Sun. 2018. A neu-\n",
      "ral interlingua for multilingual machine translation.\n",
      "InProceedings of the Third Conference on Machine\n",
      "Translation: Research Papers , pages 8492.\n",
      "Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals,\n",
      "and Wojciech Zaremba. 2015. Addressing the rare\n",
      "word problem in neural machine translation. In Pro-\n",
      "ceedings of the 53rd Annual Meeting of the Associ-\n",
      "ation for Computational Linguistics and the 7th In-\n",
      "ternational Joint Conference on Natural Language\n",
      "Processing (Volume 1: Long Papers) , pages 1119,\n",
      "Beijing, China. Association for Computational Lin-\n",
      "guistics.\n",
      "Manuel Mager, Dinico Carrillo, and Ivan Meza.\n",
      "2018a. Probabilistic finite-state morphological seg-\n",
      "menter for wixarika (huichol) language. Journal of\n",
      "Intelligent & Fuzzy Systems , 34(5):30813087.\n",
      "Manuel Mager, zlem etino glu, and Katharina Kann.\n",
      "2019. Subword-level language identification for\n",
      "intra-word code-switching. In Proceedings of the\n",
      "2019 Conference of the North American Chapter of\n",
      "the Association for Computational Linguistics: Hu-\n",
      "man Language Technologies, Volume 1 (Long and\n",
      "Short Papers) , pages 20052011, Minneapolis, Min-\n",
      "nesota. Association for Computational Linguistics.\n",
      "Manuel Mager, zlem etino glu, and Katharina\n",
      "Kann. 2020. Tackling the low-resource chal-\n",
      "lenge for canonical segmentation. arXiv preprint\n",
      "arXiv:2010.02804 .\n",
      "Manuel Mager, Ximena Gutierrez-Vasques, Gerardo\n",
      "Sierra, and Ivan Meza-Ruiz. 2018b. Challenges of\n",
      "language technologies for the indigenous languages\n",
      "of the Americas. In Proceedings of the 27th Inter-\n",
      "national Conference on Computational Linguistics ,\n",
      "pages 5569, Santa Fe, New Mexico, USA. Associ-\n",
      "ation for Computational Linguistics.\n",
      "Manuel Mager, Elisabeth Mager, Katharina Kann,\n",
      "and Ngoc Thang Vu. 2023. Ethical considerations\n",
      "for machine translation of indigenous languages:\n",
      "Giving a voice to the speakers. arXiv preprint\n",
      "arXiv:2305.19474 .\n",
      "Manuel Mager, Elisabeth Mager, Alfonso Medina-\n",
      "Urrea, Ivan Vladimir Meza Ruiz, and Katharina\n",
      "Kann. 2018c. Lost in translation: Analysis of in-\n",
      "formation loss during machine translation between\n",
      "polysynthetic and fusional languages. In Proceed-\n",
      "ings of the Workshop on Computational Modeling\n",
      "of Polysynthetic Languages , pages 7383, Santa Fe,\n",
      "New Mexico, USA. Association for Computational\n",
      "Linguistics.Manuel Mager and Ivan Meza. 2021. Retos en con-\n",
      "struccin de traductores automticos para lenguas\n",
      "indgenas de Mxico. Digital Scholarship in the Hu-\n",
      "manities , 36.\n",
      "Manuel Mager, Arturo Oncevay, Abteen Ebrahimi,\n",
      "John Ortega, Annette Rios, Angela Fan, Xi-\n",
      "mena Gutierrez-Vasques, Luis Chiruzzo, Gustavo\n",
      "Gimnez-Lugo, Ricardo Ramos, Anna Currey,\n",
      "Vishrav Chaudhary, Ivan Vladimir Meza Ruiz,\n",
      "Rolando Coto-Solano, Alexis Palmer, Elisabeth\n",
      "Mager, Ngoc Thang Vu, Graham Neubig, and\n",
      "Katharina Kann. 2021. Findings of the Americas-\n",
      "NLP 2021 Shared Task on Open Machine Transla-\n",
      "tion for Indigenous Languages of the Americas. In\n",
      "Proceedings of theThe First Workshop on NLP for\n",
      "Indigenous Languages of the Americas , Online. As-\n",
      "sociation for Computational Linguistics.\n",
      "Manuel Mager, Arturo Oncevay, Elisabeth Mager,\n",
      "Katharina Kann, and Thang Vu. 2022. BPE vs. mor-\n",
      "phological segmentation: A case study on machine\n",
      "translation of four polysynthetic languages. In Find-\n",
      "ings of the Association for Computational Linguis-\n",
      "tics: ACL 2022 , pages 961971, Dublin, Ireland.\n",
      "Association for Computational Linguistics.\n",
      "Chaitanya Malaviya, Graham Neubig, and Patrick Lit-\n",
      "tell. 2017. Learning language representations for ty-\n",
      "pology prediction. In Proceedings of the 2017 Con-\n",
      "ference on Empirical Methods in Natural Language\n",
      "Processing , pages 25292535.\n",
      "Benjamin Marie, Raphael Rubino, and Atsushi Fujita.\n",
      "2020. Tagged back-translation revisited: Why does\n",
      "it really work? In Proceedings of the 58th Annual\n",
      "Meeting of the Association for Computational Lin-\n",
      "guistics , pages 59905997, Online. Association for\n",
      "Computational Linguistics.\n",
      "Joel Martin, Howard Johnson, Benoit Farley, and Anna\n",
      "Maclachlan. 2003. Aligning and using an english-\n",
      "inuktitut parallel corpus. In Proceedings of the HLT-\n",
      "NAACL 2003 Workshop on Building and using par-\n",
      "allel texts: data driven machine translation and\n",
      "beyond-Volume 3 , pages 115118. Association for\n",
      "Computational Linguistics.\n",
      "Thomas Mayer and Michael Cysouw. 2014. Creat-\n",
      "ing a massively parallel bible corpus. Oceania ,\n",
      "135(273):40.\n",
      "Arya D. McCarthy, Rachel Wicks, Dylan Lewis,\n",
      "Aaron Mueller, Winston Wu, Oliver Adams, Gar-\n",
      "rett Nicolai, Matt Post, and David Yarowsky. 2020.\n",
      "The johns hopkins university bible corpus: 1600+\n",
      "tongues for typological exploration. In Proceedings\n",
      "of The 12th Language Resources and Evaluation\n",
      "Conference , pages 28842892, Marseille, France.\n",
      "European Language Resources Association.\n",
      "Norman A McQuown. 1955. The indigenous lan-\n",
      "guages of latin america. American Anthropologist ,\n",
      "57(3):501570.Antonio Valerio Miceli-Barone, Jind rich Helcl, Rico\n",
      "Sennrich, Barry Haddow, and Alexandra Birch.\n",
      "2017. Deep architectures for neural machine trans-\n",
      "lation. In Proceedings of the Second Conference on\n",
      "Machine Translation , pages 99107.\n",
      "Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-\n",
      "frey Dean. 2013. Efficient estimation of word\n",
      "representations in vector space. arXiv preprint\n",
      "arXiv:1301.3781 .\n",
      "Marianne Mithun. 1986. On the nature of noun incor-\n",
      "poration. Language , 62(1):3237.\n",
      "Oscar Moreno. 2021. The REPU CS Spanish\n",
      "Quechua submission to the AmericasNLP 2021\n",
      "shared task on open machine translation. In Pro-\n",
      "ceedings of the First Workshop on Natural Language\n",
      "Processing for Indigenous Languages of the Ameri-\n",
      "cas, pages 241247, Online. Association for Com-\n",
      "putational Linguistics.\n",
      "El Moatez Billah Nagoudi, Wei-Rui Chen, Muham-\n",
      "mad Abdul-Mageed, and Hasan Cavusoglu. 2021.\n",
      "IndT5: A text-to-text transformer for 10 indigenous\n",
      "languages. In Proceedings of the First Workshop on\n",
      "Natural Language Processing for Indigenous Lan-\n",
      "guages of the Americas , pages 265271, Online. As-\n",
      "sociation for Computational Linguistics.\n",
      "Wilhelmina Nekoto, Vukosi Marivate, Tshinondiwa\n",
      "Matsila, Timi Fasubaa, Taiwo Fagbohungbe,\n",
      "Solomon Oluwole Akinola, Shamsuddeen Muham-\n",
      "mad, Salomon Kabongo Kabenamualu, Salomey\n",
      "Osei, Freshia Sackey, Rubungo Andre Niyongabo,\n",
      "Ricky Macharm, Perez Ogayo, Orevaoghene Ahia,\n",
      "Musie Meressa Berhe, Mofetoluwa Adeyemi,\n",
      "Masabata Mokgesi-Selinga, Lawrence Okegbemi,\n",
      "Laura Martinus, Kolawole Tajudeen, Kevin Degila,\n",
      "Kelechi Ogueji, Kathleen Siminyu, Julia Kreutzer,\n",
      "Jason Webster, Jamiil Toure Ali, Jade Abbott,\n",
      "Iroro Orife, Ignatius Ezeani, Idris Abdulkadir\n",
      "Dangana, Herman Kamper, Hady Elsahar, Good-\n",
      "ness Duru, Ghollah Kioko, Murhabazi Espoir,\n",
      "Elan van Biljon, Daniel Whitenack, Christopher\n",
      "Onyefuluchi, Chris Chinenye Emezue, Bonaventure\n",
      "F. P. Dossou, Blessing Sibanda, Blessing Bassey,\n",
      "Ayodele Olabiyi, Arshath Ramkilowan, Alp ktem,\n",
      "Adewale Akinfaderin, and Abdallah Bashir. 2020.\n",
      "Participatory research for low-resourced machine\n",
      "translation: A case study in African languages.\n",
      "InFindings of the Association for Computational\n",
      "Linguistics: EMNLP 2020 , pages 21442160,\n",
      "Online. Association for Computational Linguistics.\n",
      "Graham Neubig. 2017. Neural machine translation\n",
      "and sequence-to-sequence models: A tutorial. arXiv\n",
      "preprint arXiv:1703.01619 .\n",
      "Graham Neubig and Junjie Hu. 2018. Rapid adapta-\n",
      "tion of neural machine translation to new languages.\n",
      "InProceedings of the 2018 Conference on Empiri-\n",
      "cal Methods in Natural Language Processing , pages\n",
      "875880.Tan Ngoc Le and Fatiha Sadat. 2020. Revitalization\n",
      "of indigenous languages through pre-processing and\n",
      "neural machine translation: The case of Inuktitut.\n",
      "InProceedings of the 28th International Conference\n",
      "on Computational Linguistics , pages 46614666,\n",
      "Barcelona, Spain (Online). International Committee\n",
      "on Computational Linguistics.\n",
      "Dat Quoc Nguyen, Kairit Sirts, and Mark Johnson.\n",
      "2015. Improving topic coherence with latent fea-\n",
      "ture word representations in MAP estimation for\n",
      "topic modeling. In Proceedings of the Australasian\n",
      "Language Technology Association Workshop 2015 ,\n",
      "pages 116121, Parramatta, Australia.\n",
      "Toan Q Nguyen and David Chiang. 2017. Trans-\n",
      "fer learning across low-resource, related languages\n",
      "for neural machine translation. In Proceedings of\n",
      "the Eighth International Joint Conference on Natu-\n",
      "ral Language Processing (Volume 2: Short Papers) ,\n",
      "pages 296301.\n",
      "Eugene Nida. 1945. Linguistics and ethnology in\n",
      "translation-problems. Word , 1(2):194208.\n",
      "Jan Niehues and Eunah Cho. 2017. Exploiting linguis-\n",
      "tic resources for neural machine translation using\n",
      "multi-task learning. In Proceedings of the Second\n",
      "Conference on Machine Translation , pages 8089,\n",
      "Copenhagen, Denmark. Association for Computa-\n",
      "tional Linguistics.\n",
      "Jan Niehues, Eunah Cho, Thanh-Le Ha, and Alex\n",
      "Waibel. 2016. Pre-translation for neural machine\n",
      "translation. In Proceedings of COLING 2016, the\n",
      "26th International Conference on Computational\n",
      "Linguistics: Technical Papers , pages 18281836,\n",
      "Osaka, Japan. The COLING 2016 Organizing Com-\n",
      "mittee.\n",
      "Farhad Nooralahzadeh, Giannis Bekoulis, Johannes\n",
      "Bjerva, and Isabelle Augenstein. 2020. Zero-shot\n",
      "cross-lingual transfer with meta learning. arXiv\n",
      "preprint arXiv:2003.02739 .\n",
      "Atul Kr. Ojha, Valentin Malykh, Alina Karakanta, and\n",
      "Chao-Hong Liu. 2020. Findings of the LoResMT\n",
      "2020 shared task on zero-shot for low-resource lan-\n",
      "guages. In Proceedings of the 3rd Workshop on\n",
      "Technologies for MT of Low Resource Languages ,\n",
      "pages 3337, Suzhou, China. Association for Com-\n",
      "putational Linguistics.\n",
      "Matthew Olson, Abraham Wyner, and Richard Berk.\n",
      "2018. Modern neural networks generalize on small\n",
      "data sets. In Advances in Neural Information Pro-\n",
      "cessing Systems , pages 36193628.\n",
      "Arturo Oncevay. 2021. Peru is multilingual, its ma-\n",
      "chine translation should be too? In Proceedings\n",
      "of the First Workshop on Natural Language Pro-\n",
      "cessing for Indigenous Languages of the Americas ,\n",
      "pages 194201, Online. Association for Computa-\n",
      "tional Linguistics.Iroro Orife, Julia Kreutzer, Blessing Sibanda, Daniel\n",
      "Whitenack, Kathleen Siminyu, Laura Martinus,\n",
      "Jamiil Toure Ali, Jade Abbott, Vukosi Marivate,\n",
      "Salomon Kabongo, et al. 2020. Masakhane\n",
      "machine translation for africa. arXiv preprint\n",
      "arXiv:2003.11529 .\n",
      "John E Ortega, Richard Castro Mamani, and\n",
      "Kyunghyun Cho. 2020a. Neural machine translation\n",
      "with a polysynthetic low resource language. Ma-\n",
      "chine Translation , 34(4):325346.\n",
      "John E Ortega, Richard Alexander Castro-Mamani, and\n",
      "Jaime Rafael Montoya Samame. 2020b. Overcom-\n",
      "ing resistance: The normalization of an Amazonian\n",
      "tribal language. In Proceedings of the 3rd Work-\n",
      "shop on Technologies for MT of Low Resource Lan-\n",
      "guages , pages 113, Suzhou, China. Association for\n",
      "Computational Linguistics.\n",
      "Yirong Pan, Xiao Li, Yating Yang, and Rui Dong. 2020.\n",
      "Multi-task neural model for agglutinative language\n",
      "translation. In Proceedings of the 58th Annual Meet-\n",
      "ing of the Association for Computational Linguis-\n",
      "tics: Student Research Workshop , pages 103110,\n",
      "Online. Association for Computational Linguistics.\n",
      "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-\n",
      "Jing Zhu. 2002. Bleu: a method for automatic eval-\n",
      "uation of machine translation. In Proceedings of the\n",
      "40th Annual Meeting of the Association for Com-\n",
      "putational Linguistics , pages 311318, Philadelphia,\n",
      "Pennsylvania, USA. Association for Computational\n",
      "Linguistics.\n",
      "Shantipriya Parida, Subhadarshi Panda, Amulya Dash,\n",
      "Esau Villatoro-Tello, A. Seza Do gruz, Rosa M.\n",
      "Ortega-Mendoza, Amadeo Hernndez, Yashvardhan\n",
      "Sharma, and Petr Motlicek. 2021. Open machine\n",
      "translation for low resource South American lan-\n",
      "guages (AmericasNLP 2021 shared task contribu-\n",
      "tion). In Proceedings of the First Workshop on Natu-\n",
      "ral Language Processing for Indigenous Languages\n",
      "of the Americas , pages 218223, Online. Associa-\n",
      "tion for Computational Linguistics.\n",
      "Jeffrey Pennington, Richard Socher, and Christopher\n",
      "Manning. 2014. GloVe: Global vectors for word\n",
      "representation. In Proceedings of the 2014 Con-\n",
      "ference on Empirical Methods in Natural Language\n",
      "Processing (EMNLP) , pages 15321543, Doha,\n",
      "Qatar. Association for Computational Linguistics.\n",
      "Asya Pereltsvaig. 2020. Languages of the World .\n",
      "Cambridge University Press.\n",
      "Matthew Peters, Mark Neumann, Mohit Iyyer, Matt\n",
      "Gardner, Christopher Clark, Kenton Lee, and Luke\n",
      "Zettlemoyer. 2018. Deep contextualized word rep-\n",
      "resentations. In Proceedings of the 2018 Confer-\n",
      "ence of the North American Chapter of the Associ-\n",
      "ation for Computational Linguistics: Human Lan-\n",
      "guage Technologies, Volume 1 (Long Papers) , pages\n",
      "22272237.Alberto Poncelas, Maja Popovi c, Dimitar Shterionov,\n",
      "Gideon Maillette de Buy Wenniger, and Andy Way.\n",
      "2019. Combining pbsmt and nmt back-translated\n",
      "data for efficient nmt. In Proceedings of the Inter-\n",
      "national Conference on Recent Advances in Natural\n",
      "Language Processing (RANLP 2019) , pages 922\n",
      "931.\n",
      "Maja Popovi c. 2017. chrf++: words helping character\n",
      "n-grams. In Proceedings of the second conference\n",
      "on machine translation , pages 612618.\n",
      "Nima Pourdamghani and Kevin Knight. 2019. Neigh-\n",
      "bors helping the poor: improving low-resource ma-\n",
      "chine translation using related languages. Machine\n",
      "Translation , 33(3):239258.\n",
      "Ofir Press and Lior Wolf. 2017. Using the output em-\n",
      "bedding to improve language models. In Proceed-\n",
      "ings of the 15th Conference of the European Chap-\n",
      "ter of the Association for Computational Linguistics:\n",
      "Volume 2, Short Papers , pages 157163.\n",
      "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\n",
      "Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\n",
      "Wei Li, and Peter J Liu. 2020. Exploring the limits\n",
      "of transfer learning with a unified text-to-text trans-\n",
      "former. Journal of Machine Learning Research ,\n",
      "21:167.\n",
      "Alessandro Raganato, Ral Vzquez, Mathias Creutz,\n",
      "and Jrg Tiedemann. 2021. An empirical investi-\n",
      "gation of word alignment supervision for zero-shot\n",
      "multilingual neural machine translation. In Pro-\n",
      "ceedings of the 2021 Conference on Empirical Meth-\n",
      "ods in Natural Language Processing , pages 8449\n",
      "8456, Online and Punta Cana, Dominican Republic.\n",
      "Association for Computational Linguistics.\n",
      "Surangika Ranathunga, En-Shiun Annie Lee, Mar-\n",
      "jana Prifti Skenduli, Ravi Shekhar, Mehreen Alam,\n",
      "and Rishemjit Kaur. 2021. Neural machine trans-\n",
      "lation for low-resource languages: A survey. arXiv\n",
      "preprint arXiv:2106.15115 .\n",
      "Shuo Ren, Yu Wu, Shujie Liu, Ming Zhou, and\n",
      "Shuai Ma. 2020. A retrieve-and-rewrite initializa-\n",
      "tion method for unsupervised machine translation.\n",
      "InProceedings of the 58th Annual Meeting of the\n",
      "Association for Computational Linguistics , pages\n",
      "34983504, Online. Association for Computational\n",
      "Linguistics.\n",
      "Parker Riley, Isaac Caswell, Markus Freitag, and David\n",
      "Grangier. 2020. Translationese as a language in\n",
      "multilingual NMT. In Proceedings of the 58th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics , pages 77377746, Online. Association\n",
      "for Computational Linguistics.\n",
      "Christian Roest, Lukas Edman, Gosse Minnema, Kevin\n",
      "Kelly, Jennifer Spenader, and Antonio Toral. 2020.\n",
      "Machine translation for EnglishInuktitut with seg-\n",
      "mentation, data acquisition and pre-training. In\n",
      "Proceedings of the Fifth Conference on MachineTranslation , pages 274281, Online. Association for\n",
      "Computational Linguistics.\n",
      "David Rolnick, Andreas Veit, Serge Belongie, and Nir\n",
      "Shavit. 2017. Deep learning is robust to massive la-\n",
      "bel noise. arXiv preprint arXiv:1705.10694 .\n",
      "Carlos Barron Romero, Jess Manuel Mager Hois, and\n",
      "Fernando Reyes Avils. 2016. Richard feynman, los\n",
      "alfabetos y los lenguajes. Relingstica aplicada ,\n",
      "(19):2.\n",
      "Mnica Jasso Rosales, Manuel Mager, and Ivan\n",
      "Vladimir Meza Ruz. Towards a twitter corpus of\n",
      "the indigenous languages of the americas.\n",
      "Devendra Singh Sachan and Graham Neubig. 2018.\n",
      "Parameter sharing methods for multilingual self-\n",
      "attentional translation models. arXiv preprint\n",
      "arXiv:1809.00252 .\n",
      "Elizabeth Salesky, David Etter, and Matt Post. 2021.\n",
      "Robust open-vocabulary translation from visual text\n",
      "representations. arXiv preprint arXiv:2104.08211 .\n",
      "Jonne Saleva and Constantine Lignos. 2021. The effec-\n",
      "tiveness of morphology-aware segmentation in low-\n",
      "resource neural machine translation. In Proceedings\n",
      "of the 16th Conference of the European Chapter of\n",
      "the Association for Computational Linguistics: Stu-\n",
      "dent Research Workshop , pages 164174, Online.\n",
      "Association for Computational Linguistics.\n",
      "Motoki Sano, Jun Suzuki, and Shun Kiyono. 2019. Ef-\n",
      "fective adversarial regularization for neural machine\n",
      "translation. In Proceedings of the 57th Annual Meet-\n",
      "ing of the Association for Computational Linguis-\n",
      "tics, pages 204210.\n",
      "Yves Scherrer, Stig-Arne Grnroos, and Sami Virpi-\n",
      "oja. 2020. The University of Helsinki and aalto\n",
      "university submissions to the WMT 2020 news and\n",
      "low-resource translation tasks. In Proceedings of\n",
      "the Fifth Conference on Machine Translation , pages\n",
      "11291138, Online. Association for Computational\n",
      "Linguistics.\n",
      "Lane Schwartz. 2022. Primum non nocere: Before\n",
      "working with indigenous data, the acl must confront\n",
      "ongoing colonialism. In Proceedings of the 60th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics (Volume 2: Short Papers) , pages 724\n",
      "731.\n",
      "Lane Schwartz, Francis Tyers, Lori Levin, Christo\n",
      "Kirov, Patrick Littell, Chi-kiu Lo, Emily\n",
      "Prudhommeaux, Hyunji Hayley Park, Ken-\n",
      "neth Steimel, Rebecca Knowles, et al. 2020. Neural\n",
      "polysynthetic language modelling. arXiv preprint\n",
      "arXiv:2005.05477 .\n",
      "Lee Sechrest, Todd L Fay, and SM Hafeez Zaidi. 1972.\n",
      "Problems of translation in cross-cultural research.\n",
      "Journal of cross-cultural psychology , 3(1):4156.Rico Sennrich, Barry Haddow, and Alexandra Birch.\n",
      "2016a. Improving neural machine translation mod-\n",
      "els with monolingual data. In Proceedings of the\n",
      "54th Annual Meeting of the Association for Compu-\n",
      "tational Linguistics (Volume 1: Long Papers) , pages\n",
      "8696, Berlin, Germany. Association for Computa-\n",
      "tional Linguistics.\n",
      "Rico Sennrich, Barry Haddow, and Alexandra Birch.\n",
      "2016b. Improving neural machine translation mod-\n",
      "els with monolingual data. In Proceedings of the\n",
      "54th Annual Meeting of the Association for Compu-\n",
      "tational Linguistics (Volume 1: Long Papers) , pages\n",
      "8696.\n",
      "Rico Sennrich, Barry Haddow, and Alexandra Birch.\n",
      "2016c. Neural machine translation of rare words\n",
      "with subword units. In Proceedings of the 54th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics (Volume 1: Long Papers) , pages 1715\n",
      "1725, Berlin, Germany. Association for Computa-\n",
      "tional Linguistics.\n",
      "Joan Serra, Didac Suris, Marius Miron, and Alexan-\n",
      "dros Karatzoglou. 2018. Overcoming catastrophic\n",
      "forgetting with hard attention to the task. In In-\n",
      "ternational Conference on Machine Learning , pages\n",
      "45484557. PMLR.\n",
      "Aditya Siddhant, Ankur Bapna, Yuan Cao, Orhan Fi-\n",
      "rat, Mia Chen, Sneha Kudugunta, Naveen Arivazha-\n",
      "gan, and Yonghui Wu. 2020. Leveraging mono-\n",
      "lingual data with self-supervision for multilingual\n",
      "neural machine translation. In Proceedings of the\n",
      "58th Annual Meeting of the Association for Compu-\n",
      "tational Linguistics , pages 28272835, Online. As-\n",
      "sociation for Computational Linguistics.\n",
      "Gerardo Sierra Martnez, Cynthia Montao, Gemma\n",
      "Bel-Enguix, Diego Crdova, and Margarita\n",
      "Mota Montoya. 2020. CPLM, a parallel corpus for\n",
      "Mexican languages: Development and interface.\n",
      "InProceedings of the 12th Language Resources\n",
      "and Evaluation Conference , pages 29472952,\n",
      "Marseille, France. European Language Resources\n",
      "Association.\n",
      "Gary F Simons and M Paul Lewis. 2013. The worlds\n",
      "languages in crisis. Responses to language endan-\n",
      "germent: In honor of Mickey Noonan. New direc-\n",
      "tions in language documentation and language revi-\n",
      "talization , 3:20.\n",
      "Peter Smit, Sami Virpioja, Stig-Arne Grnroos, Mikko\n",
      "Kurimo, et al. 2014. Morfessor 2.0: Toolkit for sta-\n",
      "tistical morphological segmentation. In The 14th\n",
      "Conference of the European Chapter of the Associa-\n",
      "tion for Computational Linguistics (EACL), Gothen-\n",
      "burg, Sweden, April 26-30, 2014 . Aalto University.\n",
      "Anders Sgaard, Sebastian Ruder, and Ivan Vuli c.\n",
      "2018. On the limitations of unsupervised bilingual\n",
      "dictionary induction. In Proceedings of the 56th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics (Volume 1: Long Papers) , pages 778\n",
      "788.Haiyue Song, Raj Dabre, Zhuoyuan Mao, Fei Cheng,\n",
      "Sadao Kurohashi, and Eiichiro Sumita. 2020. Pre-\n",
      "training via leveraging assisting languages and data\n",
      "selection for neural machine translation. arXiv\n",
      "preprint arXiv:2001.08353 .\n",
      "Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\n",
      "Yan Liu. 2019. Mass: Masked sequence to se-\n",
      "quence pre-training for language generation. In In-\n",
      "ternational Conference on Machine Learning , pages\n",
      "59265936.\n",
      "Xabier Soto, Dimitar Shterionov, Alberto Poncelas,\n",
      "and Andy Way. 2020. Selecting backtranslated data\n",
      "from multiple sources for improved neural machine\n",
      "translation. In Proceedings of the 58th Annual\n",
      "Meeting of the Association for Computational Lin-\n",
      "guistics , pages 38983908, Online. Association for\n",
      "Computational Linguistics.\n",
      "Tejas Srinivasan, Ramon Sanabria, and Florian Metze.\n",
      "2019. Multitask learning for different subword seg-\n",
      "mentations in neural machine translation. arXiv\n",
      "preprint arXiv:1910.12368 .\n",
      "Dario Stojanovski, Viktor Hangya, Matthias Huck, and\n",
      "Alexander Fraser. 2019. The lmu munich unsuper-\n",
      "vised machine translation system for wmt19. In\n",
      "Proceedings of the Fourth Conference on Machine\n",
      "Translation (Volume 2: Shared Task Papers, Day 1) ,\n",
      "pages 393399.\n",
      "Haipeng Sun, Rui Wang, Kehai Chen, Masao Utiyama,\n",
      "Eiichiro Sumita, and Tiejun Zhao. 2020. Robust un-\n",
      "supervised neural machine translation with adversar-\n",
      "ial training. arXiv preprint arXiv:2002.12549 .\n",
      "Xu Tan, Yichong Leng, Jiale Chen, Yi Ren, Tao\n",
      "Qin, and Tie-Yan Liu. 2019. A study of multi-\n",
      "lingual neural machine translation. arXiv preprint\n",
      "arXiv:1912.11625 .\n",
      "Sarah G Thomason. 2015. Endangered languages .\n",
      "Cambridge University Press.\n",
      "Jrg Tiedemann. 2016. Opusparallel corpora for ev-\n",
      "eryone. Baltic Journal of Modern Computing , page\n",
      "384.\n",
      "Jrg Tiedemann. 2018. Emerging language spaces\n",
      "learned from massively multilingual corpora. arXiv\n",
      "preprint arXiv:1802.00273 .\n",
      "Atnafu Lambebo Tonja, Christian Maldonado-\n",
      "Sifuentes, David Alejandro Mendoza Castillo, Olga\n",
      "Kolesnikova, No Castro-Snchez, Grigori Sidorov,\n",
      "and Alexander Gelbukh. 2023. Parallel corpus\n",
      "for indigenous language translation: Spanish-\n",
      "mazatec and spanish-mixtec. arXiv preprint\n",
      "arXiv:2305.17404 .\n",
      "Antonio Toral, Sheila Castilho, Ke Hu, and Andy\n",
      "Way. 2018. Attaining the unattainable? reassess-\n",
      "ing claims of human parity in neural machine trans-\n",
      "lation. In Proceedings of the Third Conference on\n",
      "Machine Translation: Research Papers , pages 113\n",
      "123.Hai-Long Trieu, Duc-Vu Tran, Ashwin Ittoo, and Le-\n",
      "Minh Nguyen. 2019. Leveraging additional re-\n",
      "sources for improving statistical machine translation\n",
      "on asian low-resource languages. ACM Trans. Asian\n",
      "Low-Resour. Lang. Inf. Process. , 18(3).\n",
      "Masao Utiyama and Hitoshi Isahara. 2007. A com-\n",
      "parison of pivot methods for phrase-based statistical\n",
      "machine translation. In Human Language Technolo-\n",
      "gies 2007: The Conference of the North American\n",
      "Chapter of the Association for Computational Lin-\n",
      "guistics; Proceedings of the Main Conference , pages\n",
      "484491.\n",
      "Clara Vania and Adam Lopez. 2017. From characters\n",
      "to words to in between: Do we capture morphol-\n",
      "ogy? In Proceedings of the 55th Annual Meeting of\n",
      "the Association for Computational Linguistics (Vol-\n",
      "ume 1: Long Papers) , pages 20162027, Vancouver,\n",
      "Canada. Association for Computational Linguistics.\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\n",
      "Uszkoreit, Llion Jones, Aidan N Gomez, ukasz\n",
      "Kaiser, and Illia Polosukhin. 2017. Attention is all\n",
      "you need. In Advances in neural information pro-\n",
      "cessing systems , pages 59986008.\n",
      "Ral Vzquez, Yves Scherrer, Sami Virpioja, and Jrg\n",
      "Tiedemann. 2021. The Helsinki submission to the\n",
      "AmericasNLP shared task. In Proceedings of the\n",
      "First Workshop on Natural Language Processing for\n",
      "Indigenous Languages of the Americas , pages 255\n",
      "264, Online. Association for Computational Lin-\n",
      "guistics.\n",
      "Ivan Vuli c, Goran Glava, Roi Reichart, and Anna Ko-\n",
      "rhonen. 2019. Do we really need fully unsuper-\n",
      "vised cross-lingual embeddings? In Proceedings of\n",
      "the 2019 Conference on Empirical Methods in Nat-\n",
      "ural Language Processing and the 9th International\n",
      "Joint Conference on Natural Language Processing\n",
      "(EMNLP-IJCNLP) , pages 44074418, Hong Kong,\n",
      "China. Association for Computational Linguistics.\n",
      "Ivan Vuli c, Sebastian Ruder, and Anders Sgaard.\n",
      "2020. Are all good word vector spaces isomorphic?\n",
      "arXiv preprint arXiv:2004.04070 .\n",
      "Liang Wang, Wei Zhao, Ruoyu Jia, Sujian Li, and\n",
      "Jingming Liu. 2019a. Denoising based sequence-\n",
      "to-sequence pre-training for text generation. In Pro-\n",
      "ceedings of the 2019 Conference on Empirical Meth-\n",
      "ods in Natural Language Processing and the 9th In-\n",
      "ternational Joint Conference on Natural Language\n",
      "Processing (EMNLP-IJCNLP) , pages 39944006.\n",
      "Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu,\n",
      "Changliang Li, Derek F. Wong, and Lidia S. Chao.\n",
      "2019b. Learning deep transformer models for ma-\n",
      "chine translation. In Proceedings of the 57th Annual\n",
      "Meeting of the Association for Computational Lin-\n",
      "guistics , pages 18101822, Florence, Italy. Associa-\n",
      "tion for Computational Linguistics.Rui Wang, Xu Tan, Renqian Luo, Tao Qin, and Tie-\n",
      "Yan Liu. 2021. A survey on low-resource neural\n",
      "machine translation. In Proceedings of the Thirtieth\n",
      "International Joint Conference on Artificial Intel-\n",
      "ligence, IJCAI-21 , pages 46364643. International\n",
      "Joint Conferences on Artificial Intelligence Organi-\n",
      "zation. Survey Track.\n",
      "Xinyi Wang, Hieu Pham, Philip Arthur, and Gra-\n",
      "ham Neubig. 2019c. Multilingual neural machine\n",
      "translation with soft decoupled encoding. In Inter-\n",
      "national Conference on Learning Representations\n",
      "(ICLR) , New Orleans, LA, USA.\n",
      "Xinyi Wang, Yulia Tsvetkov, and Graham Neubig.\n",
      "2020. Balancing training for multilingual neural\n",
      "machine translation. In Proceedings of the 58th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics , pages 85268537, Online. Association\n",
      "for Computational Linguistics.\n",
      "Karl Weiss, Taghi M Khoshgoftaar, and DingDing\n",
      "Wang. 2016. A survey of transfer learning. Jour-\n",
      "nal of Big Data , 3(1):9.\n",
      "John Wieting, Taylor Berg-Kirkpatrick, Kevin Gimpel,\n",
      "and Graham Neubig. 2019. Beyond bleu: Train-\n",
      "ing neural machine translation with semantic sim-\n",
      "ilarity. In Proceedings of the 57th Annual Meet-\n",
      "ing of the Association for Computational Linguis-\n",
      "tics, pages 43444355.\n",
      "Hua Wu and Haifeng Wang. 2007. Pivot language ap-\n",
      "proach for phrase-based statistical machine transla-\n",
      "tion. Machine Translation , 21(3):165181.\n",
      "Xing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han,\n",
      "and Songlin Hu. 2019. Conditional bert contextual\n",
      "augmentation. In International Conference on Com-\n",
      "putational Science , pages 8495. Springer.\n",
      "Haoran Xu, Benjamin Van Durme, and Kenton Murray.\n",
      "2021. BERT, mBERT, or BiBERT? a study on con-\n",
      "textualized embeddings for neural machine transla-\n",
      "tion. In Proceedings of the 2021 Conference on Em-\n",
      "pirical Methods in Natural Language Processing ,\n",
      "pages 66636675, Online and Punta Cana, Domini-\n",
      "can Republic. Association for Computational Lin-\n",
      "guistics.\n",
      "Linting Xue, Noah Constant, Adam Roberts, Mi-\n",
      "hir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\n",
      "Barua, and Colin Raffel. 2021. mt5: A massively\n",
      "multilingual pre-trained text-to-text transformer. In\n",
      "Proceedings of the 2021 Conference of the North\n",
      "American Chapter of the Association for Computa-\n",
      "tional Linguistics: Human Language Technologies ,\n",
      "pages 483498.\n",
      "Delfino Zacaras Mrquez and Ivan Vladimir\n",
      "Meza Ruiz. 2021. Ayuuk-Spanish neural ma-\n",
      "chine translator. In Proceedings of the First\n",
      "Workshop on Natural Language Processing for\n",
      "Indigenous Languages of the Americas , pages\n",
      "168172, Online. Association for Computational\n",
      "Linguistics.Lenka Zajcov. 2017. Lenguas indgenas en\n",
      "la legislacin de los pases hispanoamericanos.\n",
      "Onomzein , (NE III):171203.\n",
      "Poorya Zaremoodi, Wray Buntine, and Gholamreza\n",
      "Haffari. 2018. Adaptive knowledge sharing in\n",
      "multi-task learning: Improving low-resource neural\n",
      "machine translation. In Proceedings of the 56th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics (Volume 2: Short Papers) , pages 656\n",
      "661.\n",
      "Rodolfo Zevallos, John Ortega, William Chen, Richard\n",
      "Castro, Nria Bel, Cesar Toshio, Renzo Venturas,\n",
      "Aradiel, and Hilario Nelsi Melgarejo. 2022. Intro-\n",
      "ducing QuBERT: A large monolingual corpus and\n",
      "BERT model for Southern Quechua. In Proceed-\n",
      "ings of the Third Workshop on Deep Learning for\n",
      "Low-Resource Natural Language Processing , pages\n",
      "113, Hybrid. Association for Computational Lin-\n",
      "guistics.\n",
      "Biao Zhang, Philip Williams, Ivan Titov, and Rico\n",
      "Sennrich. 2020a. Improving massively multilingual\n",
      "neural machine translation and zero-shot translation.\n",
      "arXiv preprint arXiv:2004.11867 .\n",
      "Biao Zhang, Philip Williams, Ivan Titov, and Rico\n",
      "Sennrich. 2020b. Improving massively multilingual\n",
      "neural machine translation and zero-shot translation.\n",
      "InProceedings of the 58th Annual Meeting of the\n",
      "Association for Computational Linguistics , pages\n",
      "16281639, Online. Association for Computational\n",
      "Linguistics.\n",
      "Shiyue Zhang, Ben Frey, and Mohit Bansal. 2022.\n",
      "How can nlp help revitalize endangered languages?\n",
      "a case study and roadmap for the cherokee language.\n",
      "arXiv preprint arXiv:2204.11909 .\n",
      "Shiyue Zhang, Benjamin Frey, and Mohit Bansal.\n",
      "2020c. ChrEn: Cherokee-English machine transla-\n",
      "tion for endangered language revitalization. In Pro-\n",
      "ceedings of the 2020 Conference on Empirical Meth-\n",
      "ods in Natural Language Processing (EMNLP) ,\n",
      "pages 577595, Online. Association for Computa-\n",
      "tional Linguistics.\n",
      "Shiyue Zhang, Benjamin Frey, and Mohit Bansal.\n",
      "2021. ChrEnTranslate: Cherokee-English machine\n",
      "translation demo with quality estimation and correc-\n",
      "tive feedback. In Proceedings of the 59th Annual\n",
      "Meeting of the Association for Computational Lin-\n",
      "guistics and the 11th International Joint Conference\n",
      "on Natural Language Processing: System Demon-\n",
      "strations , pages 272279, Online. Association for\n",
      "Computational Linguistics.\n",
      "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\n",
      "Weinberger, and Yoav Artzi. 2020d. Bertscore:\n",
      "Evaluating text generation with bert. In ICLR .\n",
      "Yuhao Zhang, Ziyang Wang, Runzhe Cao, Binghao\n",
      "Wei, Weiqiao Shan, Shuhan Zhou, Abudurexiti Re-\n",
      "heman, Tao Zhou, Xin Zeng, Laohu Wang, YongyuMu, Jingnan Zhang, Xiaoqian Liu, Xuanjun Zhou,\n",
      "Yinqiao Li, Bei Li, Tong Xiao, and Jingbo Zhu.\n",
      "2020e. The NiuTrans machine translation systems\n",
      "for WMT20. In Proceedings of the Fifth Confer-\n",
      "ence on Machine Translation , pages 338345, On-\n",
      "line. Association for Computational Linguistics.\n",
      "Zhuosheng Zhang, Yafang Huang, and Hai Zhao. 2019.\n",
      "Open vocabulary learning for neural chinese pinyin\n",
      "ime. In Proceedings of the 57th Annual Meeting\n",
      "of the Association for Computational Linguistics ,\n",
      "pages 15841594.\n",
      "Francis Zheng, Machel Reid, Edison Marrese-Taylor,\n",
      "and Yutaka Matsuo. 2021. Low-resource machine\n",
      "translation using cross-lingual language model pre-\n",
      "training. In Proceedings of the First Workshop on\n",
      "Natural Language Processing for Indigenous Lan-\n",
      "guages of the Americas , pages 234240, Online. As-\n",
      "sociation for Computational Linguistics.\n",
      "Hao Zheng, Yong Cheng, and Yang Liu. 2017.\n",
      "Maximum expected likelihood estimation for zero-\n",
      "resource neural machine translation. In IJCAI ,\n",
      "pages 42514257.\n",
      "Shuyan Zhou, Xiangkai Zeng, Yingqi Zhou, Antonios\n",
      "Anastasopoulos, and Graham Neubig. 2019. Im-\n",
      "proving robustness of neural machine translation\n",
      "with multi-task learning. In Proceedings of the\n",
      "Fourth Conference on Machine Translation (Volume\n",
      "2: Shared Task Papers, Day 1) , pages 565571, Flo-\n",
      "rence, Italy. Association for Computational Linguis-\n",
      "tics.\n",
      "Changfeng Zhu, Heng Yu, Shanbo Cheng, and Weihua\n",
      "Luo. 2020a. Language-aware interlingua for multi-\n",
      "lingual neural machine translation. In Proceedings\n",
      "of the 58th Annual Meeting of the Association for\n",
      "Computational Linguistics , pages 16501655, On-\n",
      "line. Association for Computational Linguistics.\n",
      "Jinhua Zhu, Fei Gao, Lijun Wu, Yingce Xia, Tao\n",
      "Qin, Wengang Zhou, Xueqi Cheng, and Tie-Yan\n",
      "Liu. 2019. Soft contextual data augmentation\n",
      "for neural machine translation. arXiv preprint\n",
      "arXiv:1905.10523 .\n",
      "Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,\n",
      "Wengang Zhou, Houqiang Li, and Tie-Yan Liu.\n",
      "2020b. Incorporating bert into neural machine trans-\n",
      "lation. arXiv preprint arXiv:2002.06823 .\n",
      "Barret Zoph, Deniz Yuret, Jonathan May, and\n",
      "Kevin Knight. 2016. Transfer learning for low-\n",
      "resource neural machine translation. arXiv preprint\n",
      "arXiv:1604.02201 .A Appendix\n",
      "In this appendix we expand the information re-\n",
      "garding current work on MT for LRL.\n",
      "A.1 Expanded LR work on Multilingual\n",
      "supervised training\n",
      "Arivazhagan et al. (2019a) introduce a represen-\n",
      "tational invariance training objective across lan-\n",
      "guages that achieves comparable results with piv-\n",
      "oting methods. Promising results of multilingual\n",
      "models have encouraged experiments with models\n",
      "trained on a massive amount of language pairs, re-\n",
      "sulting in large multilingual models: Aharoni et al.\n",
      "(2019) train a single model on 102 languages to\n",
      "and from English in contrast to the 58 languages\n",
      "used by Neubig and Hu (2018).\n",
      "The negative aspect of this approach is the size\n",
      "of the network. Arivazhagan et al. (2019b) per-\n",
      "form an extensive study on 102 language pairs\n",
      "to explore different settings and training setups\n",
      "and achieve good results for LRLs, while main-\n",
      "taining good performance for high-resource lan-\n",
      "guages. Related massively multilingual NMT\n",
      "systems have been trained for analytic proposes\n",
      "(Tiedemann, 2018; Malaviya et al., 2017) and\n",
      "general zero-shot transfer learning (Artetxe and\n",
      "Schwenk, 2019). mRASP (Lin et al., 2020) use\n",
      "for pretraining of the multilingual model and add\n",
      "a randomly aligned substitution loss that aims to\n",
      "bring words and phrases closer in the cross-lingual\n",
      "space.\n",
      "Zhang et al. (2020a) explores the main problems\n",
      "that arise for such models: multilingual NMT usu-\n",
      "ally underperforms bilingual models (Arivazha-\n",
      "gan et al., 2019b), the larger the number of lan-\n",
      "guages gets the more the performance drops (Aha-\n",
      "roni et al., 2019), languages in datasets used for\n",
      "multilingual training are unbalanced in size, and\n",
      "poor zero-shot performance compared to pivot\n",
      "models (cf. 6.3). Zhang et al. (2020a) ad-\n",
      "dresses these problems with a language-aware in-\n",
      "put layer, a deep transformer architecture (Wang\n",
      "et al., 2019b), and an online back-translation\n",
      "approach. These modifications boost zero-shot\n",
      "translation performance for multilingual models.\n",
      "To improve the problem of imbalanced and lin-\n",
      "guistically diverse training data, mostly heuristic\n",
      "methods have been proposed: Arivazhagan et al.\n",
      "(2019b) samples training data from different lan-\n",
      "guages based on a data size scaled by temperature\n",
      "term. These heuristics have an impact on perfor-mance, and ignore other factors that are not size.\n",
      "Oversampling of data is used by Johnson et al.\n",
      "(2017); Neubig and Hu (2018); Conneau and Lam-\n",
      "ple (2019). Wang et al. (2020) proposes a differ-\n",
      "entiable data selection method that automatically\n",
      "learns to weight training data, optimizing transla-\n",
      "tion on all languages.\n",
      "Multilingual modeling Sharing all parameters\n",
      "except for the attention mechanism shows im-\n",
      "provements compared with sharing everything in\n",
      "an RNN NMT model (Blackwood et al., 2018).\n",
      "Sachan and Neubig (2018) explores parameter\n",
      "sharing in the transformer architecture for the de-\n",
      "coder in the one-to-many translation setting and\n",
      "shows that transformers are more suitable than\n",
      "RNNs for this task. Also, parameter sharing in\n",
      "the decoder and embedding layer further improves\n",
      "performance. Lu et al. (2018) proposes a shared\n",
      "layer intended to capture the interlingua knowl-\n",
      "edge and an extension to the typical RNN network\n",
      "with multiple blocks along with a trainable routing\n",
      "network. The routing network enables adaptive\n",
      "collaboration by dynamic sharing of blocks condi-\n",
      "tioned on the task at hand, input, and model state\n",
      "(Zaremoodi et al., 2018). Zhang et al. (2020a) pro-\n",
      "poses a language-aware layer to improve such ar-\n",
      "chitectures further. With a similar idea, Zhu et al.\n",
      "(2020a) incorporates two special language embed-\n",
      "dings into the self-attention mechanism. The first\n",
      "encodes the unique characteristics of each lan-\n",
      "guage, while the second captures common seman-\n",
      "tics across languages.\n",
      "One problem in multilingual NMT systems is\n",
      "the translation into the wrong language. To ad-\n",
      "dress this problem, Zhang et al. (2020b) add\n",
      "a language-aware layer normalization and a lin-\n",
      "ear transformation that is inserted between the\n",
      "encoder and the decoder to induce a language-\n",
      "specific translation. Raganato et al. (2021) explore\n",
      "to weight the target language label with jointly\n",
      "training one cross attention head with word align-\n",
      "ments.\n",
      "Other modifications of NMT model archi-\n",
      "tectures to improve their performance on low-\n",
      "resource languages include: deep RNNs (Miceli-\n",
      "Barone et al., 2017), normalization layers (Ba\n",
      "et al., 2016), direct lexical connections (Nguyen\n",
      "et al., 2015), word embedding layers conducive to\n",
      "lexical sharing (Wang et al., 2019c).A.2 Extended Multi-task training\n",
      "Zhou et al. (2019) uses this approach, but extends\n",
      "it with a cascade architecture: the first decoder\n",
      "reads the encoder, and the second decoder reads\n",
      "the encoder and the first decoder (Niehues et al.,\n",
      "2016; Anastasopoulos and Chiang, 2018). The\n",
      "auxiliary task (first decoder) is a denoising de-\n",
      "coder. With RNN NMT architectures, one can\n",
      "further decide if the attention mechanism should\n",
      "be shared among tasks (Niehues and Cho, 2017).\n",
      "The authors compare all architectures and find that\n",
      "they perform similarly, with only sharing the en-\n",
      "coder being slightly better.\n",
      "Using linguistic information as an auxiliary task\n",
      "has not yet been explored exhaustively. Niehues\n",
      "and Cho (2017) studies the usage of part-of-speech\n",
      "(POS) and named entity (NE) tags, finding that\n",
      "training on named entity recognition (NER), POS\n",
      "tagging and MT together improves performance\n",
      "the most. For agglutinative languages, morpho-\n",
      "logical auxiliary tasks can be beneficial: Pan et al.\n",
      "(2020) uses stemming with fully shared parame-\n",
      "ters.\n",
      "As an alternative to linguistically informed aux-\n",
      "iliary tasks Srinivasan et al. (2019) uses multiple\n",
      "BPE vocabulary sizes to generate different seg-\n",
      "mentations. Each segmentation is treated as an in-\n",
      "dividual task.\n",
      "A.3 Data augmentation\n",
      "Back-translation Caswell et al. (2019) shows\n",
      "that adding a special tag to the synthetic data im-\n",
      "proves performance. A technique that exploits this\n",
      "idea is training an initial translation model with\n",
      "synthetic data generated via BT and then finetune\n",
      "it with gold data (Abdulmumin et al., 2019). This\n",
      "simple yet effective training algorithm improves\n",
      "NMT for LRLs; however, it can also degrade per-\n",
      "formance on HRLs if trained without a tagging\n",
      "strategy (Marie et al., 2020).\n",
      "Multiple improvements of BT have been pro-\n",
      "posed. Edunov et al. (2018) shows that sampling\n",
      "or noisy beam search can generate more effective\n",
      "pseudo-parallel data. However, for LRLs an op-\n",
      "timal beam search and greedy decoding are bet-\n",
      "ter. A factor that influences BTs effectiveness\n",
      "is the quality of the initial MT systems (Hoang\n",
      "et al., 2018). Using back-translated data from mul-\n",
      "tiple sources (Poncelas et al., 2019) or optimizing\n",
      "the ranking of back-translated data yields further\n",
      "gains (Soto et al., 2020).BT results in gains when the parallel corpora are\n",
      "naturally occurring text and not translationese, as\n",
      "the latter would only improve automatic n metrics\n",
      "(Toral et al., 2018; Graham et al., 2020). ?shows\n",
      "that BT produces more fluent text and is preferred\n",
      "by humans. Additionally, translationese and origi-\n",
      "nal data can be modeled as separate languages in a\n",
      "multilingual model (Riley et al., 2020). BT is also\n",
      "a central part of unsupervised MT (UMT; cf. 6.4)\n",
      "and zero-shot MT (Gu et al., 2019).\n",
      "Sentence modification Zhu et al. (2019) pro-\n",
      "poses to replace a randomly chosen word in a sen-\n",
      "tence with a soft-word . That means that, instead\n",
      "of sampling a word from the lexical distribution\n",
      "of a LM like Kobayashi (2018), the authors use\n",
      "the hidden state vector of the LM directly. Wu\n",
      "et al. (2019) substitutes the RNN LMs from pre-\n",
      "vious work and use BERT (Devlin et al., 2019)\n",
      " a transformer trained with a masked language\n",
      "modeling objective  instead. The authors finetune\n",
      "BERT with a conditional masked language mod-\n",
      "eling objective that tries to avoid the prediction of\n",
      "words that do not correspond to the original sen-\n",
      "tence meaning.\n",
      "Another way to augmented MT data is by para-\n",
      "phrasing. If a good paraphrase system exists, this\n",
      "can increase the number of training instances (Hu\n",
      "et al., 2019). Paraphrasing can also be used at\n",
      "training time by sampling paraphrases of the refer-\n",
      "ence sentence from a paraphraser and training the\n",
      "MT model to predict the distribution of the para-\n",
      "phraser (Khayrallah et al., 2020). This helps the\n",
      "model to generalize. Wieting et al. (2019) propose\n",
      "a similar approach, using minimum risk training to\n",
      "optimize BLEU. To avoid BLEUs constraints to a\n",
      "specific reference, they use paraphrasing to diver-\n",
      "sify the given reference.\n",
      "Finally, existing data can be augmented by\n",
      "adding noise. This noise can be continuous or dis-\n",
      "crete. In the case of applying continuous noise,\n",
      "noise vectors are added to the word embeddings\n",
      "(Cheng et al., 2018; Sano et al., 2019). Discrete\n",
      "noise is realized by inserting, deleting, or replac-\n",
      "ing words, BPE tokens, or characters to expand\n",
      "the training set in an adversarial fashion (Belinkov\n",
      "and Bisk, 2018; Ebrahimi et al., 2018; ?; Cheng\n",
      "et al., 2019, 2020).\n",
      "Pivoting While it is simple to implement and\n",
      "effective, pivot-based approaches suffer from er-\n",
      "ror propagation. To overcome that for NMT, jointtraining Zheng et al. (2017); Cheng (2019) and\n",
      "round-trip training (Ahmadnia and Dorr, 2019)\n",
      "have been proposed.\n",
      "Pivoting with NMT systems has been used\n",
      "for translating Japanese, Indonesian, and Malay\n",
      "into Vietnamese (Trieu et al., 2019), translation\n",
      "of related languages (Pourdamghani and Knight,\n",
      "2019), multilingual zero-shot MT (Lakew et al.,\n",
      "2018), and UMT (cf. 6.4) between distant lan-\n",
      "guage pairs (Leng et al., 2019).\n",
      "A.4 Recent low-resource Shared Tasks\n",
      "First, the LoResMT 2020 shared task (Ojha\n",
      "et al., 2020) explores the case of language pairs\n",
      "which have no parallel data between them (Hindi\n",
      "Bhojpuri, HindiMagahi, and RussianHindi).\n",
      "The winning system (Laskar et al., 2020) uses\n",
      "a MASS model in a zero-shot fashion with ad-\n",
      "ditional monolingual data (see 6.4). Second,\n",
      "the WMT 2020 shared tasks on UMT and very\n",
      "low-resource supervised MT (Fraser, 2020) pro-\n",
      "vide text and 60k aligned phrases for German\n",
      "Upper Sorbian., The most important technique in\n",
      "all tracks is transfer learning, achieving surpris-\n",
      "ingly good results. For the AmericasNLP 2021\n",
      "shared task on open MT (Mager et al., 2021), 10\n",
      "indigenous language languages were paired with\n",
      "Spanish, resulting in an extreme low-resource set-\n",
      "ting (4k to 125k paired sentences), with challenges\n",
      "out as domain, dialectical, and orthographic mis-\n",
      "matches between splits and datasets. The best\n",
      "systems shows that data cleaning and collection\n",
      "(??) as well as multilingual approaches (6.1)\n",
      "result in the best performance in this conditions.\n",
      "Finally the shared task on MT in Dravidian lan-\n",
      "guages (Chakravarthi et al., 2021) features 3 lan-\n",
      "guages paired with English as well as Tamil\n",
      "Telugu. Again, the winning system uses a mul-\n",
      "tilingual approach. The best performing systems\n",
      "use BT (6.3) and BPE word segmentation (2.1).\n",
      "The results from these challenges indicate that\n",
      "the optimal selection and combination of meth-\n",
      "ods differs between cases (i.e., amount of mono-\n",
      "lingual, parallel data, cleanness of data, domain\n",
      "mismatch, linguistic closeness of languages). This\n",
      "implies that data analysis and linguistic knowl-\n",
      "edge are needed to improve a final systems per-\n",
      "formance.\n",
      "A.5 Transfer learning\n",
      "This helps low-resource tasks as a lower amount\n",
      "of data can be used for training. One applicationof transfer learning to MT is the usage of a pre-\n",
      "trained RNN LM (Gulcehre et al., 2015) as the de-\n",
      "coder in an NMT system. Zoph et al. (2016) is the\n",
      "first work that uses pretrained models to improve\n",
      "NMT systems. The authors perform two experi-\n",
      "ments with an RNN encoderdecoder architecture\n",
      "with an attention mechanism: the model is first\n",
      "pretrained on a high-resource language pair This\n",
      "works even better if related languages are used\n",
      "during pretraining (Nguyen and Chiang, 2017).\n",
      "Using pretrained LMs at decoding time and as pri-\n",
      "ors at training time also improves vanilla models\n",
      "(Baziotis et al., 2020).\n",
      "To avoid overfitting, models can be finetuned on\n",
      "both a HRLs pair and a LRLs pair in a multi-task\n",
      "fashion (Neubig and Hu, 2018).\n",
      "However, how can we represent best the vocab-\n",
      "ulary? Zoph et al. (2016) use separate embeddings\n",
      "for the source and the target language. However,\n",
      "using tied embeddings has been shown to yield\n",
      "better results (Press and Wolf, 2017). Edunov et al.\n",
      "(2019) employs ELMO (Peters et al., 2018) repre-\n",
      "sentations as pretrained features in the encoder of\n",
      "a transformer model. Song et al. (2020) shows that\n",
      "it is possible to improve performance by combin-\n",
      "ing monolingual texts from linguistically related\n",
      "languages, performing a script mapping. It is also\n",
      "possible to extract features from a BERT model\n",
      "in the source language and combining these with\n",
      "an NMT system (Zhu et al., 2020b), but using a\n",
      "BERT model pretrained with a mixed sentences\n",
      "from source and target languages lead to even bet-\n",
      "ter results (Xu et al., 2021).\n",
      "Encoder-decoder pretrained models have\n",
      "gained popularity in the last years for low-\n",
      "resource MT. Conneau and Lample (2019)\n",
      "proposes training the encoder and the decoder\n",
      "separately in order to get cross-language rep-\n",
      "resentations (XLM). This idea has further been\n",
      "extended by Song et al. (2019, MASS) to\n",
      "masking a sequence of tokens from the input.\n",
      "Training MASS in a multilingual fashion and\n",
      "using monolingual data for pretraining helps to\n",
      "improve NMT for low-resource languages and\n",
      "zero-shot translation (Siddhant et al., 2020).\n",
      "Another approach is to train the entire transformer\n",
      "model as a denoising autoencoder (BART; Lewis\n",
      "et al., 2019). The multilingual version of BART\n",
      "(mBART) is more suitable for NMT tasks and\n",
      "yields important gains (Liu et al., 2020). It is also\n",
      "possible to pretrain a transformer in a multi-task,text-to-text fashion, where one of the tasks is\n",
      "MT (T5; Raffel et al., 2020). All four models\n",
      "can be finetuned for MT or used in an unsuper-\n",
      "vised fashion. Improvements to BART can be\n",
      "obtained by augmenting the maximum likelihood\n",
      "objective with an additional objective, which is\n",
      "a data-dependent Gaussian prior distribution (Li\n",
      "et al., 2020). Huge LMs can improve zero-shot\n",
      "and few-shot learning even further (Brown et al.,\n",
      "2020), but at a high computational cost. Pursuing\n",
      "another direction, Wang et al. (2019a) develops a\n",
      "hybrid architecture between a transformer and a\n",
      "pointer-generator network. At training time, the\n",
      "authors jointly train the encoder and the decoder\n",
      "in a denoising auto-encoding fashion.\n",
      "One crucial problem for transfer-learning is\n",
      "minimizing catastrophic forgetting (Serra et al.,\n",
      "2018). Chen et al. (2021) show that it is possible\n",
      "to combine a pre-trained multilingual model, with\n",
      "fine-tuining it with one single language pair, to im-\n",
      "prove zero-shot machine translation. Another way\n",
      "to handle this problem is reducing the number of\n",
      "parameter to be updated. Gheini et al. (2021) pro-\n",
      "pose to only update the cross attention parameters.\n",
      "A.6 Unsupervised MT\n",
      "The addition of other components such as masked\n",
      "LMs and denoising auto-encoding has also been\n",
      "tried (Stojanovski et al., 2019). Unsupervised\n",
      "methods are vulnerable to adversarial attacks of\n",
      "word substitution and order change in the input.\n",
      "Adversarial training can improve performance in\n",
      "such situations (Sun et al., 2020). Since the ini-\n",
      "tialization step is crucial for UMT, Ren et al.\n",
      "(2020) aligns semantically similar sentences from\n",
      "two monolingual corpora with the help of cross-\n",
      "lingual embeddings. With these, an SMT system\n",
      "is trained to warm up an NMT system. How-\n",
      "ever, UMT still has to overcome a set of chal-\n",
      "lenges. Sgaard et al. (2018) shows that perfor-\n",
      "mance decays dramatically for languages with dif-\n",
      "ferent typological features, since, in such situa-\n",
      "tions, bilingual word embeddings (Conneau et al.,\n",
      "2017) are far from isomorphic. Vuli c et al. (2020)\n",
      "finds that isomorphism is also less likely if small\n",
      "amounts of monolingual data are used for training\n",
      "bilingual word embeddings. Nooralahzadeh et al.\n",
      "(2020) discovers that performance quickly deteri-\n",
      "orates for a mismatch of source and target domain\n",
      "and that the initialization of word embeddings can\n",
      "affect MT performance. All of this makes UMTfor LRLs or endangered languages challenging.\n",
      "Some of the described issues have been ad-\n",
      "dressed: Liu et al. (2019) proposes to combine\n",
      "word-level and subword-level embeddings to ac-\n",
      "count for morphological complexity. For the prob-\n",
      "lem of distant language pairs, Leng et al. (2019)\n",
      "proposes pivoting (cf. 6.3). Isomorphism of\n",
      "bilingual word-embeddings can be improved with\n",
      "semi-supervised methods (Vuli c et al., 2019).\n",
      "Garcia et al. (2020) introduces multilingual\n",
      "UMT systems. The main idea consists of general-\n",
      "izing UMT by using a multi-way back-translation\n",
      "objective. Recently, pretrained multilingual trans-\n",
      "former networks are used to improve UMT even\n",
      "further (cf. 6.4).\n",
      "B Ethical Considerations\n",
      "Ethical concerns when working on MT for endan-\n",
      "gered languages include a lack of community in-\n",
      "volvement during language documentation, data\n",
      "creation, and development and setup of MT sys-\n",
      "tems. For more information, we refer interested\n",
      "readers to Bird (2020). Finally, we want to men-\n",
      "tion that publicly employing low-quality MT sys-\n",
      "tems for LRLs bears a risk of translating incor-\n",
      "rectly or in biased (e.g., sexist or racist) ways.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate LSA scores for each sentence\n",
    "lsa_scores = np.sum(np.abs(lsa_matrix), axis=1)\n",
    "\n",
    "# Group sentences by section and rank within each group\n",
    "grouped_sentences = {}\n",
    "for sentence, section, score in zip(section_extraction, sections, lsa_scores):\n",
    "    grouped_sentences.setdefault(section, []).append((score, sentence))\n",
    "ranked_sentences = {\n",
    "    section: sorted(sentences, key=lambda x: x[0], reverse=True)\n",
    "    for section, sentences in grouped_sentences.items()\n",
    "}\n",
    "\n",
    "# Choose the top-ranked sentences from each section\n",
    "num_top_sentences = 5\n",
    "top_ranked_sentences = []\n",
    "\n",
    "# Iterate through each section and extract the top-ranked sentences\n",
    "for section, sentences in ranked_sentences.items():\n",
    "    top_sentences = sentences[:num_top_sentences]\n",
    "    top_ranked_sentences.extend(sentence[1] for sentence in top_sentences)\n",
    "    print(f\"Top-ranked sentences for Section {section}:\\n\" + ''.join(sentence[1] + '\\n' for sentence in top_sentences) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges to solve by next week:\n",
    "1) Extracting section headers where subheadings are not properly formatted within PDF.\n",
    "2) Even after choosing num of sentences threshold for summary, it is not working.\n",
    "3) Implementing LLM APIs for summarization on extracted and processed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
