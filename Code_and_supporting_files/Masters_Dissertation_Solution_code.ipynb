{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABZhA936C-Wm"
      },
      "source": [
        "### Extraction of title from pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyPDF2 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from -r requirements.txt (line 1)) (3.0.1)\n",
            "Requirement already satisfied: nltk in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from -r requirements.txt (line 2)) (3.8.1)\n",
            "Requirement already satisfied: numpy in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from -r requirements.txt (line 3)) (1.26.2)\n",
            "Requirement already satisfied: scikit-learn in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from -r requirements.txt (line 4)) (1.3.2)\n",
            "Requirement already satisfied: pandas in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from -r requirements.txt (line 5)) (2.1.4)\n",
            "Requirement already satisfied: PyMupdf in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from -r requirements.txt (line 6)) (1.19.6)\n",
            "Requirement already satisfied: transformers in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from -r requirements.txt (line 7)) (4.36.2)\n",
            "Requirement already satisfied: tiktoken in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from -r requirements.txt (line 8)) (0.5.2)\n",
            "Requirement already satisfied: tensorflow in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from -r requirements.txt (line 9)) (2.15.0)\n",
            "Requirement already satisfied: typing_extensions>=3.10.0.0 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from PyPDF2->-r requirements.txt (line 1)) (4.9.0)\n",
            "Requirement already satisfied: click in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from nltk->-r requirements.txt (line 2)) (8.1.7)\n",
            "Requirement already satisfied: joblib in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from nltk->-r requirements.txt (line 2)) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from nltk->-r requirements.txt (line 2)) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from nltk->-r requirements.txt (line 2)) (4.66.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 4)) (1.11.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 4)) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from pandas->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from pandas->-r requirements.txt (line 5)) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from pandas->-r requirements.txt (line 5)) (2023.4)\n",
            "Requirement already satisfied: filelock in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from transformers->-r requirements.txt (line 7)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from transformers->-r requirements.txt (line 7)) (0.20.2)\n",
            "Requirement already satisfied: packaging>=20.0 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from transformers->-r requirements.txt (line 7)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from transformers->-r requirements.txt (line 7)) (6.0.1)\n",
            "Requirement already satisfied: requests in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from transformers->-r requirements.txt (line 7)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from transformers->-r requirements.txt (line 7)) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from transformers->-r requirements.txt (line 7)) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-intel==2.15.0 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from tensorflow->-r requirements.txt (line 9)) (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (2.0.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (3.10.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (4.23.4)\n",
            "Requirement already satisfied: setuptools in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (68.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (2.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (0.31.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (2.15.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers->-r requirements.txt (line 7)) (2023.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from requests->transformers->-r requirements.txt (line 7)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from requests->transformers->-r requirements.txt (line 7)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from requests->transformers->-r requirements.txt (line 7)) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from requests->transformers->-r requirements.txt (line 7)) (2023.11.17)\n",
            "Requirement already satisfied: colorama in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from tqdm->nltk->-r requirements.txt (line 2)) (0.4.6)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (2.26.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (3.5.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (7.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (2.1.3)\n",
            "Requirement already satisfied: zipp>=0.5 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (3.17.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in d:\\msc. ai (7th - 8th term)\\dissertation\\code\\lsa\\denv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow->-r requirements.txt (line 9)) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "# Install all necessary packages within virtual environment\n",
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "aeFFkIcuC-Wo"
      },
      "outputs": [],
      "source": [
        "# importing necessary libraries and packages\n",
        "import fitz\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "FbTCiKO3C-Wp"
      },
      "outputs": [],
      "source": [
        "# Input pdf path\n",
        "pdf_path = 'satellite_imagery.pdf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "kP8gxDiw2Rjp"
      },
      "outputs": [],
      "source": [
        "# Extract the title of pdf\n",
        "\n",
        "def extract_title(pdf_file):\n",
        "    with open(pdf_file, 'rb') as pdf_file_obj:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file_obj)\n",
        "\n",
        "        # Attempt to extract the title from the first line of the text\n",
        "        pdf_text = pdf_reader.pages[0].extract_text()\n",
        "        line = pdf_text.split('\\n')[:2]\n",
        "        first_line = ' '.join(line).strip()\n",
        "\n",
        "        # If the first line is not empty, use it as the title\n",
        "        if first_line:\n",
        "            pdf_title = first_line\n",
        "        elif pdf_title == '':\n",
        "        # If the first line is empty, use the PDF file name\n",
        "            pdf_title = pdf_file.split('/')[-1]\n",
        "        else:\n",
        "            document_info = pdf_reader.metadata\n",
        "            pdf_title = document_info.get('/Title')\n",
        "\n",
        "        return pdf_title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "En2wq8sjC-Wq"
      },
      "outputs": [],
      "source": [
        "title = extract_title(pdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztV4VWr7C-Wq",
        "outputId": "2669a5ce-361c-447a-81ab-b1b0262f54e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Using Convolutional Networks and Satellite Imagery to Identify Pa/t_terns in Urban Environments at a Large Scale',\n",
              " str)"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "title, type(title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "7ggR3NRlC-Wr"
      },
      "outputs": [],
      "source": [
        "# Getting the details of fontname and fontsize attribute for each line within pdfs\n",
        "\n",
        "def scrape(filePath):\n",
        "    results = [] # list of tuples that store the information as (text, font size, font name)\n",
        "    pdf = fitz.open(filePath) # filePath is a string that contains the path to the pdf\n",
        "    for page in pdf:\n",
        "        dict = page.get_text(\"dict\")\n",
        "        blocks = dict[\"blocks\"]\n",
        "        for block in blocks:\n",
        "            if \"lines\" in block.keys():\n",
        "                spans = block['lines']\n",
        "                for span in spans:\n",
        "                    data = span['spans']\n",
        "                    for lines in data:\n",
        "                            results.append((lines['text'], lines['size'], lines['font']))\n",
        "                            # lines['text'] -> string, lines['size'] -> font size, lines['font'] -> font name\n",
        "    pdf.close()\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf72jOTEC-Wr",
        "outputId": "44ece68a-8682-419f-c754-168f138590d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Using Convolutional Networks and Satellite Imagery to Identify',\n",
              "  16.752822875976562,\n",
              "  'LinBiolinumTB'),\n",
              " ('Paterns in Urban Environments at a Large Scale',\n",
              "  16.752822875976562,\n",
              "  'LinBiolinumTB'),\n",
              " ('Adrian Albert', 11.633963584899902, 'LinLibertineT'),\n",
              " ('∗', 8.53152847290039, 'LinLibertineT'),\n",
              " ('Massachusets Institute of Technology', 9.597465515136719, 'LinLibertineT'),\n",
              " ('Civil and Environmental Engineering', 9.690055847167969, 'LinLibertineT'),\n",
              " ('77 Massachusets Ave', 9.694904327392578, 'LinLibertineT'),\n",
              " ('Cambridge, MA 02139', 9.694904327392578, 'LinLibertineT'),\n",
              " ('adalbert@mit.edu', 9.694904327392578, 'LinLibertineT'),\n",
              " ('Jasleen Kaur', 11.633963584899902, 'LinLibertineT'),\n",
              " ('Philips Lighting Research North', 9.694904327392578, 'LinLibertineT'),\n",
              " ('America', 9.694904327392578, 'LinLibertineT'),\n",
              " ('2 Canal Park', 9.694904327392578, 'LinLibertineT'),\n",
              " ('Cambridge, MA 02141', 9.694904327392578, 'LinLibertineT'),\n",
              " ('jasleen.kaur1@philips.com', 9.694904327392578, 'LinLibertineT'),\n",
              " ('Marta C. Gonz´alez', 11.633963584899902, 'LinLibertineT'),\n",
              " ('Massachusets Institute of Technology', 9.597465515136719, 'LinLibertineT'),\n",
              " ('Civil and Environmental Engineering', 9.690055847167969, 'LinLibertineT'),\n",
              " ('77 Massachusets Ave', 9.694904327392578, 'LinLibertineT'),\n",
              " ('Cambridge, MA 02139', 9.694904327392578, 'LinLibertineT'),\n",
              " ('martag@mit.edu', 9.694904327392578, 'LinLibertineT'),\n",
              " ('ABSTRACT', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('Urban planning applications (energy audits, investment, etc.) re-',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('quire an understanding of built infrastructure and its environment,',\n",
              "  8.650988578796387,\n",
              "  'LinLibertineT'),\n",
              " ('i.e., both low-level, physical features (amount of vegetation, build-',\n",
              "  8.721108436584473,\n",
              "  'LinLibertineT'),\n",
              " ('ing area and geometry etc.), as well as higher-level concepts such',\n",
              "  8.747259140014648,\n",
              "  'LinLibertineT'),\n",
              " ('as land use classes (which encode expert understanding of socio-',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('economic end uses). Tis kind of data is expensive and labor-',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('intensive to obtain, which limits its availability (particularly in',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('developing countries). We analyze paterns in land use in urban',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('neighborhoods using large-scale satellite imagery data (which is',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('available worldwide from third-party providers) and state-of-the-',\n",
              "  8.777668952941895,\n",
              "  'LinLibertineT'),\n",
              " ('art computer vision techniques based on deep convolutional neural',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('networks. For supervision, given the limited availability of standard',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('benchmarks for remote-sensing data, we obtain ground truth land',\n",
              "  8.694879531860352,\n",
              "  'LinLibertineT'),\n",
              " ('use class labels carefully sampled from open-source surveys, in',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('particular the Urban Atlas land classifcation dataset of 20 land use',\n",
              "  8.642183303833008,\n",
              "  'LinLibertineT'),\n",
              " ('classes across 300 European cities. We use this data to train and',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('compare deep architectures which have recently shown good per-',\n",
              "  8.72983455657959,\n",
              "  'LinLibertineT'),\n",
              " ('formance on standard computer vision tasks (image classifcation',\n",
              "  8.747259140014648,\n",
              "  'LinLibertineT'),\n",
              " ('and segmentation), including on geospatial data. Furthermore, we',\n",
              "  8.708003997802734,\n",
              "  'LinLibertineT'),\n",
              " ('show that the deep representations extracted from satellite imagery',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('of urban environments can be used to compare neighborhoods',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('across several cities. We make our dataset available for other ma-',\n",
              "  8.782005310058594,\n",
              "  'LinLibertineT'),\n",
              " ('chine learning researchers to use for remote-sensing applications.',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('CCS CONCEPTS', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('•', 8.725472450256348, 'LinLibertineT'),\n",
              " ('Computing methodologies', 8.725472450256348, 'LinLibertineTB'),\n",
              " (' →', 8.725472450256348, 'LinLibertineT'),\n",
              " ('Computer vision; Neural net-', 8.725472450256348, 'LinLibertineTB'),\n",
              " ('works;', 8.725472450256348, 'LinLibertineTB'),\n",
              " (' •', 8.725472450256348, 'LinLibertineT'),\n",
              " ('Applied computing', 8.725472450256348, 'LinLibertineTB'),\n",
              " (' →', 8.725472450256348, 'LinLibertineT'),\n",
              " ('Environmental sciences;', 8.725472450256348, 'LinLibertineTB'),\n",
              " ('KEYWORDS', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('Satellite imagery, land use classifcation, convolutional networks',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('1', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('INTRODUCTION', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('Land use classifcation is an important input for applications rang-',\n",
              "  8.686119079589844,\n",
              "  'LinLibertineT'),\n",
              " ('ing from urban planning, zoning and the issuing of business per-',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('mits, to real-estate construction and evaluation to infrastructure',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('∗', 5.332265377044678, 'LinLibertineT'),\n",
              " ('Corresponding author.', 6.78641414642334, 'LinLibertineT'),\n",
              " ('Permission to make digital or hard copies of part or all of this work for personal or',\n",
              "  6.853942394256592,\n",
              "  'LinLibertineT'),\n",
              " ('classroom use is granted without fee provided that copies are not made or distributed',\n",
              "  6.738741874694824,\n",
              "  'LinLibertineT'),\n",
              " ('for proft or commercial advantage and that copies bear this notice and the full citation',\n",
              "  6.718207359313965,\n",
              "  'LinLibertineT'),\n",
              " ('on the frst page. Copyrights for third-party components of this work must be honored.',\n",
              "  6.718207359313965,\n",
              "  'LinLibertineT'),\n",
              " ('For all other uses, contact the owner/author(s).',\n",
              "  6.78641414642334,\n",
              "  'LinLibertineT'),\n",
              " ('KDD’17, August 13–17, 2017, Halifax, NS, Canada.',\n",
              "  6.78641414642334,\n",
              "  'LinLibertineTI'),\n",
              " ('© 2017 Copyright held by the owner/author(s). 978-1-4503-4887-4/17/08.',\n",
              "  6.78641414642334,\n",
              "  'LinLibertineT'),\n",
              " ('DOI: htp://dx.doi.org/10.1145/3097983.3098070',\n",
              "  6.78641414642334,\n",
              "  'LinLibertineT'),\n",
              " ('development. Urban land use classifcation is typically based on',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('surveys performed by trained professionals. As such, this task',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('is labor-intensive, infrequent, slow, and costly. As a result, such',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('data are mostly available in developed countries and big cities that',\n",
              "  8.659784317016602,\n",
              "  'LinLibertineT'),\n",
              " ('have the resources and the vision necessary to collect and curate it;',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('this information is usually not available in many poorer regions,',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('including many developing countries [', 8.637777328491211, 'LinLibertineT'),\n",
              " ('9', 8.725472450256348, 'LinLibertineT'),\n",
              " ('] where it is mostly needed.', 8.637777328491211, 'LinLibertineT'),\n",
              " ('Tis paper builds on two recent trends that promise to make',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('the analysis of urban environments a more democratic and inclu-',\n",
              "  8.777668952941895,\n",
              "  'LinLibertineT'),\n",
              " ('sive task. On the one hand, recent years have seen signifcant',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('improvements in satellite technology and its deployment (primar-',\n",
              "  8.734193801879883,\n",
              "  'LinLibertineT'),\n",
              " ('ily through commercial operators), which allows to obtain high',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('and medium-resolution imagery of most urbanized areas of the',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('Earth with an almost daily revisit rate. On the other hand, the',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('recent breakthroughs in computer vision methods, in particular',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('deep learning models for image classifcation and object detection,',\n",
              "  8.686119079589844,\n",
              "  'LinLibertineT'),\n",
              " ('now make possible to obtain a much more accurate representation',\n",
              "  8.664178848266602,\n",
              "  'LinLibertineT'),\n",
              " ('of the composition built infrastructure and its environments.',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('Our contributions are to both the applied deep learning literature,',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('and to the incipient study of “smart cities” using remote sensing',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('data. We contrast state-of-the-art convolutional architectures (the',\n",
              "  8.712373733520508,\n",
              "  'LinLibertineT'),\n",
              " ('VGG-16 [', 8.637777328491211, 'LinLibertineT'),\n",
              " ('19', 8.725472450256348, 'LinLibertineT'),\n",
              " ('] and ResNet [', 8.637777328491211, 'LinLibertineT'),\n",
              " ('7', 8.725472450256348, 'LinLibertineT'),\n",
              " ('] networks) to train classifers that recog-',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('nize broad land use classes from satellite imagery. We then use the',\n",
              "  8.650988578796387,\n",
              "  'LinLibertineT'),\n",
              " ('features extracted from the model to perform a large-scale compar-',\n",
              "  8.642183303833008,\n",
              "  'LinLibertineT'),\n",
              " ('ison of urban environments. For this, we construct a novel dataset',\n",
              "  8.686119079589844,\n",
              "  'LinLibertineT'),\n",
              " ('for land use classifcation, pairing carefully sampled locations with',\n",
              "  8.642183303833008,\n",
              "  'LinLibertineT'),\n",
              " ('ground truth land use class labels obtained from the Urban Atlas',\n",
              "  8.803651809692383,\n",
              "  'LinLibertineT'),\n",
              " ('survey [', 8.812295913696289, 'LinLibertineT'),\n",
              " ('22', 8.725472450256348, 'LinLibertineT'),\n",
              " ('] with satellite imagery obtained from Google Maps’s',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('static API. Our dataset - which we have made available publicly',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('for other researchers - covers, for now, 10 cities in Europe (chosen',\n",
              "  8.690500259399414,\n",
              "  'LinLibertineT'),\n",
              " ('out of the original 300) with 10 land use classes (from the original',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('20). As the Urban Atlas is a widely-used, standardized dataset for',\n",
              "  8.764649391174316,\n",
              "  'LinLibertineT'),\n",
              " ('land use classifcation, we hope that making this dataset available',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('will encourage the development analyses and algorithms for ana-',\n",
              "  8.782005310058594,\n",
              "  'LinLibertineT'),\n",
              " ('lyzing the built infrastructure in urban environments. Moreover,',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('given that satellite imagery is available virtually everywhere on',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('the globe, the methods presented here allow for automated, rapid',\n",
              "  8.75161075592041,\n",
              "  'LinLibertineT'),\n",
              " ('classifcation of urban environments that can potentially be applied',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('to locations where survey and zoning data is not available.',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('Land use', 8.812295913696289, 'LinLibertineTI'),\n",
              " (' classifcation refers to the combination of physical',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('land atributes and what cultural and socio-economic function land',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('serves (which is a subjective judgement by experts) [',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('2', 8.725472450256348, 'LinLibertineT'),\n",
              " (']. In this paper,', 8.637777328491211, 'LinLibertineT'),\n",
              " ('we take the view that land use classes are just a useful discretization',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('of a more continuous spectrum of paterns in the organization of',\n",
              "  8.782005310058594,\n",
              "  'LinLibertineT'),\n",
              " ('urban environments. Tis viewpoint is illustrated in Figure 2: while',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('arXiv:1704.02965v2  [cs.CV]  13 Sep 2017', 20.0, 'Times-Roman'),\n",
              " ('Figure 1: Urban land use maps for six example cities. We compare the ground truth (',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineTB'),\n",
              " ('top row', 8.725472450256348, 'LinLibertineTBI'),\n",
              " (') with the predicted land use maps,', 8.725472450256348, 'LinLibertineTB'),\n",
              " ('either from using separate data collected from the same city (',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineTB'),\n",
              " ('middle row', 8.725472450256348, 'LinLibertineTBI'),\n",
              " ('), or using data from all other cities (',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineTB'),\n",
              " ('botom row', 8.725472450256348, 'LinLibertineTBI'),\n",
              " (').', 8.725472450256348, 'LinLibertineTB'),\n",
              " ('Figure 2:', 8.725472450256348, 'LinLibertineTB'),\n",
              " (' Lef:', 8.725472450256348, 'LinLibertineTBI'),\n",
              " (' Comparing urban environments via deep hi-',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineTB'),\n",
              " ('erarchical representations of satellite image samples.',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineTB'),\n",
              " (' Right:', 8.725472450256348, 'LinLibertineTBI'),\n",
              " ('approach outline - data collection, classifcation, feature ex-',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineTB'),\n",
              " ('traction, clustering, validation.', 8.725472450256348, 'LinLibertineTB'),\n",
              " ('some atributes (e.g., amount of built structures or vegetation) are',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('directly interpretable, some others may not be. Nevertheless, these',\n",
              "  8.655387878417969,\n",
              "  'LinLibertineT'),\n",
              " ('paterns infuence, and are infuenced by, socio-economic factors',\n",
              "  8.795000076293945,\n",
              "  'LinLibertineT'),\n",
              " ('(e.g., economic activity), resource use (energy), and dynamic human',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('behavior (e.g., mobility, building occupancy). We see the work',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('on cheaply curating a large-scale land use classifcation dataset',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('and comparing neighborhoods using deep representations that',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('this paper puts forth as a necessary frst step towards a granular',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('understanding of urban environments in data-poor regions.',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('Subsequently, in Section 2 we review related studies that apply',\n",
              "  8.734193801879883,\n",
              "  'LinLibertineT'),\n",
              " ('deep learning methods and other machine learning techniques',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('to problems of land use classifcation, object detection, and image',\n",
              "  8.738551139831543,\n",
              "  'LinLibertineT'),\n",
              " ('segmentation in aerial imagery. In Section 3 we describe the dataset',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('we curated based on the Urban Atlas survey. Section 4 reviews the',\n",
              "  8.664178848266602,\n",
              "  'LinLibertineT'),\n",
              " ('deep learning architectures we used. Section 5 describes model',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('validation and analysis results. We conclude in Section 6.',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('All the code used to acquire, process, and analyze the data, as',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('well as to train the models discussed in this paper is available at',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('htp://www.github.com/adrianalbert/urban-environments.',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('2', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('LITERATURE', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('Te literature on the use of remote sensing data for applications in',\n",
              "  8.655387878417969,\n",
              "  'LinLibertineT'),\n",
              " ('land use cover, urban planning, environmental science, and others,',\n",
              "  8.668572425842285,\n",
              "  'LinLibertineT'),\n",
              " ('has a long and rich history. Tis paper however is concerned more',\n",
              "  8.655387878417969,\n",
              "  'LinLibertineT'),\n",
              " ('narrowly with newer work that employs widely-available data',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('and machine learning models - and in particular deep learning',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('architectures - to study urban environments.',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('Deep learning methods have only recently started to be deployed',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('to the analysis of satellite imagery. As such, land use classifcation',\n",
              "  8.677350044250488,\n",
              "  'LinLibertineT'),\n",
              " ('using these tools is still a very incipient literature. Probably the frst',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('studies (yet currently only 1-2 years old) include the application',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('of convolutional neural networks to land use classifcation [',\n",
              "  8.75595760345459,\n",
              "  'LinLibertineT'),\n",
              " ('2', 8.725472450256348, 'LinLibertineT'),\n",
              " ('] us-', 8.75595760345459, 'LinLibertineT'),\n",
              " ('ing the UC Merced land use dataset [', 8.694879531860352, 'LinLibertineT'),\n",
              " ('25', 8.725472450256348, 'LinLibertineT'),\n",
              " ('] (of 2100 images spanning', 8.694879531860352, 'LinLibertineT'),\n",
              " ('21 classes) and the classifcation of agricultural images of cofee',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('plantations [', 8.812295913696289, 'LinLibertineT'),\n",
              " ('17', 8.725472450256348, 'LinLibertineT'),\n",
              " (']. Similar early studies on land use classifcation',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('that employ deep learning techniques are [',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('21', 8.725472450256348, 'LinLibertineT'),\n",
              " ('], [', 8.812295913696289, 'LinLibertineT'),\n",
              " ('18', 8.725472450256348, 'LinLibertineT'),\n",
              " ('], and [', 8.812295913696289, 'LinLibertineT'),\n",
              " ('15', 8.725472450256348, 'LinLibertineT'),\n",
              " (']. In', 8.812295913696289, 'LinLibertineT'),\n",
              " ('[', 8.68173599243164, 'LinLibertineT'),\n",
              " ('11', 8.725472450256348, 'LinLibertineT'),\n",
              " ('], a spatial pyramid pooling technique is employed for land use',\n",
              "  8.68173599243164,\n",
              "  'LinLibertineT'),\n",
              " ('classifcation using satellite imagery. Te authors of these studies',\n",
              "  8.747259140014648,\n",
              "  'LinLibertineT'),\n",
              " ('adapted architectures pre-trained to recognize natural images from',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('the ImageNet dataset (such as the VGG16 [',\n",
              "  8.668572425842285,\n",
              "  'LinLibertineT'),\n",
              " ('19', 8.725472450256348, 'LinLibertineT'),\n",
              " ('], which we also use),', 8.668572425842285, 'LinLibertineT'),\n",
              " ('and fne-tuned them on their (much smaller) land use data. More',\n",
              "  8.786338806152344,\n",
              "  'LinLibertineT'),\n",
              " ('recent studies use the DeepSat land use benchmark dataset [',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('1', 8.725472450256348, 'LinLibertineT'),\n",
              " ('],', 8.812295913696289, 'LinLibertineT'),\n",
              " ('which we also use and describe in more detail in Section 2.1. An-',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('other topic that is closely related to ours is remote-sensing image',\n",
              "  8.75161075592041,\n",
              "  'LinLibertineT'),\n",
              " ('segmentation and object detection, where modern deep learning',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('models have also started to be applied. Some of the earliest work',\n",
              "  8.790670394897461,\n",
              "  'LinLibertineT'),\n",
              " ('that develops and applies deep neural networks for this tasks is that',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('of [', 8.637777328491211, 'LinLibertineT'),\n",
              " ('13', 8.725472450256348, 'LinLibertineT'),\n",
              " (']. Examples of recent studies include [',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('26', 8.725472450256348, 'LinLibertineT'),\n",
              " ('] and [', 8.637777328491211, 'LinLibertineT'),\n",
              " ('12', 8.725472450256348, 'LinLibertineT'),\n",
              " ('], where the', 8.637777328491211, 'LinLibertineT'),\n",
              " ('authors propose a semantic image segmentation technique com-',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('bining texture features and boundary detection in an end-to-end',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('trainable architecture.', 8.725472450256348, 'LinLibertineT'),\n",
              " ('Remote-sensing data and deep learning methods have been put',\n",
              "  8.699256896972656,\n",
              "  'LinLibertineT'),\n",
              " ('to use to other related ends, e.g., geo-localization of ground-level',\n",
              "  8.786338806152344,\n",
              "  'LinLibertineT'),\n",
              " ('photos via satellite images [', 8.668572425842285, 'LinLibertineT'),\n",
              " ('3', 8.725472450256348, 'LinLibertineT'),\n",
              " (',', 8.668572425842285, 'LinLibertineT'),\n",
              " (' 24', 8.725472450256348, 'LinLibertineT'),\n",
              " ('] or predicting ground-level scene', 8.668572425842285, 'LinLibertineT'),\n",
              " ('images from corresponding aerial imagery [',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('27', 8.725472450256348, 'LinLibertineT'),\n",
              " (']. Other applications', 8.637777328491211, 'LinLibertineT'),\n",
              " ('have included predicting survey estimates on poverty levels in',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('several countries in Africa by frst learning to predict levels of night',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('lights (considered as proxies of economic activity and measured',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('by satellites) from day-time, visual-range imagery from Google',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('Maps, then transferring the learning from this later task to the',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('former [', 8.637777328491211, 'LinLibertineT'),\n",
              " ('9', 8.725472450256348, 'LinLibertineT'),\n",
              " (']. Our work takes a similar approach, in that we aim to use',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('remote-sensing data (which is widely-available for most parts of',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('the world) to infer land use types in those locations where ground',\n",
              "  8.686119079589844,\n",
              "  'LinLibertineT'),\n",
              " ('truth surveys are not available.', 8.725472450256348, 'LinLibertineT'),\n",
              " ('Urban environments have been analyzed using other types of',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('imagery data that have become recently available. In [',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('4', 8.725472450256348, 'LinLibertineT'),\n",
              " (',', 8.812295913696289, 'LinLibertineT'),\n",
              " (' 14', 8.725472450256348, 'LinLibertineT'),\n",
              " ('], the', 8.812295913696289, 'LinLibertineT'),\n",
              " ('authors propose to use the same type of imagery from Google Street',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('View to measure the relationship between urban appearance and',\n",
              "  8.777668952941895,\n",
              "  'LinLibertineT'),\n",
              " ('quality of life measures such as perceived safety. For this, they',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('hand-craf standard image features widely used in the computer',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('vision community, and train a shallow machine learning classifer',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('(a support vector machine). In a similar fashion, [',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('5', 8.725472450256348, 'LinLibertineT'),\n",
              " ('] trained a', 8.812295913696289, 'LinLibertineT'),\n",
              " ('convolutional neural network on ground-level Street View imagery',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('paired with a crowd-sourced mechanism for collecting ground truth',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('labels to predict subjective perceptions of urban environments such',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('as “beauty”, “wealth”, and “liveliness”.',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('Land use classifcation has been studied with other new data',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('sources in recent years. For example, ground-level imagery has been',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('employed to accurately predict land use classes on an university',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('campus [', 8.812295913696289, 'LinLibertineT'),\n",
              " ('28', 8.725472450256348, 'LinLibertineT'),\n",
              " (']. Another related literature strand is work that uses',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('mobile phone call records to extract spatial and temporal mobility',\n",
              "  8.708003997802734,\n",
              "  'LinLibertineT'),\n",
              " ('paterns, which are then used to infer land use classes for several',\n",
              "  8.773331642150879,\n",
              "  'LinLibertineT'),\n",
              " ('cities [', 8.637777328491211, 'LinLibertineT'),\n",
              " ('6', 8.725472450256348, 'LinLibertineT'),\n",
              " (',', 8.637777328491211, 'LinLibertineT'),\n",
              " (' 10', 8.725472450256348, 'LinLibertineT'),\n",
              " (',', 8.637777328491211, 'LinLibertineT'),\n",
              " (' 20', 8.725472450256348, 'LinLibertineT'),\n",
              " (']. Our work builds on some of the ideas for sampling',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('geospatial data presented there.', 8.725472450256348, 'LinLibertineT'),\n",
              " ('2.1', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('Existing land use benchmark datasets',\n",
              "  10.615971565246582,\n",
              "  'LinLibertineTB'),\n",
              " ('Public benchmark data for land use classifcation using aerial im-',\n",
              "  8.790670394897461,\n",
              "  'LinLibertineT'),\n",
              " ('agery are still in relatively short supply. Presently there are two',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('such datasets that we are aware of, discussed below.',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('UC Merced.', 8.725472450256348, 'LinLibertineTB'),\n",
              " (' Tis dataset was published in 2010 [', 8.812295913696289, 'LinLibertineT'),\n",
              " ('25', 8.725472450256348, 'LinLibertineT'),\n",
              " ('] and con-', 8.812295913696289, 'LinLibertineT'),\n",
              " ('tains 2100 256', 8.812295913696289, 'LinLibertineT'),\n",
              " (' ×', 8.725472450256348, 'txsy'),\n",
              " (' 256, 1', 8.812295913696289, 'LinLibertineT'),\n",
              " ('m', 8.725472450256348, 'LinLibertineI'),\n",
              " ('/', 8.725472450256348, 'rtxmi'),\n",
              " ('px', 8.725472450256348, 'LinLibertineI'),\n",
              " (' aerial RGB images over 21 land use', 8.812295913696289, 'LinLibertineT'),\n",
              " ('classes. It is considered a “solved problem”, as modern neural net-',\n",
              "  8.747259140014648,\n",
              "  'LinLibertineT'),\n",
              " ('work based classifers [2] have achieved',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " (' >', 8.725472450256348, 'rtxmi'),\n",
              " (' 95% accuracy on it.', 8.725472450256348, 'LinLibertineT'),\n",
              " ('DeepSat.', 8.725472450256348, 'LinLibertineTB'),\n",
              " (' Te DeepSat [', 8.812295913696289, 'LinLibertineT'),\n",
              " ('1', 8.725472450256348, 'LinLibertineT'),\n",
              " ('] dataset', 8.812295913696289, 'LinLibertineT'),\n",
              " ('1', 7.077282428741455, 'LinLibertineT'),\n",
              " (' ', 8.812295913696289, 'LinLibertineT'),\n",
              " ('was released in 2015. It', 8.812295913696289, 'LinLibertineT'),\n",
              " ('contains two benchmarks: the', 8.694879531860352, 'LinLibertineT'),\n",
              " (' Sat-4', 8.694879531860352, 'LinLibertineTI'),\n",
              " (' data of 500', 8.694879531860352, 'LinLibertineT'),\n",
              " (',', 8.725472450256348, 'rtxmi'),\n",
              " (' 000 images over 4', 8.694879531860352, 'LinLibertineT'),\n",
              " ('land use classes (', 8.72983455657959, 'LinLibertineT'),\n",
              " ('barren land, trees, grassland, other', 8.72983455657959, 'LinLibertineTI'),\n",
              " ('), and the', 8.72983455657959, 'LinLibertineT'),\n",
              " (' Sat-6', 8.72983455657959, 'LinLibertineTI'),\n",
              " ('data of 405', 8.812295913696289, 'LinLibertineT'),\n",
              " (',', 8.725472450256348, 'rtxmi'),\n",
              " (' 000 images over 6 land use classes (', 8.812295913696289, 'LinLibertineT'),\n",
              " ('barren land, trees,', 8.812295913696289, 'LinLibertineTI'),\n",
              " ('grassland, roads, buildings, water bodies',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineTI'),\n",
              " ('). All the samples are 28', 8.637777328491211, 'LinLibertineT'),\n",
              " ('×', 8.725472450256348, 'txsy'),\n",
              " ('28', 8.637777328491211, 'LinLibertineT'),\n",
              " ('in size at a 1', 8.812295913696289, 'LinLibertineT'),\n",
              " ('m', 8.725472450256348, 'LinLibertineI'),\n",
              " ('/', 8.725472450256348, 'rtxmi'),\n",
              " ('px', 8.725472450256348, 'LinLibertineI'),\n",
              " (' spatial resolution and contain 4 channels (red,',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('green, blue, and NIR - near infrared). Currently less than two years',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('old, this dataset is already a “solved problem”, with previous studies',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('[', 8.664178848266602, 'LinLibertineT'),\n",
              " ('15', 8.725472450256348, 'LinLibertineT'),\n",
              " ('] (and our own experiments) achieving classifcation accuracies',\n",
              "  8.664178848266602,\n",
              "  'LinLibertineT'),\n",
              " ('1', 5.332265377044678, 'LinLibertineT'),\n",
              " ('Available at htp://csc.lsu.edu/', 6.78641414642334, 'LinLibertineT'),\n",
              " ('∼', 5.332265377044678, 'txsy'),\n",
              " ('saikat/deepsat/.', 6.78641414642334, 'LinLibertineT'),\n",
              " ('of over 99% using convolutional architectures. While useful as input',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('for pre-training more complex models, (e.g., image segmentation),',\n",
              "  8.721108436584473,\n",
              "  'LinLibertineT'),\n",
              " ('this dataset does not allow to take the further steps for detailed',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('land use analysis and comparison of urban environments across',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('cities, which gap we hope our dataset will address.',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('Other open-source eforts.', 8.725472450256348, 'LinLibertineTB'),\n",
              " (' Tere are several other projects', 8.812295913696289, 'LinLibertineT'),\n",
              " ('that we are aware of related to land use classifcation using open-',\n",
              "  8.764649391174316,\n",
              "  'LinLibertineT'),\n",
              " ('source data. Te TerraPatern', 8.712373733520508, 'LinLibertineT'),\n",
              " ('2', 7.077282428741455, 'LinLibertineT'),\n",
              " (' ', 8.712373733520508, 'LinLibertineT'),\n",
              " ('project uses satellite imagery from', 8.712373733520508, 'LinLibertineT'),\n",
              " ('Google Maps (just like we do) paired with truth labels over a large',\n",
              "  8.664178848266602,\n",
              "  'LinLibertineT'),\n",
              " ('number (450) of detailed classes obtained using the Open Street',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('Map API', 8.812295913696289, 'LinLibertineT'),\n",
              " ('3', 7.077282428741455, 'LinLibertineT'),\n",
              " ('. (Open Street Maps is a comprehensive, open-access,',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('crowd-sourced mapping system.) Te project’s intended use is as',\n",
              "  8.773331642150879,\n",
              "  'LinLibertineT'),\n",
              " ('a search tool for satellite imagery, and as such, the classes they',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('employ are very specifc, e.g., baseball diamonds, churches, or',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('roundabouts. Te authors use a ResNet architecture [',\n",
              "  8.768990516662598,\n",
              "  'LinLibertineT'),\n",
              " ('7', 8.725472450256348, 'LinLibertineT'),\n",
              " ('] to train a', 8.768990516662598, 'LinLibertineT'),\n",
              " ('classifcation model, which they use to embed images in a high-',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('dimensional feature space, where “similar” images to an input image',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('can be identifed. A second open-source project related to ours is',\n",
              "  8.782005310058594,\n",
              "  'LinLibertineT'),\n",
              " ('the DeepOSM', 8.812295913696289, 'LinLibertineT'),\n",
              " ('4', 7.077282428741455, 'LinLibertineT'),\n",
              " (', in which the authors take the same approach of',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('pairing OpenStreetMap labels with satellite imagery obtained from',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('Google Maps, and use a convolutional architecture for classifcation.',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('Tese are excellent starting points from a practical standpoint,',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('allowing interested researchers to quickly familiarize themselves',\n",
              "  8.786338806152344,\n",
              "  'LinLibertineT'),\n",
              " ('with programming aspects of data collection, API calls, etc.',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('3', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('THE URBAN ENVIRONMENTS DATASET', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('3.1', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('Urban Atlas: a standard in land use analysis',\n",
              "  10.615971565246582,\n",
              "  'LinLibertineTB'),\n",
              " ('Te Urban Atlas [', 8.812295913696289, 'LinLibertineT'),\n",
              " ('22', 8.725472450256348, 'LinLibertineT'),\n",
              " ('] is an open-source, standardized land use',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('dataset that covers', 8.646587371826172, 'LinLibertineT'),\n",
              " (' ∼', 8.725472450256348, 'txsy'),\n",
              " (' 300 European cities of 100', 8.646587371826172, 'LinLibertineT'),\n",
              " (',', 8.725472450256348, 'rtxmi'),\n",
              " (' 000 inhabitants or', 8.646587371826172, 'LinLibertineT'),\n",
              " ('more, distributed relatively evenly across major geographical and',\n",
              "  8.738551139831543,\n",
              "  'LinLibertineT'),\n",
              " ('geopolitical regions. Te dataset was created between 2005-2011 as',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('part of a major efort by the European Union to provide a uniform',\n",
              "  8.686119079589844,\n",
              "  'LinLibertineT'),\n",
              " ('framework for the geospatial analysis of urban areas in Europe.',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('Land use classifcation is encoded via detailed polygons organized',\n",
              "  8.699256896972656,\n",
              "  'LinLibertineT'),\n",
              " ('in commonly-used GIS/ESRI shape fles. Te dataset covers 20',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('standardized land use classes. In this work we selected classes of',\n",
              "  8.803651809692383,\n",
              "  'LinLibertineT'),\n",
              " ('interest and consolidated them into 10 fnal classes used for analysis',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('(see Figure 3). Producing the original Urban Atlas dataset required',\n",
              "  8.677350044250488,\n",
              "  'LinLibertineT'),\n",
              " ('fusing several data sources: high and medium-resolution satellite',\n",
              "  8.768990516662598,\n",
              "  'LinLibertineT'),\n",
              " ('imagery, topographic maps, navigation and road layout data, and',\n",
              "  8.764649391174316,\n",
              "  'LinLibertineT'),\n",
              " ('local zoning (cadastral) databases. More information on the method-',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('ology used by the Urban Atlas researchers can be obtained from',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('the European Environment Agency', 8.812295913696289, 'LinLibertineT'),\n",
              " ('5', 7.077282428741455, 'LinLibertineT'),\n",
              " ('. We chose expressly to use', 8.812295913696289, 'LinLibertineT'),\n",
              " ('the Urban Atlas dataset over other sources (described in Section 2.1',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('because', 8.812295913696289, 'LinLibertineT'),\n",
              " (' i)', 8.812295913696289, 'LinLibertineTI'),\n",
              " (' it is a comprehensive and consistent survey at a large',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('scale, which has been extensively curated by experts and used in',\n",
              "  8.795000076293945,\n",
              "  'LinLibertineT'),\n",
              " ('research, planning, and socio-economic work over the past decade,',\n",
              "  8.655387878417969,\n",
              "  'LinLibertineT'),\n",
              " ('and', 8.812295913696289, 'LinLibertineT'),\n",
              " (' ii)', 8.812295913696289, 'LinLibertineTI'),\n",
              " (' the land use classes refect higher-level (socio-economic,',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('cultural) functions of the land as used in applications.',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('We note that there is a wide variance in the distribution of land',\n",
              "  8.68173599243164,\n",
              "  'LinLibertineT'),\n",
              " ('use classes across and within the 300 cities. Figure 3 illustrates',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('the diferences in the distribution in ground truth polygon areas',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('2', 5.332265377044678, 'LinLibertineT'),\n",
              " ('htp://www.terrapatern.com/', 6.78641414642334, 'LinLibertineT'),\n",
              " ('3', 5.332265377044678, 'LinLibertineT'),\n",
              " ('htp://www.openstreetmap.org', 6.78641414642334, 'LinLibertineT'),\n",
              " ('4', 5.332265377044678, 'LinLibertineT'),\n",
              " ('htps://github.com/trailbehind/DeepOSM', 6.78641414642334, 'LinLibertineT'),\n",
              " ('5', 5.332265377044678, 'LinLibertineT'),\n",
              " ('htp://www.eea.europa.eu/data-and-maps/data/urban-atlas/',\n",
              "  6.78641414642334,\n",
              "  'LinLibertineT'),\n",
              " ('Figure 3: Ground truth land use distribution (by area) for',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineTB'),\n",
              " ('three example cities in the Urban Environments dataset.',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineTB'),\n",
              " ('for each of the classes for three example cities (Budapest, Rome,',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('Barcelona) from the dataset (from Eastern, Central, and Western',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('Europe, respectively). Tis wide disparity in the spatial distribution',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('paterns of diferent land use classes and across diferent cities',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('motivates us to design a careful sampling procedure for collecting',\n",
              "  8.703631401062012,\n",
              "  'LinLibertineT'),\n",
              " ('training data, described in detail below.',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('3.2', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('Data sampling and acquisition', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('We set out to develop a strategy to obtain high-quality samples',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('of the type (satellite image, ground truth label) to use in training',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('convolutional architectures for image classifcation. Our frst re-',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('quirement is to do this solely with freely-available data sources,',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('as to keep costs very low or close to zero. For this, we chose to',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('use the Google Maps Static API', 8.812295913696289, 'LinLibertineT'),\n",
              " ('6', 7.077282428741455, 'LinLibertineT'),\n",
              " (' ', 8.812295913696289, 'LinLibertineT'),\n",
              " ('as a source of satellite imagery.', 8.812295913696289, 'LinLibertineT'),\n",
              " ('Tis service allows for 25', 8.677350044250488, 'LinLibertineT'),\n",
              " (',', 8.725472450256348, 'rtxmi'),\n",
              " (' 000 API requests/day free of charge. For',\n",
              "  8.677350044250488,\n",
              "  'LinLibertineT'),\n",
              " ('a given sampling location given by (latitude, longitude), we ob-',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('tained 224', 8.637777328491211, 'LinLibertineT'),\n",
              " ('×', 8.725472450256348, 'txsy'),\n",
              " ('224 images at a zoom level 17 (around 1',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('.', 8.725472450256348, 'rtxmi'),\n",
              " ('20', 8.637777328491211, 'LinLibertineT'),\n",
              " ('m', 8.725472450256348, 'LinLibertineI'),\n",
              " ('/', 8.725472450256348, 'rtxmi'),\n",
              " ('px', 8.725472450256348, 'LinLibertineI'),\n",
              " (' spatial', 8.637777328491211, 'LinLibertineT'),\n",
              " ('resolution, or', 8.725472450256348, 'LinLibertineT'),\n",
              " (' ∼', 8.725472450256348, 'txsy'),\n",
              " (' 250', 8.725472450256348, 'LinLibertineT'),\n",
              " ('m', 8.725472450256348, 'LinLibertineI'),\n",
              " (' ×', 8.725472450256348, 'txsy'),\n",
              " (' 250', 8.725472450256348, 'LinLibertineT'),\n",
              " ('m', 8.725472450256348, 'LinLibertineI'),\n",
              " (' coverage for an image).', 8.725472450256348, 'LinLibertineT'),\n",
              " ('Te goals of our sampling strategy are twofold. First, we want',\n",
              "  8.790670394897461,\n",
              "  'LinLibertineT'),\n",
              " ('to ensure that the resulting dataset is as much as possible balanced',\n",
              "  8.642183303833008,\n",
              "  'LinLibertineT'),\n",
              " ('with respect to the land use classes. Te challenge is that the classes',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('are highly imbalanced among the ground truth polygons in the',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('dataset (e.g., many more polygons are agricultural land and isolated',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('structures than airports). Second, the satellite images should be',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('representative of the ground truth class associated to them. To this',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('end, we require that the image contain at least 25% (by area) of',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('the associated ground truth polygon. Tus, our strategy to obtain',\n",
              "  8.747259140014648,\n",
              "  'LinLibertineT'),\n",
              " ('training samples is as follows (for a given city):',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('•', 8.725472450256348, 'txsy'),\n",
              " (' Sort ground truth polygons in decreasing order according to',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('their size, and retain only those polygons with areas larger than',\n",
              "  8.659784317016602,\n",
              "  'LinLibertineT'),\n",
              " ('1', 7.077282428741455, 'LinLibertineT'),\n",
              " ('4', 7.077282428741455, 'LinLibertineT'),\n",
              " (' ', 8.725472450256348, 'LinLibertine'),\n",
              " ('(', 8.725472450256348, 'LinLibertine'),\n",
              " ('224', 8.725472450256348, 'LinLibertineT'),\n",
              " (' ×', 8.725472450256348, 'txsy'),\n",
              " (' 1', 8.725472450256348, 'LinLibertineT'),\n",
              " ('.', 8.725472450256348, 'rtxmi'),\n",
              " ('2', 8.725472450256348, 'LinLibertineT'),\n",
              " ('m', 8.725472450256348, 'LinLibertineI'),\n",
              " (')', 8.725472450256348, 'LinLibertine'),\n",
              " ('2', 7.077282428741455, 'LinLibertineT'),\n",
              " (' =', 8.725472450256348, 'rtxr'),\n",
              " (' 0', 8.725472450256348, 'LinLibertineT'),\n",
              " ('.', 8.725472450256348, 'rtxmi'),\n",
              " ('06', 8.725472450256348, 'LinLibertineT'),\n",
              " ('km', 8.725472450256348, 'LinLibertineI'),\n",
              " ('2', 7.077282428741455, 'LinLibertineT'),\n",
              " (';', 8.725472450256348, 'LinLibertineT'),\n",
              " ('•', 8.725472450256348, 'txsy'),\n",
              " (' From each decile of the distribution of areas, sample a propor-',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('tionally larger number of polygons, such that some of the smaller',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('polygons also are picked, and more of the larger ones;',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('•', 8.725472450256348, 'txsy'),\n",
              " (' For each picked polygon, sample a number of images propor-',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('tional to the area of the polygon, and assign each image the',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('polygon class as ground truth label;', 8.725472450256348, 'LinLibertineT'),\n",
              " ('6', 5.332265377044678, 'LinLibertineT'),\n",
              " ('htps://developers.google.com/maps/documentation/static-maps/',\n",
              "  6.78641414642334,\n",
              "  'LinLibertineT'),\n",
              " ('Figure 4: Example satellite images for the original land use',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineTB'),\n",
              " ('classes in the Urban Atlas dataset.', 8.725472450256348, 'LinLibertineTB'),\n",
              " ('Example satellite images for each of the 10 land use classes in',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('the Urban Environments dataset are given in Figure 4. Note the',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('signifcant variety (in color schemes, textures, etc) in environments',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('denoted as having the same land use class. Tis is because of several',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('factors, including the time of the year when the image was acquired',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('(e.g., agricultural lands appear diferent in the spring than in the',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('fall), the diferent physical form and appearance of environments',\n",
              "  8.764649391174316,\n",
              "  'LinLibertineT'),\n",
              " ('that serve the same socioeconomic or cultural function (e.g., green',\n",
              "  8.668572425842285,\n",
              "  'LinLibertineT'),\n",
              " ('urban areas may look very diferent in diferent cities or in even',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('in diferent parts of the same city; what counts as “dense urban',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('fabric” in one city may not be dense at all in other cities), and',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('change in the landscape during the several years that have passed',\n",
              "  8.712373733520508,\n",
              "  'LinLibertineT'),\n",
              " ('since the compilation of the Urban Atlas dataset and the time of',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('acquisition of the satellite image (e.g., construction sites may not',\n",
              "  8.786338806152344,\n",
              "  'LinLibertineT'),\n",
              " ('refect accurately anymore the reality on the ground).',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('Apart from these training images, we constructed ground truth',\n",
              "  8.703631401062012,\n",
              "  'LinLibertineT'),\n",
              " ('rasters to validate model output for each city. For that, we defned',\n",
              "  8.694879531860352,\n",
              "  'LinLibertineT'),\n",
              " ('uniform validation grids of 100', 8.812295913696289, 'LinLibertineT'),\n",
              " (' ×', 8.725472450256348, 'txsy'),\n",
              " (' 100', 8.812295913696289, 'LinLibertineT'),\n",
              " (' (', 8.725472450256348, 'LinLibertine'),\n",
              " ('25', 8.812295913696289, 'LinLibertineT'),\n",
              " ('km', 8.725472450256348, 'LinLibertineI'),\n",
              " (' ×', 8.725472450256348, 'txsy'),\n",
              " (' 25', 8.812295913696289, 'LinLibertineT'),\n",
              " ('km', 8.725472450256348, 'LinLibertineI'),\n",
              " (')', 8.725472450256348, 'LinLibertine'),\n",
              " (' around the', 8.812295913696289, 'LinLibertineT'),\n",
              " ('(geographical) center of a given city of interest. We take a satellite',\n",
              "  8.686119079589844,\n",
              "  'LinLibertineT'),\n",
              " ('image sample in each grid cell, and assign to it as label the class',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('of the polygon that has the maximum intersection area with that',\n",
              "  8.782005310058594,\n",
              "  'LinLibertineT'),\n",
              " ('cell. Examples of land use maps for the six cities we analyze here',\n",
              "  8.773331642150879,\n",
              "  'LinLibertineT'),\n",
              " ('are given in Figure 1 (top row). Tere, each grid cell is assigned the',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('class of the ground truth polygon whose intersection with the cell',\n",
              "  8.694879531860352,\n",
              "  'LinLibertineT'),\n",
              " ('has maximum coverage fraction by area. Classes are color-coded',\n",
              "  8.795000076293945,\n",
              "  'LinLibertineT'),\n",
              " ('following the original Urban Atlas documentation.',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('In Table 1 we present summaries of the training (lef) and vali-',\n",
              "  8.782005310058594,\n",
              "  'LinLibertineT'),\n",
              " ('dation (right) datasets we used for the analysis in this paper. Te',\n",
              "  8.79932689666748,\n",
              "  'LinLibertineT'),\n",
              " ('validation dataset consists of the images sampled at the centers of',\n",
              "  8.712373733520508,\n",
              "  'LinLibertineT'),\n",
              " ('each cell in the 25', 8.686119079589844, 'LinLibertineT'),\n",
              " ('km', 8.725472450256348, 'LinLibertineI'),\n",
              " (' ×', 8.725472450256348, 'txsy'),\n",
              " (' 25', 8.686119079589844, 'LinLibertineT'),\n",
              " ('km', 8.725472450256348, 'LinLibertineI'),\n",
              " (' grid as discussed above. Tis dataset', 8.686119079589844, 'LinLibertineT'),\n",
              " ('consists of', 8.75595760345459, 'LinLibertineT'),\n",
              " (' ∼', 8.725472450256348, 'txsy'),\n",
              " (' 140', 8.75595760345459, 'LinLibertineT'),\n",
              " (',', 8.725472450256348, 'rtxmi'),\n",
              " (' 000 images distributed across 10 urban environ-',\n",
              "  8.75595760345459,\n",
              "  'LinLibertineT'),\n",
              " ('ment classes from 6 cities: Roma (Rome), Madrid, Berlin, Budapest,',\n",
              "  8.659784317016602,\n",
              "  'LinLibertineT'),\n",
              " ('Barcelona, and Athina (Athens). Because of the high variation in',\n",
              "  8.803651809692383,\n",
              "  'LinLibertineT'),\n",
              " ('appearance upon visual inspection, we chose to consolidate several',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('classes from the original dataset, in particular classes that indicated',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('urban fabric into “High Density Urban Fabric”, “Medium Density',\n",
              "  8.786338806152344,\n",
              "  'LinLibertineT'),\n",
              " ('Urban Fabric, and “Low Density Urban Fabric”. As mentioned above',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('and illustrated in Figure 3, we did notice a great disparity in the',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('numbers and distribution of ground truth polygons for other ex-',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('ample cities that we investigated in the Urban Atlas dataset. As',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('such,for the analysis in this paper, we have chosen cities where',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('enough ground truth polygons were available for each class (that',\n",
              "  8.760305404663086,\n",
              "  'LinLibertineT'),\n",
              " ('is, at least 50 samples) to allow for statistical comparisons.',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('4', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('EXPERIMENTAL SETUP', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('4.1', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('Neural network architectures and training',\n",
              "  10.615971565246582,\n",
              "  'LinLibertineTB'),\n",
              " ('For all experiments in this paper we compared the VGG-16 [',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('19', 8.725472450256348, 'LinLibertineT'),\n",
              " (']', 8.812295913696289, 'LinLibertineT'),\n",
              " ('and ResNet [7, 8] architectures.', 8.725472450256348, 'LinLibertineT'),\n",
              " ('VGG-16.', 8.725472450256348, 'LinLibertineTB'),\n",
              " (' Tis architecture [', 8.637777328491211, 'LinLibertineT'),\n",
              " ('19', 8.725472450256348, 'LinLibertineT'),\n",
              " ('] has become one of the most pop-', 8.637777328491211, 'LinLibertineT'),\n",
              " ('ular models in computer vision for classifcation and segmentation',\n",
              "  8.650988578796387,\n",
              "  'LinLibertineT'),\n",
              " ('tasks. It consists of 16 trainable layers organized in blocks. It starts',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('with a 5-block convolutional base of neurons with 3',\n",
              "  8.742905616760254,\n",
              "  'LinLibertineT'),\n",
              " (' ×', 8.725472450256348, 'txsy'),\n",
              " (' 3 receptive', 8.742905616760254, 'LinLibertineT'),\n",
              " ('felds (alternated with max-pooling layers that efectively increase',\n",
              "  8.686119079589844,\n",
              "  'LinLibertineT'),\n",
              " ('the receptive feld of neurons further downstream). Following each',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('convolutional layer is a ReLU activation function [',\n",
              "  8.68173599243164,\n",
              "  'LinLibertineT'),\n",
              " ('19', 8.725472450256348, 'LinLibertineT'),\n",
              " (']. Te feature', 8.68173599243164, 'LinLibertineT'),\n",
              " ('maps thus obtained are fed into a set of fully-connected layers (a',\n",
              "  8.79932689666748,\n",
              "  'LinLibertineT'),\n",
              " ('deep neural network classifer). See Table 2 for a summary.',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('ResNet.', 8.725472450256348, 'LinLibertineTB'),\n",
              " (' Tis architecture [', 8.812295913696289, 'LinLibertineT'),\n",
              " ('7', 8.725472450256348, 'LinLibertineT'),\n",
              " (',', 8.812295913696289, 'LinLibertineT'),\n",
              " (' 8', 8.725472450256348, 'LinLibertineT'),\n",
              " ('] has achieved state-of-the-art', 8.812295913696289, 'LinLibertineT'),\n",
              " ('performance on image classifcation on several popular natural',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('image benchmark datasets. It consists of blocks of convolutional',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('layers, each of which is followed by a ReLU non-linearity. As before,',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('each block in the convolutional base is followed by a max-pooling',\n",
              "  8.699256896972656,\n",
              "  'LinLibertineT'),\n",
              " ('operation. Finally, the output of the last convolutional layer serves',\n",
              "  8.655387878417969,\n",
              "  'LinLibertineT'),\n",
              " ('as input feature map for a fully-connected layer with a sofmax',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('activation function. Te key diference in this architecture is that',\n",
              "  8.768990516662598,\n",
              "  'LinLibertineT'),\n",
              " ('shortcut', 8.812295913696289, 'LinLibertineTI'),\n",
              " (' connections are implemented that skip blocks of convo-',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('lutional layers, allowing the network to learn residual mappings',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('between layer input and output. Here we used an implementation',\n",
              "  8.699256896972656,\n",
              "  'LinLibertineT'),\n",
              " ('with 50 trainable layers per [7]. See Table 3 for a summary.',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('Transfer learning.', 8.725472450256348, 'LinLibertineTB'),\n",
              " (' As it is common practice in the literature,',\n",
              "  8.807974815368652,\n",
              "  'LinLibertineT'),\n",
              " ('we have experimented with training our models on the problem of',\n",
              "  8.655387878417969,\n",
              "  'LinLibertineT'),\n",
              " ('interest (urban environment classifcation) starting from architec-',\n",
              "  8.742905616760254,\n",
              "  'LinLibertineT'),\n",
              " ('tures pre-trained on datasets from other domains (',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('transfer learning', 8.637777328491211, 'LinLibertineTI'),\n",
              " (').', 8.637777328491211, 'LinLibertineT'),\n",
              " ('Tis procedure has been shown to yield both beter performance',\n",
              "  8.79932689666748,\n",
              "  'LinLibertineT'),\n",
              " ('and faster training times, as the network already has learned to',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('recognize basic shapes and paterns that are characteristic of im-',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('ages across many domains (e.g., [', 8.664178848266602, 'LinLibertineT'),\n",
              " ('9', 8.725472450256348, 'LinLibertineT'),\n",
              " (',', 8.664178848266602, 'LinLibertineT'),\n",
              " (' 12', 8.725472450256348, 'LinLibertineT'),\n",
              " (',', 8.664178848266602, 'LinLibertineT'),\n",
              " (' 15', 8.725472450256348, 'LinLibertineT'),\n",
              " (']). We have implemented', 8.664178848266602, 'LinLibertineT'),\n",
              " ('the following approaches:', 8.812295913696289, 'LinLibertineT'),\n",
              " (' 1)', 8.812295913696289, 'LinLibertineTI'),\n",
              " (' we used models pre-trained on the', 8.812295913696289, 'LinLibertineT'),\n",
              " ('ImageNet dataset, then further fne-tuned them on the Urban Atlas',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('dataset; and', 8.690500259399414, 'LinLibertineT'),\n",
              " (' 2)', 8.690500259399414, 'LinLibertineTI'),\n",
              " (' we pre-trained on the DeepSat dataset (See Section',\n",
              "  8.690500259399414,\n",
              "  'LinLibertineT'),\n",
              " ('2), then further refned on the Urban Atlas dataset. As expected,',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('the later strategy - frst training a model (itself pre-trained on Ima-',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('geNet data) on the DeepSat benchmark, and the further refning',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('on the Urban Atlas dataset - yielded the best results, achieving',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('increases of around 5% accuracy for a given training time.',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('Given the large amount of variation in the visual appearance',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('of urban environments across diferent cities (because of diferent',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('climates, diferent architecture styles, various other socio-economic',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('factors), it is of interest to study to what extent a model learned on',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('one geographical location can be applied to a diferent geographical',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('location. As such, we perform experiments in which we train a',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('model for one (or more) cities, then apply the model to a diferent set',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('of cities. Intuitively, one would expect that, the more neighborhoods',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('and other urban features at one location are similar to those at a',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('diferent location, the beter learning would transfer, and the higher',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('the classifcation accuracy obtained would be. Results for these',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('experiments are summarized in Figure 6.',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('4.2', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('Comparing urban environments', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('We next used the convolutional architectures to extract features',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('for validation images. As in other recent studies (e.g., [',\n",
              "  8.738551139831543,\n",
              "  'LinLibertineT'),\n",
              " ('9', 8.725472450256348, 'LinLibertineT'),\n",
              " (']), we use', 8.738551139831543, 'LinLibertineT'),\n",
              " ('the last layer of a network as feature extractor. Tis amounts to',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('feature vectors of', 8.637777328491211, 'LinLibertineT'),\n",
              " (' D', 8.725472450256348, 'LinLibertineI'),\n",
              " (' =', 8.725472450256348, 'rtxr'),\n",
              " (' 4096 dimensions for the VGG16 architecture',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('and', 8.812295913696289, 'LinLibertineT'),\n",
              " (' D', 8.725472450256348, 'LinLibertineI'),\n",
              " (' =', 8.725472450256348, 'rtxr'),\n",
              " (' 2048 dimensions for the ResNet-50 architecture. Te',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('codes', 8.812295913696289, 'LinLibertineT'),\n",
              " (' x', 8.725472450256348, 'LinLibertineI'),\n",
              " (' ∈', 8.725472450256348, 'txsy'),\n",
              " (' R', 8.725472450256348, 'txsyb'),\n",
              " ('D', 7.077282428741455, 'LinLibertineI7'),\n",
              " (' ', 8.812295913696289, 'LinLibertineT'),\n",
              " ('are the image representations that either network',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('derives as most representative to discriminate the high-level land',\n",
              "  8.75595760345459,\n",
              "  'LinLibertineT'),\n",
              " ('use concepts it is trained to predict.',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('We would like to study how ”similar” diferent classes of urban',\n",
              "  8.716742515563965,\n",
              "  'LinLibertineT'),\n",
              " ('environments are across two example cities (here we picked Berlin',\n",
              "  8.642183303833008,\n",
              "  'LinLibertineT'),\n",
              " ('and Barcelona, which are fairly diferent from a cultural and archi-',\n",
              "  8.68173599243164,\n",
              "  'LinLibertineT'),\n",
              " ('tectural standpoint). For this, we focus only on the 25',\n",
              "  8.782005310058594,\n",
              "  'LinLibertineT'),\n",
              " ('km', 8.725472450256348, 'LinLibertineI'),\n",
              " (' ×', 8.725472450256348, 'txsy'),\n",
              " (' 25', 8.782005310058594, 'LinLibertineT'),\n",
              " ('km', 8.725472450256348, 'LinLibertineI'),\n",
              " (',', 8.782005310058594, 'LinLibertineT'),\n",
              " ('100', 8.637777328491211, 'LinLibertineT'),\n",
              " (' ×', 8.725472450256348, 'txsy'),\n",
              " (' 100-cell grids around the city center as in Figure 1. To be able',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('to quantify similarity in local urban environments, we construct',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('a KD-tree', 8.716742515563965, 'LinLibertineT'),\n",
              " (' T', 8.725472450256348, 'txsy'),\n",
              " (' (using a high-performance implementation available',\n",
              "  8.716742515563965,\n",
              "  'LinLibertineT'),\n",
              " ('in the Python package', 8.812295913696289, 'LinLibertineT'),\n",
              " (' scikit-learn', 8.725472450256348, 'Inconsolata-zi4r'),\n",
              " (' [', 8.812295913696289, 'LinLibertineT'),\n",
              " ('16', 8.725472450256348, 'LinLibertineT'),\n",
              " (']) using all the gridded', 8.812295913696289, 'LinLibertineT'),\n",
              " ('samples. Tis data structure allows to fnd',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " (' k', 8.725472450256348, 'LinLibertineI'),\n",
              " ('-nearest neighbors of a', 8.637777328491211, 'LinLibertineT'),\n",
              " ('query image in an efcient way. In this way, the feature space can',\n",
              "  8.699256896972656,\n",
              "  'LinLibertineT'),\n",
              " ('be probed in an efcient way.', 8.725472450256348, 'LinLibertineT'),\n",
              " ('5', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('RESULTS AND DISCUSSION', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('In Figure 1 we show model performance on the 100',\n",
              "  8.672961235046387,\n",
              "  'LinLibertineT'),\n",
              " (' ×', 8.725472450256348, 'txsy'),\n",
              " (' 100 (25', 8.672961235046387, 'LinLibertineT'),\n",
              " ('km', 8.725472450256348, 'LinLibertineI'),\n",
              " (' ×', 8.725472450256348, 'txsy'),\n",
              " ('25', 8.708003997802734, 'LinLibertineT'),\n",
              " ('km', 8.725472450256348, 'LinLibertineI'),\n",
              " (') raster grids we used for testing. Te top row shows ground',\n",
              "  8.708003997802734,\n",
              "  'LinLibertineT'),\n",
              " ('truth grids, where the class in each cell was assigned as the most',\n",
              "  8.790670394897461,\n",
              "  'LinLibertineT'),\n",
              " ('prevalent land use class by area (see also Section 3). Te botom row',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('shows model predictions, where each cell in a raster is painted in',\n",
              "  8.760305404663086,\n",
              "  'LinLibertineT'),\n",
              " ('the color corresponding to the maximum probability class estimated',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('by the model (here ResNet-50). Columns in the fgure show results',\n",
              "  8.650988578796387,\n",
              "  'LinLibertineT'),\n",
              " ('for each of the 6 cities we used in our dataset. Even at a frst visual',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('inspection, the model is able to recreate from satellite imagery',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('qualitatively the urban land use classifcation map.',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineT'),\n",
              " ('Further, looking at the individual classes separately and the con-',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('fdence of the model in its predictions (the probability distribution',\n",
              "  8.686119079589844,\n",
              "  'LinLibertineT'),\n",
              " ('over classes computed by the model), the picture is again qualita-',\n",
              "  8.777668952941895,\n",
              "  'LinLibertineT'),\n",
              " ('tively very encouraging. In Figure 5 we show grayscale raster maps',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('encoding the spatial layout of the class probability distribution for',\n",
              "  8.686119079589844,\n",
              "  'LinLibertineT'),\n",
              " ('one example city, Barcelona. Particularly good qualitative agree-',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('ment is observed for agricultural lands, water bodies, industrial,',\n",
              "  8.812295913696289,\n",
              "  'LinLibertineT'),\n",
              " ('public, and commercial land, forests, green urban areas, low density',\n",
              "  8.637777328491211,\n",
              "  'LinLibertineT'),\n",
              " ('urban fabric, airports, and sports and leisure facilities. Te model',\n",
              "  8.768990516662598,\n",
              "  'LinLibertineT'),\n",
              " ('appears to struggle with reconstructing the spatial distribution of',\n",
              "  8.738551139831543,\n",
              "  'LinLibertineT'),\n",
              " ('roads, which is not unexpected, given that roads typically appear',\n",
              "  8.768990516662598,\n",
              "  'LinLibertineT'),\n",
              " ('in many other scenes that have a diferent functional classifcation',\n",
              "  8.668572425842285,\n",
              "  'LinLibertineT'),\n",
              " ('for urban planning purposes.', 8.725472450256348, 'LinLibertineT'),\n",
              " ('Table 1: Urban Environments dataset: sample size summary.',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineTB'),\n",
              " ('(a) Dataset used for training & validation (',\n",
              "  7.755943298339844,\n",
              "  'LinLibertineTB'),\n",
              " ('80%', 7.755943298339844, 'LinLibertineT'),\n",
              " (' and', 7.755943298339844, 'LinLibertineTB'),\n",
              " (' 20%', 7.755943298339844, 'LinLibertineT'),\n",
              " (', respectively)', 7.755943298339844, 'LinLibertineTB'),\n",
              " ('class/city', 5.816981792449951, 'LinLibertineT'),\n",
              " ('athina', 5.816981792449951, 'LinLibertineT'),\n",
              " ('barcelona', 5.816981792449951, 'LinLibertineT'),\n",
              " ('berlin', 5.816981792449951, 'LinLibertineT'),\n",
              " ('budapest', 5.816981792449951, 'LinLibertineT'),\n",
              " ('madrid', 5.816981792449951, 'LinLibertineT'),\n",
              " ('roma', 5.816981792449951, 'LinLibertineT'),\n",
              " ('class', 5.816981792449951, 'LinLibertineT'),\n",
              " ('total', 5.816981792449951, 'LinLibertineT'),\n",
              " ('Agricultural + Semi-', 5.816981792449951, 'LinLibertineT'),\n",
              " ('natural areas + Wet-', 5.816981792449951, 'LinLibertineT'),\n",
              " ('lands', 5.816981792449951, 'LinLibertineT'),\n",
              " ('4347', 5.816981792449951, 'LinLibertineT'),\n",
              " ('2987', 5.816981792449951, 'LinLibertineT'),\n",
              " ('7602', 5.816981792449951, 'LinLibertineT'),\n",
              " ('2211', 5.816981792449951, 'LinLibertineT'),\n",
              " ('4662', 5.816981792449951, 'LinLibertineT'),\n",
              " ('4043', 5.816981792449951, 'LinLibertineT'),\n",
              " ('25852', 5.816981792449951, 'LinLibertineT'),\n",
              " ('Airports', 5.816981792449951, 'LinLibertineT'),\n",
              " ('382', 5.816981792449951, 'LinLibertineT'),\n",
              " ('452', 5.816981792449951, 'LinLibertineT'),\n",
              " ('232', 5.816981792449951, 'LinLibertineT'),\n",
              " ('138', 5.816981792449951, 'LinLibertineT'),\n",
              " ('124', 5.816981792449951, 'LinLibertineT'),\n",
              " ('142', 5.816981792449951, 'LinLibertineT'),\n",
              " ('1470', 5.816981792449951, 'LinLibertineT'),\n",
              " ('Forests', 5.816981792449951, 'LinLibertineT'),\n",
              " ('1806', 5.816981792449951, 'LinLibertineT'),\n",
              " ('2438', 5.816981792449951, 'LinLibertineT'),\n",
              " ('7397', 5.816981792449951, 'LinLibertineT'),\n",
              " ('1550', 5.816981792449951, 'LinLibertineT'),\n",
              " ('2685', 5.816981792449951, 'LinLibertineT'),\n",
              " ('2057', 5.816981792449951, 'LinLibertineT'),\n",
              " ('17933', 5.816981792449951, 'LinLibertineT'),\n",
              " ('Green urban areas', 5.816981792449951, 'LinLibertineT'),\n",
              " ('990', 5.816981792449951, 'LinLibertineT'),\n",
              " ('722', 5.816981792449951, 'LinLibertineT'),\n",
              " ('1840', 5.816981792449951, 'LinLibertineT'),\n",
              " ('1342', 5.816981792449951, 'LinLibertineT'),\n",
              " ('1243', 5.816981792449951, 'LinLibertineT'),\n",
              " ('1401', 5.816981792449951, 'LinLibertineT'),\n",
              " ('7538', 5.816981792449951, 'LinLibertineT'),\n",
              " ('High Density Urban', 5.816981792449951, 'LinLibertineT'),\n",
              " ('Fabric', 5.816981792449951, 'LinLibertineT'),\n",
              " ('967', 5.816981792449951, 'LinLibertineT'),\n",
              " ('996', 5.816981792449951, 'LinLibertineT'),\n",
              " ('8975', 5.816981792449951, 'LinLibertineT'),\n",
              " ('6993', 5.816981792449951, 'LinLibertineT'),\n",
              " ('2533', 5.816981792449951, 'LinLibertineT'),\n",
              " ('3103', 5.816981792449951, 'LinLibertineT'),\n",
              " ('23567', 5.816981792449951, 'LinLibertineT'),\n",
              " ('Industrial, commer-', 5.816981792449951, 'LinLibertineT'),\n",
              " ('cial, public, military', 5.816981792449951, 'LinLibertineT'),\n",
              " ('and pr…', 5.816981792449951, 'LinLibertineT'),\n",
              " ('1887', 5.816981792449951, 'LinLibertineT'),\n",
              " ('2116', 5.816981792449951, 'LinLibertineT'),\n",
              " ('4761', 5.816981792449951, 'LinLibertineT'),\n",
              " ('1850', 5.816981792449951, 'LinLibertineT'),\n",
              " ('3203', 5.816981792449951, 'LinLibertineT'),\n",
              " ('2334', 5.816981792449951, 'LinLibertineT'),\n",
              " ('16151', 5.816981792449951, 'LinLibertineT'),\n",
              " ('Low Density Urban', 5.816981792449951, 'LinLibertineT'),\n",
              " ('Fabric', 5.816981792449951, 'LinLibertineT'),\n",
              " ('1424', 5.816981792449951, 'LinLibertineT'),\n",
              " ('1520', 5.816981792449951, 'LinLibertineT'),\n",
              " ('2144', 5.816981792449951, 'LinLibertineT'),\n",
              " ('575', 5.816981792449951, 'LinLibertineT'),\n",
              " ('2794', 5.816981792449951, 'LinLibertineT'),\n",
              " ('3689', 5.816981792449951, 'LinLibertineT'),\n",
              " ('12146', 5.816981792449951, 'LinLibertineT'),\n",
              " ('Medium Density Ur-', 5.816981792449951, 'LinLibertineT'),\n",
              " ('ban Fabric', 5.816981792449951, 'LinLibertineT'),\n",
              " ('2144', 5.816981792449951, 'LinLibertineT'),\n",
              " ('1128', 5.816981792449951, 'LinLibertineT'),\n",
              " ('6124', 5.816981792449951, 'LinLibertineT'),\n",
              " ('1661', 5.816981792449951, 'LinLibertineT'),\n",
              " ('1833', 5.816981792449951, 'LinLibertineT'),\n",
              " ('2100', 5.816981792449951, 'LinLibertineT'),\n",
              " ('14990', 5.816981792449951, 'LinLibertineT'),\n",
              " ('Sports and leisure fa-', 5.816981792449951, 'LinLibertineT'),\n",
              " ('cilities', 5.816981792449951, 'LinLibertineT'),\n",
              " ('750', 5.816981792449951, 'LinLibertineT'),\n",
              " ('1185', 5.816981792449951, 'LinLibertineT'),\n",
              " ('2268', 5.816981792449951, 'LinLibertineT'),\n",
              " ('1305', 5.816981792449951, 'LinLibertineT'),\n",
              " ('1397', 5.816981792449951, 'LinLibertineT'),\n",
              " ('1336', 5.816981792449951, 'LinLibertineT'),\n",
              " ('8241', 5.816981792449951, 'LinLibertineT'),\n",
              " ('Water bodies', 5.816981792449951, 'LinLibertineT'),\n",
              " ('537', 5.816981792449951, 'LinLibertineT'),\n",
              " ('408', 5.816981792449951, 'LinLibertineT'),\n",
              " ('1919', 5.816981792449951, 'LinLibertineT'),\n",
              " ('807', 5.816981792449951, 'LinLibertineT'),\n",
              " ('805', 5.816981792449951, 'LinLibertineT'),\n",
              " ('619', 5.816981792449951, 'LinLibertineT'),\n",
              " ('5095', 5.816981792449951, 'LinLibertineT'),\n",
              " ('city total', 5.816981792449951, 'LinLibertineT'),\n",
              " ('15234', 5.816981792449951, 'LinLibertineT'),\n",
              " ('13952', 5.816981792449951, 'LinLibertineT'),\n",
              " ('43262', 5.816981792449951, 'LinLibertineT'),\n",
              " ('18432', 5.816981792449951, 'LinLibertineT'),\n",
              " ('21279', 5.816981792449951, 'LinLibertineT'),\n",
              " ('20824', 5.816981792449951, 'LinLibertineT'),\n",
              " ('132983', 5.816981792449951, 'LinLibertineT'),\n",
              " ('(b)', 7.755943298339844, 'LinLibertineTB'),\n",
              " (' 25', 7.755943298339844, 'LinLibertineT'),\n",
              " ('km', 7.755943298339844, 'LinLibertineI7'),\n",
              " (' ×', 7.755943298339844, 'txsy'),\n",
              " (' 25', 7.755943298339844, 'LinLibertineT'),\n",
              " ('km', 7.755943298339844, 'LinLibertineI7'),\n",
              " (' ground truth test grids (fractions of city total)',\n",
              "  7.755943298339844,\n",
              "  'LinLibertineTB'),\n",
              " ('class / city', 5.816981792449951, 'LinLibertineT'),\n",
              " ('athina', 5.816981792449951, 'LinLibertineT'),\n",
              " ('barcelona', 5.816981792449951, 'LinLibertineT'),\n",
              " ('berlin', 5.816981792449951, 'LinLibertineT'),\n",
              " ('budapest', 5.816981792449951, 'LinLibertineT'),\n",
              " ('madrid', 5.816981792449951, 'LinLibertineT'),\n",
              " ('roma', 5.816981792449951, 'LinLibertineT'),\n",
              " ('Agricultural + Semi-', 5.816981792449951, 'LinLibertineT'),\n",
              " ('natural areas + Wet-', 5.816981792449951, 'LinLibertineT'),\n",
              " ('lands', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.350', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.261', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.106', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.181', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.395', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.473', 5.816981792449951, 'LinLibertineT'),\n",
              " ('Airports', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.003', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.030', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.013', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.000', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.044', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.006', 5.816981792449951, 'LinLibertineT'),\n",
              " ('Forests', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.031', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.192', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.087', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.211', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.013', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.019', 5.816981792449951, 'LinLibertineT'),\n",
              " ('Green urban areas', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.038', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.030', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.072', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.027', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.125', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.054', 5.816981792449951, 'LinLibertineT'),\n",
              " ('High Density Urban', 5.816981792449951, 'LinLibertineT'),\n",
              " ('Fabric', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.389', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.217', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.284', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.365', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.170', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.215', 5.816981792449951, 'LinLibertineT'),\n",
              " ('Industrial, commer-', 5.816981792449951, 'LinLibertineT'),\n",
              " ('cial, public, military', 5.816981792449951, 'LinLibertineT'),\n",
              " ('and pr…', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.109', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.160', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.190', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.096', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.138', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.129', 5.816981792449951, 'LinLibertineT'),\n",
              " ('Low Density Urban', 5.816981792449951, 'LinLibertineT'),\n",
              " ('Fabric', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.016', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.044', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.012', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.006', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.036', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.029', 5.816981792449951, 'LinLibertineT'),\n",
              " ('Medium Density Ur-', 5.816981792449951, 'LinLibertineT'),\n",
              " ('ban Fabric', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.041', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.025', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.129', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.045', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.042', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.047', 5.816981792449951, 'LinLibertineT'),\n",
              " ('Sports and leisure fa-', 5.816981792449951, 'LinLibertineT'),\n",
              " ('cilities', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.017', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.034', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.080', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.025', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.036', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.025', 5.816981792449951, 'LinLibertineT'),\n",
              " ('Water bodies', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.005', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.006', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.026', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.044', 5.816981792449951, 'LinLibertineT'),\n",
              " ('<', 6.301697731018066, 'rtxmi7'),\n",
              " ('0.001', 5.816981792449951, 'LinLibertineT'),\n",
              " ('0.004', 5.816981792449951, 'LinLibertineT'),\n",
              " ('Figure 5: Barcelona: ground truth (', 8.725472450256348, 'LinLibertineTB'),\n",
              " ('top', 8.725472450256348, 'LinLibertineTBI'),\n",
              " (') and predicted probabilities (', 8.725472450256348, 'LinLibertineTB'),\n",
              " ('botom', 8.725472450256348, 'LinLibertineTBI'),\n",
              " (').', 8.725472450256348, 'LinLibertineTB'),\n",
              " ('Table 2: Te VGG16 architecture [19].', 8.725472450256348, 'LinLibertineTB'),\n",
              " ('Block 1', 6.78641414642334, 'LinLibertineTB'),\n",
              " ('Block 2', 6.78641414642334, 'LinLibertineTB'),\n",
              " ('Block 3', 6.78641414642334, 'LinLibertineTB'),\n",
              " ('Block 4', 6.78641414642334, 'LinLibertineTB'),\n",
              " ('Block 5', 6.78641414642334, 'LinLibertineTB'),\n",
              " ('Block 6', 6.78641414642334, 'LinLibertineTB'),\n",
              " ('Conv(3,64)', 6.853942394256592, 'LinLibertineT'),\n",
              " ('Conv(3,64)', 6.853942394256592, 'LinLibertineT'),\n",
              " ('Max-', 6.853942394256592, 'LinLibertineT'),\n",
              " ('Pool(2,2)', 6.78641414642334, 'LinLibertineT'),\n",
              " ('Conv(3,128)', 6.853942394256592, 'LinLibertineT'),\n",
              " ('Conv(3,128)', 6.853942394256592, 'LinLibertineT'),\n",
              " ('Max-', 6.853942394256592, 'LinLibertineT'),\n",
              " ('Pool(2,2)', 6.78641414642334, 'LinLibertineT'),\n",
              " ('Conv(3,256)', 6.853942394256592, 'LinLibertineT'),\n",
              " ('Conv(3,256)', 6.853942394256592, 'LinLibertineT'),\n",
              " ('Conv(3,256)', 6.853942394256592, 'LinLibertineT'),\n",
              " ('Max-', 6.853942394256592, 'LinLibertineT'),\n",
              " ('Pool(2,2)', 6.78641414642334, 'LinLibertineT'),\n",
              " ('Conv(3,512)', 6.853942394256592, 'LinLibertineT'),\n",
              " ('Conv(3,512)', 6.853942394256592, 'LinLibertineT'),\n",
              " ('Conv(3,512)', 6.853942394256592, 'LinLibertineT'),\n",
              " ('Max-', 6.853942394256592, 'LinLibertineT'),\n",
              " ('Pool(2,2)', 6.78641414642334, 'LinLibertineT'),\n",
              " ('Conv(3,512)', 6.853942394256592, 'LinLibertineT'),\n",
              " ('Conv(3,512)', 6.853942394256592, 'LinLibertineT'),\n",
              " ('Conv(3,512)', 6.853942394256592, 'LinLibertineT'),\n",
              " ('Max-', 6.853942394256592, 'LinLibertineT'),\n",
              " ('Pool(2,2)', 6.78641414642334, 'LinLibertineT'),\n",
              " ('FC(4096)', 6.853942394256592, 'LinLibertineT'),\n",
              " ('FC(4096)', 6.853942394256592, 'LinLibertineT'),\n",
              " ('FC(', 6.853942394256592, 'LinLibertineT'),\n",
              " ('N', 7.077282428741455, 'LinLibertineI7'),\n",
              " ('classes', 5.332265377044678, 'LinLibertineT'),\n",
              " (')', 6.853942394256592, 'LinLibertineT'),\n",
              " ('SofMax', 6.78641414642334, 'LinLibertineT'),\n",
              " ('Table 3: Te ResNet-50 architecture [7].',\n",
              "  8.725472450256348,\n",
              "  'LinLibertineTB'),\n",
              " ('Block 1', 6.78641414642334, 'LinLibertineTB'),\n",
              " ('Block 2', 6.78641414642334, 'LinLibertineTB'),\n",
              " ('Block 3', 6.78641414642334, 'LinLibertineTB'),\n",
              " ('Block 4', 6.78641414642334, 'LinLibertineTB'),\n",
              " ('Block 5', 6.78641414642334, 'LinLibertineTB'),\n",
              " ('Block 6', 6.78641414642334, 'LinLibertineTB'),\n",
              " ('Conv(7,64)', 6.718207359313965, 'LinLibertineT'),\n",
              " ('Max-', 6.853942394256592, 'LinLibertineT'),\n",
              " ('Pool(3,2)', 6.78641414642334, 'LinLibertineT'),\n",
              " ('3x[Conv(1,64)', 6.718207359313965, 'LinLibertineT'),\n",
              " ('Conv(3,64)', 6.853942394256592, 'LinLibertineT'),\n",
              " ('Conv(3,256)]', 6.718207359313965, 'LinLibertineT'),\n",
              " ('4x[Conv(1,128)', 6.718207359313965, 'LinLibertineT'),\n",
              " ('Conv(3,128)', 6.853942394256592, 'LinLibertineT'),\n",
              " ('Conv(1,512)]', 6.78641414642334, 'LinLibertineT'),\n",
              " ('6x[Conv(1,256)', 6.718207359313965, 'LinLibertineT'),\n",
              " ('Conv(3,256)', 6.853942394256592, 'LinLibertineT'),\n",
              " ('Conv(1,1024)]', 6.718207359313965, 'LinLibertineT'),\n",
              " ('3x[Conv(1,512)', 6.718207359313965, 'LinLibertineT'),\n",
              " ('Conv(3,512)', 6.853942394256592, 'LinLibertineT'),\n",
              " ('Conv(1,2048)]', 6.718207359313965, 'LinLibertineT'),\n",
              " ('FC(', 6.718207359313965, 'LinLibertineT'),\n",
              " ('N', 7.077282428741455, 'LinLibertineI7'),\n",
              " ('classes', 5.332265377044678, 'LinLibertineT'),\n",
              " (')', 6.718207359313965, 'LinLibertineT'),\n",
              " ('SofMax', 6.78641414642334, 'LinLibertineT'),\n",
              " ('5.1', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('Classifcation results', 10.615971565246582, 'LinLibertineTB'),\n",
              " ...]"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output = scrape(pdf_path)\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vki4B4pbC-Wr",
        "outputId": "e5d22c81-a51a-44eb-bcee-7f1646aabbb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Convolutional Networks and Satellite Imagery to Identify Pa/t_terns in Urban Environments at a Large Scale\n"
          ]
        }
      ],
      "source": [
        "## Validation of correct title extraction\n",
        "if title is None or title == '':\n",
        "      new_title = ''\n",
        "      max_font_size = max(output, key=lambda x: x[1])[1]\n",
        "      elements_with_max_font = [element for element in output if element[1] == max_font_size]\n",
        "\n",
        "      print(\"PDF title is:\\n\")\n",
        "      for element in elements_with_max_font:\n",
        "            new_title += ' ' + element[0]\n",
        "      print(new_title)\n",
        "else:\n",
        "      new_title = title\n",
        "      print(new_title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zR3stimC-Wr",
        "outputId": "cebe1dd0-43e8-47f2-d2a8-469726df55d2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# packages required for text pre-processing\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "4LH8WyKlC-Ws"
      },
      "outputs": [],
      "source": [
        "# Tokenization, lower-case conversion and stop-word removal\n",
        "\n",
        "def preprocess_text(text):\n",
        "\n",
        "  sentences = sent_tokenize(text)  # Split into sentences\n",
        "  tokens = [word_tokenize(sentence.lower()) for sentence in sentences]  # Tokenize each sentence\n",
        "  preprocessed_tokens = []\n",
        "  for sentence_tokens in tokens:\n",
        "    filtered_tokens = [token for token in sentence_tokens if token not in stop_words]  # Remove stop words\n",
        "    preprocessed_tokens.extend(filtered_tokens)\n",
        "  return preprocessed_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLNFgXmxC-Ws",
        "outputId": "7618e91a-339a-4d06-f8fb-aefd7a87cd70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['using', 'convolutional', 'networks', 'satellite', 'imagery', 'identify', 'pa/t_terns', 'urban', 'environments', 'large', 'scale']\n"
          ]
        }
      ],
      "source": [
        "clean_title = preprocess_text(new_title)\n",
        "print(clean_title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRC-sqefC-Ws",
        "outputId": "c67831c8-685f-406f-80d9-75b21a1b4a02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text without punctuations: using convolutional networks satellite imagery identify pa/t_terns urban environments large scale\n"
          ]
        }
      ],
      "source": [
        "# Removing punctuations from title\n",
        "\n",
        "import string\n",
        "\n",
        "def remove_punctuations(text):\n",
        "\n",
        "  punctuations = string.punctuation + \"/_*\"\n",
        "\n",
        "  no_punct_text = \" \".join([c for c in text if c not in punctuations])\n",
        "\n",
        "  return no_punct_text\n",
        "\n",
        "clean_title = remove_punctuations(clean_title)\n",
        "print(f\"Text without punctuations: {clean_title}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oC8kM2OC-Ws"
      },
      "source": [
        "### Extraction of text from other sections of pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvFZidBGC-Wt"
      },
      "source": [
        "##### Extracting section headers using fontsize and fontname attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4-r7M3LC-Wt",
        "outputId": "f1f65d77-7e88-4bfb-c448-cfcd6173b67f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10.615971565246582 LinLibertineTB\n"
          ]
        }
      ],
      "source": [
        "# Determining the font size and font name of 'Abstract' using its index position\n",
        "\n",
        "abstract_index = next(i for i, item in enumerate(output) if item[0] == \"Abstract \" or item[0] == 'ABSTRACT ' or item[0] == \"Abstract\" or item[0] == 'ABSTRACT'  or item[0] == \"Abstract: \")\n",
        "\n",
        "font_size = output[abstract_index][1]\n",
        "font_name = output[abstract_index][2]\n",
        "print(font_size, font_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9vV3EfsC-Wt",
        "outputId": "18143358-83f0-420e-ed81-c07c5a25960a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('ABSTRACT', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('CCS CONCEPTS', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('KEYWORDS', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('1', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('INTRODUCTION', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('2', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('LITERATURE', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('2.1', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('Existing land use benchmark datasets',\n",
              "  10.615971565246582,\n",
              "  'LinLibertineTB'),\n",
              " ('3', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('THE URBAN ENVIRONMENTS DATASET', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('3.1', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('Urban Atlas: a standard in land use analysis',\n",
              "  10.615971565246582,\n",
              "  'LinLibertineTB'),\n",
              " ('3.2', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('Data sampling and acquisition', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('4', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('EXPERIMENTAL SETUP', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('4.1', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('Neural network architectures and training',\n",
              "  10.615971565246582,\n",
              "  'LinLibertineTB'),\n",
              " ('4.2', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('Comparing urban environments', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('5', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('RESULTS AND DISCUSSION', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('5.1', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('Classifcation results', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('5.2', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('Comparing urban environments', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('6', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('CONCLUSIONS', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('A', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('PRACTICAL TRAINING DETAILS.', 10.615971565246582, 'LinLibertineTB')]"
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Filtering out only those items which have similar properties as 'Abstract' and considering them as other section headers\n",
        "\n",
        "filtered_items = [item for item in output if item[1] == font_size and item[2] == font_name]\n",
        "filtered_items = filtered_items[:-1] # Excluding references from headers\n",
        "filtered_items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VP9j6opbC-Wt",
        "outputId": "9b1d8659-9c5e-4a88-d62f-2cc01735e12f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('ABSTRACT', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('CCS CONCEPTS', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('KEYWORDS', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('1', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('INTRODUCTION', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('2', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('LITERATURE', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('2.1', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('Existing land use benchmark datasets',\n",
              "  10.615971565246582,\n",
              "  'LinLibertineTB'),\n",
              " ('3', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('THE URBAN ENVIRONMENTS DATASET', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('3.1', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('Urban Atlas: a standard in land use analysis',\n",
              "  10.615971565246582,\n",
              "  'LinLibertineTB'),\n",
              " ('3.2', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('Data sampling and acquisition', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('4', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('EXPERIMENTAL SETUP', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('4.1', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('Neural network architectures and training',\n",
              "  10.615971565246582,\n",
              "  'LinLibertineTB'),\n",
              " ('4.2', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('Comparing urban environments', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('5', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('RESULTS AND DISCUSSION', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('5.1', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('Classifcation results', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('5.2', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('Comparing urban environments', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('6', 10.615971565246582, 'LinLibertineTB'),\n",
              " ('CONCLUSIONS', 10.615971565246582, 'LinLibertineTB')]"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Removing unwanted parts of section headers (For eg. Author names, Acknowledgements, References etc.)\n",
        "\n",
        "new_abstract_index = next((i for i, item in enumerate(filtered_items) if item[0] == 'Abstract 'or item[0] == 'ABSTRACT'\n",
        "                           or item[0] == 'Abstract' or item[0] == 'ABSTRACT '), None)\n",
        "\n",
        "new_items = filtered_items[new_abstract_index:] if new_abstract_index is not None else filtered_items\n",
        "\n",
        "new_con_index = next((i for i, element in enumerate(new_items) if element[0] == 'CONCLUSIONS' or element[0] == 'CONCLUSIONS '\n",
        "                      or element[0] == 'Conclusions' or element[0] == 'Conclusions ' or element[0] == 'Conclusion'\n",
        "                      or element[0] == 'Conclusion '), None)\n",
        "\n",
        "if new_con_index is not None:\n",
        "    new_items = new_items[:new_con_index + 1]\n",
        "\n",
        "new_items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1esaE2MW2Rjz"
      },
      "source": [
        "##### Formatting the headers to match the actual pdf headers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgJMod05C-Wu",
        "outputId": "c3195921-629b-494c-9c47-a6203e66a702"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ABSTRACT CCS CONCEPTS KEYWORDS', '1 INTRODUCTION', '2 LITERATURE 2.1 Existing land use benchmark datasets', '3 THE URBAN ENVIRONMENTS DATASET 3.1 Urban Atlas: a standard in land use analysis 3.2 Data sampling and acquisition', '4 EXPERIMENTAL SETUP 4.1 Neural network architectures and training 4.2 Comparing urban environments', '5 RESULTS AND DISCUSSION 5.1 Classifcation results 5.2 Comparing urban environments', '6 CONCLUSIONS']\n"
          ]
        }
      ],
      "source": [
        "# Formatting the name of the headers to match the pattern given in original pdfs\n",
        "\n",
        "output = []\n",
        "current_element = ''\n",
        "\n",
        "for i, item in enumerate(new_items):\n",
        "    if i > 0 and item[0].isdigit() and not new_items[i - 1][0].isdigit():\n",
        "\n",
        "        output.append(current_element.strip())\n",
        "        current_element = item[0]\n",
        "    else:\n",
        "        current_element += ' ' + item[0]\n",
        "\n",
        "# Add the last element to the output\n",
        "if current_element:\n",
        "    output.append(current_element.strip())\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eP3SnxZAC-Wu",
        "outputId": "80547295-b5ae-4897-9663-836cc037d084"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ABSTRACT']"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Removing other subsections falling under 'Abstract' for less complication\n",
        "\n",
        "new_output = []\n",
        "\n",
        "if(len(output[0].split(' '))>1):\n",
        "    words = output[0].split(' ')\n",
        "    new_output.append(words[0])\n",
        "\n",
        "else:\n",
        "    new_output = output\n",
        "\n",
        "new_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bL2k8JRSC-Wu",
        "outputId": "b5f3777d-e0f2-4dbb-bdfa-84b2783b3990"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ABSTRACT',\n",
              " '1 INTRODUCTION',\n",
              " '2 LITERATURE',\n",
              " '3 THE URBAN ENVIRONMENTS DATASET',\n",
              " '4 EXPERIMENTAL SETUP',\n",
              " '5 RESULTS AND DISCUSSION',\n",
              " '6 CONCLUSIONS']"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Making a cleaner list of all section headers for individual pdfs\n",
        "\n",
        "for i in range(1,len(output)):\n",
        "  if(output != new_output):\n",
        "    element = output[i].split('.')[0][:-1]\n",
        "    element_new = ''\n",
        "    if(output[i].split('.')[0][-1].isdigit()):\n",
        "        element_new = element\n",
        "    else:\n",
        "        element_new = output[i].split('.')[0]\n",
        "\n",
        "    new_output.append(element_new.rstrip())\n",
        "  else:\n",
        "     pass\n",
        "\n",
        "new_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlF6FvKr8gia"
      },
      "source": [
        "### Extract the entire text of pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "xFaeQnajC-Wt"
      },
      "outputs": [],
      "source": [
        "# Parse the entire text from individual pdf\n",
        "\n",
        "def extract_text_from_pdf(pdf_file_path):\n",
        "    \"\"\"Extracts text from a PDF file and returns it as a string.\"\"\"\n",
        "\n",
        "    with open(pdf_file_path, 'rb') as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        num_pages = len(pdf_reader.pages)\n",
        "\n",
        "        full_text = \"\"\n",
        "        for page_num in range(num_pages):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            page_text = page.extract_text()\n",
        "            full_text += page_text\n",
        "\n",
        "    return full_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tIgUt8zC-Wt",
        "outputId": "dec49a1b-1080-4cad-b38e-31d8f75b3074"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Convolutional Networks and Satellite Imagery to Identify\n",
            "Pa/t_terns in Urban Environments at a Large Scale\n",
            "Adrian Albert∗\n",
            "Massachuse/t_ts Institute of Technology\n",
            "Civil and Environmental Engineering\n",
            "77 Massachuse/t_ts Ave\n",
            "Cambridge, MA 02139\n",
            "adalbert@mit.eduJasleen Kaur\n",
            "Philips Lighting Research North\n",
            "America\n",
            "2 Canal Park\n",
            "Cambridge, MA 02141\n",
            "jasleen.kaur1@philips.comMarta C. Gonz ´alez\n",
            "Massachuse/t_ts Institute of Technology\n",
            "Civil and Environmental Engineering\n",
            "77 Massachuse/t_ts Ave\n",
            "Cambridge, MA 02139\n",
            "martag@mit.edu\n",
            "ABSTRACT\n",
            "Urban planning applications (energy audits, investment, etc.) re-\n",
            "quire an understanding of built infrastructure and its environment,\n",
            "i.e., both low-level, physical features (amount of vegetation, build-\n",
            "ing area and geometry etc.), as well as higher-level concepts such\n",
            "as land use classes (which encode expert understanding of socio-\n",
            "economic end uses). /T_his kind of data is expensive and labor-\n",
            "intensive to obtain, which limits its availability (particularly in\n",
            "developing countries). We analyze pa/t_terns in land use in urban\n",
            "neighborhoods using large-scale satellite imagery data (which is\n",
            "available worldwide from third-party providers) and state-of-the-\n",
            "art computer vision techniques based on deep convolutional neural\n",
            "networks. For supervision, given the limited availability of standard\n",
            "benchmarks for remote-sensing data, we obtain ground truth land\n",
            "use class labels carefully sampled from open-source surveys, in\n",
            "particular the Urban Atlas land classi/f_ication dataset of 20 land use\n",
            "classes across 300 European cities. We use this data to train and\n",
            "compare deep architectures which have recently shown good per-\n",
            "formance on standard computer vision tasks (image classi/f_ication\n",
            "and segmentation), including on geospatial data. Furthermore, we\n",
            "show that the deep representations extracted from satellite imagery\n",
            "of urban environments can be used to compare neighborhoods\n",
            "across several cities. We make our dataset available for other ma-\n",
            "chine learning researchers to use for remote-sensing applications.\n",
            "CCS CONCEPTS\n",
            "•Computing methodologies →Computer vision; Neural net-\n",
            "works; •Applied computing →Environmental sciences;\n",
            "KEYWORDS\n",
            "Satellite imagery, land use classi/f_ication, convolutional networks\n",
            "1 INTRODUCTION\n",
            "Land use classi/f_ication is an important input for applications rang-\n",
            "ing from urban planning, zoning and the issuing of business per-\n",
            "mits, to real-estate construction and evaluation to infrastructure\n",
            "∗Corresponding author.\n",
            "Permission to make digital or hard copies of part or all of this work for personal or\n",
            "classroom use is granted without fee provided that copies are not made or distributed\n",
            "for pro/f_it or commercial advantage and that copies bear this notice and the full citation\n",
            "on the /f_irst page. Copyrights for third-party components of this work must be honored.\n",
            "For all other uses, contact the owner/author(s).\n",
            "KDD’17, August 13–17, 2017, Halifax, NS, Canada.\n",
            "© 2017 Copyright held by the owner/author(s). 978-1-4503-4887-4/17/08.\n",
            "DOI: h/t_tp://dx.doi.org/10.1145/3097983.3098070development. Urban land use classi/f_ication is typically based on\n",
            "surveys performed by trained professionals. As such, this task\n",
            "is labor-intensive, infrequent, slow, and costly. As a result, such\n",
            "data are mostly available in developed countries and big cities that\n",
            "have the resources and the vision necessary to collect and curate it;\n",
            "this information is usually not available in many poorer regions,\n",
            "including many developing countries [ 9] where it is mostly needed.\n",
            "/T_his paper builds on two recent trends that promise to make\n",
            "the analysis of urban environments a more democratic and inclu-\n",
            "sive task. On the one hand, recent years have seen signi/f_icant\n",
            "improvements in satellite technology and its deployment (primar-\n",
            "ily through commercial operators), which allows to obtain high\n",
            "and medium-resolution imagery of most urbanized areas of the\n",
            "Earth with an almost daily revisit rate. On the other hand, the\n",
            "recent breakthroughs in computer vision methods, in particular\n",
            "deep learning models for image classi/f_ication and object detection,\n",
            "now make possible to obtain a much more accurate representation\n",
            "of the composition built infrastructure and its environments.\n",
            "Our contributions are to both the applied deep learning literature,\n",
            "and to the incipient study of “smart cities” using remote sensing\n",
            "data. We contrast state-of-the-art convolutional architectures (the\n",
            "VGG-16 [ 19] and ResNet [ 7] networks) to train classi/f_iers that recog-\n",
            "nize broad land use classes from satellite imagery. We then use the\n",
            "features extracted from the model to perform a large-scale compar-\n",
            "ison of urban environments. For this, we construct a novel dataset\n",
            "for land use classi/f_ication, pairing carefully sampled locations with\n",
            "ground truth land use class labels obtained from the Urban Atlas\n",
            "survey [ 22] with satellite imagery obtained from Google Maps’s\n",
            "static API. Our dataset - which we have made available publicly\n",
            "for other researchers - covers, for now, 10 cities in Europe (chosen\n",
            "out of the original 300) with 10 land use classes (from the original\n",
            "20). As the Urban Atlas is a widely-used, standardized dataset for\n",
            "land use classi/f_ication, we hope that making this dataset available\n",
            "will encourage the development analyses and algorithms for ana-\n",
            "lyzing the built infrastructure in urban environments. Moreover,\n",
            "given that satellite imagery is available virtually everywhere on\n",
            "the globe, the methods presented here allow for automated, rapid\n",
            "classi/f_ication of urban environments that can potentially be applied\n",
            "to locations where survey and zoning data is not available.\n",
            "Land use classi/f_ication refers to the combination of physical\n",
            "land a/t_tributes and what cultural and socio-economic function land\n",
            "serves (which is a subjective judgement by experts) [ 2]. In this paper,\n",
            "we take the view that land use classes are just a useful discretization\n",
            "of a more continuous spectrum of pa/t_terns in the organization of\n",
            "urban environments. /T_his viewpoint is illustrated in Figure 2: while\n",
            "arXiv:1704.02965v2  [cs.CV]  13 Sep 2017Figure 1: Urban land use maps for six example cities. We compare the ground truth ( top row ) with the predicted land use maps,\n",
            "either from using separate data collected from the same city ( middle row ), or using data from all other cities ( bo/t_tom row ).\n",
            "Figure 2: Le/f_t: Comparing urban environments via deep hi-\n",
            "erarchical representations of satellite image samples. Right:\n",
            "approach outline - data collection, classi/f_ication, feature ex-\n",
            "traction, clustering, validation.\n",
            "some a/t_tributes (e.g., amount of built structures or vegetation) are\n",
            "directly interpretable, some others may not be. Nevertheless, these\n",
            "pa/t_terns in/f_luence, and are in/f_luenced by, socio-economic factors\n",
            "(e.g., economic activity), resource use (energy), and dynamic human\n",
            "behavior (e.g., mobility, building occupancy). We see the work\n",
            "on cheaply curating a large-scale land use classi/f_ication dataset\n",
            "and comparing neighborhoods using deep representations that\n",
            "this paper puts forth as a necessary /f_irst step towards a granular\n",
            "understanding of urban environments in data-poor regions.\n",
            "Subsequently, in Section 2 we review related studies that apply\n",
            "deep learning methods and other machine learning techniques\n",
            "to problems of land use classi/f_ication, object detection, and image\n",
            "segmentation in aerial imagery. In Section 3 we describe the datasetwe curated based on the Urban Atlas survey. Section 4 reviews the\n",
            "deep learning architectures we used. Section 5 describes model\n",
            "validation and analysis results. We conclude in Section 6.\n",
            "All the code used to acquire, process, and analyze the data, as\n",
            "well as to train the models discussed in this paper is available at\n",
            "h/t_tp://www.github.com/adrianalbert/urban-environments.\n",
            "2 LITERATURE\n",
            "/T_he literature on the use of remote sensing data for applications in\n",
            "land use cover, urban planning, environmental science, and others,\n",
            "has a long and rich history. /T_his paper however is concerned more\n",
            "narrowly with newer work that employs widely-available data\n",
            "and machine learning models - and in particular deep learning\n",
            "architectures - to study urban environments.\n",
            "Deep learning methods have only recently started to be deployed\n",
            "to the analysis of satellite imagery. As such, land use classi/f_ication\n",
            "using these tools is still a very incipient literature. Probably the /f_irst\n",
            "studies (yet currently only 1-2 years old) include the application\n",
            "of convolutional neural networks to land use classi/f_ication [ 2] us-\n",
            "ing the UC Merced land use dataset [ 25] (of 2100 images spanning\n",
            "21 classes) and the classi/f_ication of agricultural images of coﬀee\n",
            "plantations [ 17]. Similar early studies on land use classi/f_ication\n",
            "that employ deep learning techniques are [ 21], [18], and [ 15]. In\n",
            "[11], a spatial pyramid pooling technique is employed for land use\n",
            "classi/f_ication using satellite imagery. /T_he authors of these studies\n",
            "adapted architectures pre-trained to recognize natural images from\n",
            "the ImageNet dataset (such as the VGG16 [ 19], which we also use),\n",
            "and /f_ine-tuned them on their (much smaller) land use data. More\n",
            "recent studies use the DeepSat land use benchmark dataset [ 1],\n",
            "which we also use and describe in more detail in Section 2.1. An-\n",
            "other topic that is closely related to ours is remote-sensing image\n",
            "segmentation and object detection, where modern deep learning\n",
            "models have also started to be applied. Some of the earliest work\n",
            "that develops and applies deep neural networks for this tasks is thatof [13]. Examples of recent studies include [ 26] and [ 12], where the\n",
            "authors propose a semantic image segmentation technique com-\n",
            "bining texture features and boundary detection in an end-to-end\n",
            "trainable architecture.\n",
            "Remote-sensing data and deep learning methods have been put\n",
            "to use to other related ends, e.g., geo-localization of ground-level\n",
            "photos via satellite images [ 3,24] or predicting ground-level scene\n",
            "images from corresponding aerial imagery [ 27]. Other applications\n",
            "have included predicting survey estimates on poverty levels in\n",
            "several countries in Africa by /f_irst learning to predict levels of night\n",
            "lights (considered as proxies of economic activity and measured\n",
            "by satellites) from day-time, visual-range imagery from Google\n",
            "Maps, then transferring the learning from this la/t_ter task to the\n",
            "former [ 9]. Our work takes a similar approach, in that we aim to use\n",
            "remote-sensing data (which is widely-available for most parts of\n",
            "the world) to infer land use types in those locations where ground\n",
            "truth surveys are not available.\n",
            "Urban environments have been analyzed using other types of\n",
            "imagery data that have become recently available. In [ 4,14], the\n",
            "authors propose to use the same type of imagery from Google Street\n",
            "View to measure the relationship between urban appearance and\n",
            "quality of life measures such as perceived safety. For this, they\n",
            "hand-cra/f_t standard image features widely used in the computer\n",
            "vision community, and train a shallow machine learning classi/f_ier\n",
            "(a support vector machine). In a similar fashion, [ 5] trained a\n",
            "convolutional neural network on ground-level Street View imagery\n",
            "paired with a crowd-sourced mechanism for collecting ground truth\n",
            "labels to predict subjective perceptions of urban environments such\n",
            "as “beauty”, “wealth”, and “liveliness”.\n",
            "Land use classi/f_ication has been studied with other new data\n",
            "sources in recent years. For example, ground-level imagery has been\n",
            "employed to accurately predict land use classes on an university\n",
            "campus [ 28]. Another related literature strand is work that uses\n",
            "mobile phone call records to extract spatial and temporal mobility\n",
            "pa/t_terns, which are then used to infer land use classes for several\n",
            "cities [ 6,10,20]. Our work builds on some of the ideas for sampling\n",
            "geospatial data presented there.\n",
            "2.1 Existing land use benchmark datasets\n",
            "Public benchmark data for land use classi/f_ication using aerial im-\n",
            "agery are still in relatively short supply. Presently there are two\n",
            "such datasets that we are aware of, discussed below.\n",
            "UC Merced. /T_his dataset was published in 2010 [ 25] and con-\n",
            "tains 2100 256 ×256, 1 m/pxaerial RGB images over 21 land use\n",
            "classes. It is considered a “solved problem”, as modern neural net-\n",
            "work based classi/f_iers [2] have achieved >95% accuracy on it.\n",
            "DeepSat. /T_he DeepSat [ 1] dataset1was released in 2015. It\n",
            "contains two benchmarks: the Sat-4 data of 500 ,000 images over 4\n",
            "land use classes ( barren land, trees, grassland, other ), and the Sat-6\n",
            "data of 405 ,000 images over 6 land use classes ( barren land, trees,\n",
            "grassland, roads, buildings, water bodies ). All the samples are 28 ×28\n",
            "in size at a 1 m/pxspatial resolution and contain 4 channels (red,\n",
            "green, blue, and NIR - near infrared). Currently less than two years\n",
            "old, this dataset is already a “solved problem”, with previous studies\n",
            "[15] (and our own experiments) achieving classi/f_ication accuracies\n",
            "1Available at h/t_tp://csc.lsu.edu/ ∼saikat/deepsat/.of over 99% using convolutional architectures. While useful as input\n",
            "for pre-training more complex models, (e.g., image segmentation),\n",
            "this dataset does not allow to take the further steps for detailed\n",
            "land use analysis and comparison of urban environments across\n",
            "cities, which gap we hope our dataset will address.\n",
            "Other open-source eﬀorts. /T_here are several other projects\n",
            "that we are aware of related to land use classi/f_ication using open-\n",
            "source data. /T_he TerraPa/t_tern2project uses satellite imagery from\n",
            "Google Maps (just like we do) paired with truth labels over a large\n",
            "number (450) of detailed classes obtained using the Open Street\n",
            "Map API3. (Open Street Maps is a comprehensive, open-access,\n",
            "crowd-sourced mapping system.) /T_he project’s intended use is as\n",
            "a search tool for satellite imagery, and as such, the classes they\n",
            "employ are very speci/f_ic, e.g., baseball diamonds, churches, or\n",
            "roundabouts. /T_he authors use a ResNet architecture [ 7] to train a\n",
            "classi/f_ication model, which they use to embed images in a high-\n",
            "dimensional feature space, where “similar” images to an input image\n",
            "can be identi/f_ied. A second open-source project related to ours is\n",
            "the DeepOSM4, in which the authors take the same approach of\n",
            "pairing OpenStreetMap labels with satellite imagery obtained from\n",
            "Google Maps, and use a convolutional architecture for classi/f_ication.\n",
            "/T_hese are excellent starting points from a practical standpoint,\n",
            "allowing interested researchers to quickly familiarize themselves\n",
            "with programming aspects of data collection, API calls, etc.\n",
            "3 THE URBAN ENVIRONMENTS DATASET\n",
            "3.1 Urban Atlas: a standard in land use analysis\n",
            "/T_he Urban Atlas [ 22] is an open-source, standardized land use\n",
            "dataset that covers ∼300 European cities of 100 ,000 inhabitants or\n",
            "more, distributed relatively evenly across major geographical and\n",
            "geopolitical regions. /T_he dataset was created between 2005-2011 as\n",
            "part of a major eﬀort by the European Union to provide a uniform\n",
            "framework for the geospatial analysis of urban areas in Europe.\n",
            "Land use classi/f_ication is encoded via detailed polygons organized\n",
            "in commonly-used GIS/ESRI shape /f_iles. /T_he dataset covers 20\n",
            "standardized land use classes. In this work we selected classes of\n",
            "interest and consolidated them into 10 /f_inal classes used for analysis\n",
            "(see Figure 3). Producing the original Urban Atlas dataset required\n",
            "fusing several data sources: high and medium-resolution satellite\n",
            "imagery, topographic maps, navigation and road layout data, and\n",
            "local zoning (cadastral) databases. More information on the method-\n",
            "ology used by the Urban Atlas researchers can be obtained from\n",
            "the European Environment Agency5. We chose expressly to use\n",
            "the Urban Atlas dataset over other sources (described in Section 2.1\n",
            "because i)it is a comprehensive and consistent survey at a large\n",
            "scale, which has been extensively curated by experts and used in\n",
            "research, planning, and socio-economic work over the past decade,\n",
            "andii)the land use classes re/f_lect higher-level (socio-economic,\n",
            "cultural) functions of the land as used in applications.\n",
            "We note that there is a wide variance in the distribution of land\n",
            "use classes across and within the 300 cities. Figure 3 illustrates\n",
            "the diﬀerences in the distribution in ground truth polygon areas\n",
            "2h/t_tp://www.terrapa/t_tern.com/\n",
            "3h/t_tp://www.openstreetmap.org\n",
            "4h/t_tps://github.com/trailbehind/DeepOSM\n",
            "5h/t_tp://www.eea.europa.eu/data-and-maps/data/urban-atlas/Figure 3: Ground truth land use distribution (by area) for\n",
            "three example cities in the Urban Environments dataset.\n",
            "for each of the classes for three example cities (Budapest, Rome,\n",
            "Barcelona) from the dataset (from Eastern, Central, and Western\n",
            "Europe, respectively). /T_his wide disparity in the spatial distribution\n",
            "pa/t_terns of diﬀerent land use classes and across diﬀerent cities\n",
            "motivates us to design a careful sampling procedure for collecting\n",
            "training data, described in detail below.\n",
            "3.2 Data sampling and acquisition\n",
            "We set out to develop a strategy to obtain high-quality samples\n",
            "of the type (satellite image, ground truth label) to use in training\n",
            "convolutional architectures for image classi/f_ication. Our /f_irst re-\n",
            "quirement is to do this solely with freely-available data sources,\n",
            "as to keep costs very low or close to zero. For this, we chose to\n",
            "use the Google Maps Static API6as a source of satellite imagery.\n",
            "/T_his service allows for 25 ,000 API requests/day free of charge. For\n",
            "a given sampling location given by (latitude, longitude), we ob-\n",
            "tained 224 ×224 images at a zoom level 17 (around 1 .20m/pxspatial\n",
            "resolution, or ∼250m×250mcoverage for an image).\n",
            "/T_he goals of our sampling strategy are twofold. First, we want\n",
            "to ensure that the resulting dataset is as much as possible balanced\n",
            "with respect to the land use classes. /T_he challenge is that the classes\n",
            "are highly imbalanced among the ground truth polygons in the\n",
            "dataset (e.g., many more polygons are agricultural land and isolated\n",
            "structures than airports). Second, the satellite images should be\n",
            "representative of the ground truth class associated to them. To this\n",
            "end, we require that the image contain at least 25% (by area) of\n",
            "the associated ground truth polygon. /T_hus, our strategy to obtain\n",
            "training samples is as follows (for a given city):\n",
            "•Sort ground truth polygons in decreasing order according to\n",
            "their size, and retain only those polygons with areas larger than\n",
            "1\n",
            "4(224×1.2m)2=0.06km2;\n",
            "•From each decile of the distribution of areas, sample a propor-\n",
            "tionally larger number of polygons, such that some of the smaller\n",
            "polygons also are picked, and more of the larger ones;\n",
            "•For each picked polygon, sample a number of images propor-\n",
            "tional to the area of the polygon, and assign each image the\n",
            "polygon class as ground truth label;\n",
            "6h/t_tps://developers.google.com/maps/documentation/static-maps/\n",
            "Figure 4: Example satellite images for the original land use\n",
            "classes in the Urban Atlas dataset.\n",
            "Example satellite images for each of the 10 land use classes in\n",
            "the Urban Environments dataset are given in Figure 4. Note the\n",
            "signi/f_icant variety (in color schemes, textures, etc) in environments\n",
            "denoted as having the same land use class. /T_his is because of several\n",
            "factors, including the time of the year when the image was acquired\n",
            "(e.g., agricultural lands appear diﬀerent in the spring than in the\n",
            "fall), the diﬀerent physical form and appearance of environments\n",
            "that serve the same socioeconomic or cultural function (e.g., green\n",
            "urban areas may look very diﬀerent in diﬀerent cities or in even\n",
            "in diﬀerent parts of the same city; what counts as “dense urban\n",
            "fabric” in one city may not be dense at all in other cities), and\n",
            "change in the landscape during the several years that have passed\n",
            "since the compilation of the Urban Atlas dataset and the time of\n",
            "acquisition of the satellite image (e.g., construction sites may not\n",
            "re/f_lect accurately anymore the reality on the ground).\n",
            "Apart from these training images, we constructed ground truth\n",
            "rasters to validate model output for each city. For that, we de/f_ined\n",
            "uniform validation grids of 100 ×100 (25km×25km )around the\n",
            "(geographical) center of a given city of interest. We take a satellite\n",
            "image sample in each grid cell, and assign to it as label the class\n",
            "of the polygon that has the maximum intersection area with that\n",
            "cell. Examples of land use maps for the six cities we analyze here\n",
            "are given in Figure 1 (top row). /T_here, each grid cell is assigned the\n",
            "class of the ground truth polygon whose intersection with the cell\n",
            "has maximum coverage fraction by area. Classes are color-coded\n",
            "following the original Urban Atlas documentation.\n",
            "In Table 1 we present summaries of the training (le/f_t) and vali-\n",
            "dation (right) datasets we used for the analysis in this paper. /T_he\n",
            "validation dataset consists of the images sampled at the centers of\n",
            "each cell in the 25 km×25kmgrid as discussed above. /T_his dataset\n",
            "consists of ∼140,000 images distributed across 10 urban environ-\n",
            "ment classes from 6 cities: Roma (Rome), Madrid, Berlin, Budapest,\n",
            "Barcelona, and Athina (Athens). Because of the high variation in\n",
            "appearance upon visual inspection, we chose to consolidate severalclasses from the original dataset, in particular classes that indicated\n",
            "urban fabric into “High Density Urban Fabric”, “Medium Density\n",
            "Urban Fabric, and “Low Density Urban Fabric”. As mentioned above\n",
            "and illustrated in Figure 3, we did notice a great disparity in the\n",
            "numbers and distribution of ground truth polygons for other ex-\n",
            "ample cities that we investigated in the Urban Atlas dataset. As\n",
            "such,for the analysis in this paper, we have chosen cities where\n",
            "enough ground truth polygons were available for each class (that\n",
            "is, at least 50 samples) to allow for statistical comparisons.\n",
            "4 EXPERIMENTAL SETUP\n",
            "4.1 Neural network architectures and training\n",
            "For all experiments in this paper we compared the VGG-16 [ 19]\n",
            "and ResNet [7, 8] architectures.\n",
            "VGG-16. /T_his architecture [ 19] has become one of the most pop-\n",
            "ular models in computer vision for classi/f_ication and segmentation\n",
            "tasks. It consists of 16 trainable layers organized in blocks. It starts\n",
            "with a 5-block convolutional base of neurons with 3 ×3 receptive\n",
            "/f_ields (alternated with max-pooling layers that eﬀectively increase\n",
            "the receptive /f_ield of neurons further downstream). Following each\n",
            "convolutional layer is a ReLU activation function [ 19]. /T_he feature\n",
            "maps thus obtained are fed into a set of fully-connected layers (a\n",
            "deep neural network classi/f_ier). See Table 2 for a summary.\n",
            "ResNet. /T_his architecture [ 7,8] has achieved state-of-the-art\n",
            "performance on image classi/f_ication on several popular natural\n",
            "image benchmark datasets. It consists of blocks of convolutional\n",
            "layers, each of which is followed by a ReLU non-linearity. As before,\n",
            "each block in the convolutional base is followed by a max-pooling\n",
            "operation. Finally, the output of the last convolutional layer serves\n",
            "as input feature map for a fully-connected layer with a so/f_tmax\n",
            "activation function. /T_he key diﬀerence in this architecture is that\n",
            "shortcut connections are implemented that skip blocks of convo-\n",
            "lutional layers, allowing the network to learn residual mappings\n",
            "between layer input and output. Here we used an implementation\n",
            "with 50 trainable layers per [7]. See Table 3 for a summary.\n",
            "Transfer learning. As it is common practice in the literature,\n",
            "we have experimented with training our models on the problem of\n",
            "interest (urban environment classi/f_ication) starting from architec-\n",
            "tures pre-trained on datasets from other domains ( transfer learning ).\n",
            "/T_his procedure has been shown to yield both be/t_ter performance\n",
            "and faster training times, as the network already has learned to\n",
            "recognize basic shapes and pa/t_terns that are characteristic of im-\n",
            "ages across many domains (e.g., [ 9,12,15]). We have implemented\n",
            "the following approaches: 1)we used models pre-trained on the\n",
            "ImageNet dataset, then further /f_ine-tuned them on the Urban Atlas\n",
            "dataset; and 2)we pre-trained on the DeepSat dataset (See Section\n",
            "2), then further re/f_ined on the Urban Atlas dataset. As expected,\n",
            "the la/t_ter strategy - /f_irst training a model (itself pre-trained on Ima-\n",
            "geNet data) on the DeepSat benchmark, and the further re/f_ining\n",
            "on the Urban Atlas dataset - yielded the best results, achieving\n",
            "increases of around 5% accuracy for a given training time.\n",
            "Given the large amount of variation in the visual appearance\n",
            "of urban environments across diﬀerent cities (because of diﬀerent\n",
            "climates, diﬀerent architecture styles, various other socio-economic\n",
            "factors), it is of interest to study to what extent a model learned on\n",
            "one geographical location can be applied to a diﬀerent geographicallocation. As such, we perform experiments in which we train a\n",
            "model for one (or more) cities, then apply the model to a diﬀerent set\n",
            "of cities. Intuitively, one would expect that, the more neighborhoods\n",
            "and other urban features at one location are similar to those at a\n",
            "diﬀerent location, the be/t_ter learning would transfer, and the higher\n",
            "the classi/f_ication accuracy obtained would be. Results for these\n",
            "experiments are summarized in Figure 6.\n",
            "4.2 Comparing urban environments\n",
            "We next used the convolutional architectures to extract features\n",
            "for validation images. As in other recent studies (e.g., [ 9]), we use\n",
            "the last layer of a network as feature extractor. /T_his amounts to\n",
            "feature vectors of D=4096 dimensions for the VGG16 architecture\n",
            "andD=2048 dimensions for the ResNet-50 architecture. /T_he\n",
            "codes x∈RDare the image representations that either network\n",
            "derives as most representative to discriminate the high-level land\n",
            "use concepts it is trained to predict.\n",
            "We would like to study how ”similar” diﬀerent classes of urban\n",
            "environments are across two example cities (here we picked Berlin\n",
            "and Barcelona, which are fairly diﬀerent from a cultural and archi-\n",
            "tectural standpoint). For this, we focus only on the 25 km×25km,\n",
            "100×100-cell grids around the city center as in Figure 1. To be able\n",
            "to quantify similarity in local urban environments, we construct\n",
            "a KD-tree T(using a high-performance implementation available\n",
            "in the Python package scikit-learn [16]) using all the gridded\n",
            "samples. /T_his data structure allows to /f_ind k-nearest neighbors of a\n",
            "query image in an eﬃcient way. In this way, the feature space can\n",
            "be probed in an eﬃcient way.\n",
            "5 RESULTS AND DISCUSSION\n",
            "In Figure 1 we show model performance on the 100 ×100 (25 km×\n",
            "25km) raster grids we used for testing. /T_he top row shows ground\n",
            "truth grids, where the class in each cell was assigned as the most\n",
            "prevalent land use class by area (see also Section 3). /T_he bo/t_tom row\n",
            "shows model predictions, where each cell in a raster is painted in\n",
            "the color corresponding to the maximum probability class estimated\n",
            "by the model (here ResNet-50). Columns in the /f_igure show results\n",
            "for each of the 6 cities we used in our dataset. Even at a /f_irst visual\n",
            "inspection, the model is able to recreate from satellite imagery\n",
            "qualitatively the urban land use classi/f_ication map.\n",
            "Further, looking at the individual classes separately and the con-\n",
            "/f_idence of the model in its predictions (the probability distribution\n",
            "over classes computed by the model), the picture is again qualita-\n",
            "tively very encouraging. In Figure 5 we show grayscale raster maps\n",
            "encoding the spatial layout of the class probability distribution for\n",
            "one example city, Barcelona. Particularly good qualitative agree-\n",
            "ment is observed for agricultural lands, water bodies, industrial,\n",
            "public, and commercial land, forests, green urban areas, low density\n",
            "urban fabric, airports, and sports and leisure facilities. /T_he model\n",
            "appears to struggle with reconstructing the spatial distribution of\n",
            "roads, which is not unexpected, given that roads typically appear\n",
            "in many other scenes that have a diﬀerent functional classi/f_ication\n",
            "for urban planning purposes.Table 1: Urban Environments dataset: sample size summary.\n",
            "(a) Dataset used for training & validation ( 80%and20%, respectively)\n",
            "class/city athina barcelona berlin budapest madrid roma class\n",
            "total\n",
            "Agricultural + Semi-\n",
            "natural areas + Wet-\n",
            "lands4347 2987 7602 2211 4662 4043 25852\n",
            "Airports 382 452 232 138 124 142 1470\n",
            "Forests 1806 2438 7397 1550 2685 2057 17933\n",
            "Green urban areas 990 722 1840 1342 1243 1401 7538\n",
            "High Density Urban\n",
            "Fabric967 996 8975 6993 2533 3103 23567\n",
            "Industrial, commer-\n",
            "cial, public, military\n",
            "and pr…1887 2116 4761 1850 3203 2334 16151\n",
            "Low Density Urban\n",
            "Fabric1424 1520 2144 575 2794 3689 12146\n",
            "Medium Density Ur-\n",
            "ban Fabric2144 1128 6124 1661 1833 2100 14990\n",
            "Sports and leisure fa-\n",
            "cilities750 1185 2268 1305 1397 1336 8241\n",
            "Water bodies 537 408 1919 807 805 619 5095\n",
            "city total 15234 13952 43262 18432 21279 20824 132983(b)25km×25kmground truth test grids (fractions of city total)\n",
            "class / city athina barcelona berlin budapest madrid roma\n",
            "Agricultural + Semi-\n",
            "natural areas + Wet-\n",
            "lands0.350 0.261 0.106 0.181 0.395 0.473\n",
            "Airports 0.003 0.030 0.013 0.000 0.044 0.006\n",
            "Forests 0.031 0.192 0.087 0.211 0.013 0.019\n",
            "Green urban areas 0.038 0.030 0.072 0.027 0.125 0.054\n",
            "High Density Urban\n",
            "Fabric0.389 0.217 0.284 0.365 0.170 0.215\n",
            "Industrial, commer-\n",
            "cial, public, military\n",
            "and pr…0.109 0.160 0.190 0.096 0.138 0.129\n",
            "Low Density Urban\n",
            "Fabric0.016 0.044 0.012 0.006 0.036 0.029\n",
            "Medium Density Ur-\n",
            "ban Fabric0.041 0.025 0.129 0.045 0.042 0.047\n",
            "Sports and leisure fa-\n",
            "cilities0.017 0.034 0.080 0.025 0.036 0.025\n",
            "Water bodies 0.005 0.006 0.026 0.044 <0.001 0.004\n",
            "Figure 5: Barcelona: ground truth ( top) and predicted probabilities ( bo/t_tom ).\n",
            "Table 2: /T_he VGG16 architecture [19].\n",
            "Block 1 Block 2 Block 3 Block 4 Block 5 Block 6\n",
            "Conv(3,64)\n",
            "Conv(3,64)\n",
            "Max-\n",
            "Pool(2,2)Conv(3,128)\n",
            "Conv(3,128)\n",
            "Max-\n",
            "Pool(2,2)Conv(3,256)\n",
            "Conv(3,256)\n",
            "Conv(3,256)\n",
            "Max-\n",
            "Pool(2,2)Conv(3,512)\n",
            "Conv(3,512)\n",
            "Conv(3,512)\n",
            "Max-\n",
            "Pool(2,2)Conv(3,512)\n",
            "Conv(3,512)\n",
            "Conv(3,512)\n",
            "Max-\n",
            "Pool(2,2)FC(4096)\n",
            "FC(4096)\n",
            "FC(Nclasses )\n",
            "So/f_tMax\n",
            "Table 3: /T_he ResNet-50 architecture [7].\n",
            "Block 1 Block 2 Block 3 Block 4 Block 5 Block 6\n",
            "Conv(7,64)\n",
            "Max-\n",
            "Pool(3,2)3x[Conv(1,64)\n",
            "Conv(3,64)\n",
            "Conv(3,256)]4x[Conv(1,128)\n",
            "Conv(3,128)\n",
            "Conv(1,512)]6x[Conv(1,256)\n",
            "Conv(3,256)\n",
            "Conv(1,1024)]3x[Conv(1,512)\n",
            "Conv(3,512)\n",
            "Conv(1,2048)]FC(Nclasses )\n",
            "So/f_tMax\n",
            "5.1 Classi/f_ication results\n",
            "We performed experiments training the two architectures described\n",
            "in Section 4 on datasets for each of the 6 cities considered, and for\n",
            "a combined dataset ( all) of all the cities. /T_he diagonal in Figure 6\n",
            "summarizes the (validation set) classi/f_ication performance for each\n",
            "model. All /f_igures are averages computed over balanced subsets\n",
            "of 2000 samples each. While accuracies or ∼0.70−0.80 may not\n",
            "look as impressive as those obtained by convolutional architectures\n",
            "on well-studied benchmarks and other classi/f_ication tasks (e.g.,natural images from ImageNet or small aerial patches from Deep-\n",
            "Sat), this only a/t_tests to the diﬃculty of the task of understanding\n",
            "high-level, subjective concepts of urban planning in complex ur-\n",
            "ban environments. First, satellite imagery typically contains much\n",
            "more semantic variation than natural images (as also noted, e.g.,\n",
            "in [2,13]), i.e., there is no “central” concept that the image is of\n",
            "(unlike the image of a cat or a /f_lower). Second, the type of labels we\n",
            "use for supervision are higher-level concepts (such as “low density\n",
            "urban fabric”, or “sports and leisure facilities”), which are much\n",
            "less speci/f_ic than more physical land features e.g., “buildings” or\n",
            "”trees” (which are classes used in the DeepSat dataset). Moreover,\n",
            "top-down imagery poses speci/f_ic challenges to convolutional archi-\n",
            "tectures, as these models are inherently not rotationally-symmetric.\n",
            "Urban environments, especially from from a top-down point of\n",
            "view, come in many complex layouts, for which rotations are ir-\n",
            "relevant. Nevertheless, these results are encouraging, especially\n",
            "since this is a harder problem by focusing on wider-area images and\n",
            "on higher-level, subjective concepts used in urban planning rather\n",
            "than on the standard, lower-level physical features such as in [ 1] or\n",
            "[17]. /T_his suggests that such models may be useful feature extrac-\n",
            "tors. Moreover, as more researchers tackle problems with the aid of\n",
            "satellite imagery (which is still a relatively under-researched source\n",
            "of data in the machine learning community), more open-sourceFigure 6: Transferability (classi/f_ication accuracy) of models\n",
            "learned at one location and applied at another. Training on\n",
            "a more diverse set of cities ( all) yields encouraging results\n",
            "compared with just pairwise training/testing.\n",
            "datasets (like this one) are released, performance will certainly im-\n",
            "prove. For the remainder of this section we report results using\n",
            "the ResNet-50 architecture [ 7], as it consistently yielded (if only\n",
            "slightly) be/t_ter classi/f_ication results in our experiments than the\n",
            "VGG-16 architecture.\n",
            "Transfer learning and classi/f_ication performance. Next,\n",
            "we investigated how models trained in one se/t_ting (city or set of\n",
            "cities) perform when applied to other geographical locations. Figure\n",
            "6 summarizes these experiments. In general, performance is poor\n",
            "when training on samples from a given city and testing on samples\n",
            "from a diﬀerent city (the oﬀ-diagonal terms). /T_his is expected, as\n",
            "these environments can be very diﬀerent in appearance for cities as\n",
            "diﬀerent as e.g., Budapest and Barcelona. However, we notice that\n",
            "a more diverse set ( all) yields be/t_ter performance when applied at\n",
            "diﬀerent locations than models trained on individual cities. /T_his is\n",
            "encouraging for our purpose of analyzing the high level “similarity”\n",
            "of urban neighborhoods via satellite imagery.\n",
            "We next looked at per-class model performance to understand\n",
            "what types of environments are harder for the model to distin-\n",
            "guish. Figure 7 shows such an example analysis for three example\n",
            "cities, of which a pair is “similar” according to Figure 6 (Rome and\n",
            "Barcelona), and another dissimilar (Rome and Budapest). /T_he le/f_t\n",
            "panel shows model performance when training on samples from\n",
            "Barcelona, and predicting on test samples from Barcelona (intra-\n",
            "city). /T_he middle panel shows training on Rome, and predicting\n",
            "on test samples in Barcelona, which can be assumed to be “sim-\n",
            "ilar” to Rome from a cultural and architectural standpoint (both\n",
            "Latin cities in warm climates). /T_he right /f_igure shows training on\n",
            "Barcelona, and predicting on test samples in Budapest, which can\n",
            "be assumed a rather diﬀerent city from a cultural and architectural\n",
            "standpoint. For all cases, the classes that the model most struggles\n",
            "with are “High Density Urban Fabric”, “Low Density Urban Fabric,\n",
            "and “Medium Density Urban Fabric”. Considerable overlap can be\n",
            "noticed between these classes - which is not surprising given the\n",
            "highly subjective nature of these concepts. Other examples where\n",
            "the model performance is lower is forests and low-density urban\n",
            "areas being sometimes misclassifed as “green urban areas”, which,again, is not surprising. /T_his is especially apparent in the cross-city\n",
            "case, where the model struggles with telling apart these classes. For\n",
            "both the case of training and testing on “diﬀerent cities” (Budapest\n",
            "and Barcelona) and on “similar” cities (Rome and Barcelona), we\n",
            "note that airports and forests are relatively easier to distinguish.\n",
            "However, more subjective, high-level urban-planning concepts such\n",
            "as “high density urban fabric” are harder to infer (and more easily\n",
            "confused with “medium density” or “low density” urban fabric) in\n",
            "the case of more similar cities (Rome and Barcelona) rather than\n",
            "dissimilar cities (Budapest and Barcelona). Urban environments\n",
            "containing sports and leisure facilities and green areas are under\n",
            "this view more similar between Rome and Barcelona than they are\n",
            "between Budapest and Barcelona.\n",
            "Choosing the spatial scale: sensitivity analysis. So far, we\n",
            "have presented results assuming that tiles of 250 mis an appropriate\n",
            "spatial scale for this analysis. Our intuition suggested that tiles of\n",
            "this size have enough variation and information to be recognized\n",
            "(even by humans) as belonging to one of the high-level concepts\n",
            "of land use classes that we study in this paper. However, one\n",
            "can /f_ind arguments in favor of smaller tile sizes, e.g., in many\n",
            "cities the size of a typical city block is 100 m. /T_hus, we trained\n",
            "models at diﬀerent spatial scales and computed test-set accuracy\n",
            "values for three example cities, Barcelona, Roma, and Budapest\n",
            "- see Figure 8. It is apparent that, for all example cities, smaller\n",
            "spatial scales (50 m, 100m, 150m) that we analyzed yield poorer\n",
            "performance than the scale we chose for the analysis in this paper\n",
            "(250m). /T_his is likely because images at smaller scales do not capture\n",
            "enough variation in urban form (number and type of buildings,\n",
            "relative amount of vegetation, roads etc.) to allow for discriminating\n",
            "between concepts that are fairly high-level. /T_his is in contrast with a\n",
            "benchmark such as DeepSat [ 1] that focuses on lower-level, physical\n",
            "concepts (“trees”, “buildings”, etc.). /T_here, a good spatial scale is\n",
            "by necessity smaller (28 mfor DeepSat), as variation in appearance\n",
            "and compositional elements is unwanted.\n",
            "5.2 Comparing urban environments\n",
            "Finally, we set to understand, at least on an initial qualitative level,\n",
            "how “similar” urban environments are to one another, across formal\n",
            "land use classes and geographies. Our /f_irst experiment was to\n",
            "project sample images for each class and city in this analysis to\n",
            "lower-dimensional manifolds, using the t-SNE algorithm [ 23]. /T_his\n",
            "serves the purpose of both visualization (as t-SNE is widely used\n",
            "for visualizing high-dimensional data), as well as for providing an\n",
            "initial, coarse continuous representation of urban land use classes.\n",
            "In our experiments, we used balanced samples of size N=6000, or\n",
            "100 samples for each of the 10 classes for each city. We extracted\n",
            "features for each of these samples using the allmodels (trained\n",
            "on a train set with samples across all cities except for the test one).\n",
            "Figure 9 visualizes such t-SNE embeddings for the six cities in\n",
            "our analysis. For most cities, classes such as low density urban\n",
            "fabric, forests, and water bodies are well-resolved, while sports\n",
            "and leisure facilities seem to consistently blend into other types of\n",
            "environments (which is not surprising, given that these types of\n",
            "facilities can be found within many types of locations that have a\n",
            "diﬀerent formal urban planning class assigned). Intriguing diﬀer-\n",
            "ences emerge in this picture among the cities. For example, greenFigure 7: Example classi/f_ication confusion matrix for land use inference. Le/f_t: training and testing on Barcelona; Middle :\n",
            "training on Rome, testing on Barcelona; Right: training on Rome, predicting on Budapest.\n",
            "Figure 8: Sensitivity of training patch size vs test accuracy.\n",
            "Figure 9: t-SNE visualization (the /f_irst 2 dimensions) of ur-\n",
            "ban environments (satellite image samples) across six cities.\n",
            "urban spaces seem fairly well resolved for most cities. Commercial\n",
            "neighborhoods in Barcelona seem more integrated with the other\n",
            "types of environments in the city, whereas for Berlin they appear\n",
            "more distinct. Urban water bodies are more embedded with urban\n",
            "parks for Barcelona than for other cities. Such reasoning (with\n",
            "more rigorous quantitative analysis) can serve as coarse way to\n",
            "benchmark and compare neighborhoods as input to further analysis\n",
            "about e.g., energy use, livelihood, or traﬃc in urban environments.\n",
            "Figure 10: Comparing urban environments across cities\n",
            "(with reference to Barcelona) We show relative inter-city\n",
            "similarity measures computed as the sum of squares across\n",
            "the clusters in Figure 9.\n",
            "We further illustrate how “similar” the six cities we used through-\n",
            "out this analysis are starting oﬀ the embeddings plots in Figure 9.\n",
            "For each land use class, we compute intra-city sum of squares in\n",
            "the 2-d t-SNE embedding, and display the results in Figure 10. Note\n",
            "that the distances are always shown with Barcelona as a reference\n",
            "point (chosen arbitrarily). For each panel, the normalization is with\n",
            "respect to the largest inter-city distance for that land use class. /T_his\n",
            "visualization aids quick understanding of similarity between urban\n",
            "environments. For example, agricultural lands in Barcelona are\n",
            "most dissimilar to those in Budapest. Airports in Barcelona are\n",
            "most similar to those in Athens, and most dissimilar to those in\n",
            "Berlin and Budapest. Barcelona’s forests and parks are most dissim-\n",
            "ilar to Budapest’s. Water bodies in Barcelona are very dissimilar to\n",
            "all other cities. /T_his point is enforced by Figure 11 below, which\n",
            "suggests that areas marked as water bodies in Barcelona are ocean\n",
            "waterfronts, whereas this class for all other cities represents rivers\n",
            "or lakes.Figure 11: Samples from three urban environments across\n",
            "our 6 example cities. We sampled the 2-d t-SNE embedding\n",
            "of Figure 9 and queried for the closest real sample to the\n",
            "centroid using an eﬃcient KD-tree search.\n",
            "Finally, we explore the feature maps extracted by the convolu-\n",
            "tional model in order to illustrate how “similar” the six cities we\n",
            "used throughout this analysis are across three example environ-\n",
            "ments, green urban areas, water bodies, and medium density urban\n",
            "fabric. For each city and land use class, we start oﬀ the centroid of\n",
            "the point cloud in the 2-d space of Figure 9, and /f_ind the nearest\n",
            "several samples using the KD-tree method described in Section 4.\n",
            "We present the results in Figure 11. Visual inspection indicates\n",
            "that the model has learned useful feature maps about urban envi-\n",
            "ronments: the sample image patches show a very good qualitative\n",
            "agreement with the region of the space where they’re sampled from,\n",
            "indicated by the land use class of neighboring points. /Q_ualitatively,\n",
            "it is clear that the features extracted from the top layer of the con-\n",
            "volutional model allow a comparison between urban environments\n",
            "by high-level concepts used in urban planning.\n",
            "6 CONCLUSIONS\n",
            "/T_his paper has investigated the use of convolutional neural net-\n",
            "works for analyzing urban environments through satellite imagery\n",
            "at the scale of entire cities. Given the current relative dearth of\n",
            "labeled satellite imagery in the machine learning community, we\n",
            "have constructed an open dataset of over 140 ,000 samples over 10\n",
            "consistent land use classes from 6 cities in Europe. As we continue\n",
            "to improve, curate, and expand this dataset, we hope that it can help\n",
            "other researchers in machine learning, smart cities, urban planning,\n",
            "and related /f_ields in their work on understanding cities.\n",
            "We set out to study similarity and variability across urban envi-\n",
            "ronments, as being able to quantify such pa/t_terns will enable richer\n",
            "applications in topics such as urban energy analysis, infrastructure\n",
            "benchmarking, and socio-economic composition of communities.\n",
            "We formulated this as a two-step task: /f_irst predicting urban land\n",
            "use classes from satellite imagery, then turning this (rigid) clas-\n",
            "si/f_ication into a continuous spectrum by embedding the features\n",
            "extracted from the convolutional classi/f_ier into a lower-dimensional\n",
            "manifold. We show that the classi/f_ication task achieves encour-\n",
            "aging results, given the large variety in physical appearance of\n",
            "urban environments having the same functional class. Moreover,we exemplify how the features extracted from the convolutional\n",
            "network allow for identifying “neighbors” of any given query im-\n",
            "age, allowing a rich comparison analysis of urban environments by\n",
            "their visual composition.\n",
            "/T_he analysis in this paper shows that some types urban envi-\n",
            "ronments are easier to infer than others, both in the intra- and\n",
            "inter-city cases. For example, in all our experiments, the models\n",
            "had most trouble telling apart “high”, “medium”, and “low” den-\n",
            "sity urban environments, a/t_testing to the subjectivity of such a\n",
            "high-level classi/f_ication for urban planning purposes. However,\n",
            "agricultural lands, forests, and airports tend to be visually similar\n",
            "across diﬀerent cities - and the amount of relative dissimilarity can\n",
            "be quanti/f_ied using the methods in this paper. Green urban areas\n",
            "(parks) are generally similar to forests or to leisure facilities, and\n",
            "the models do be/t_ter in the intra-city case than predicting across\n",
            "cities. How industrial areas look is again less geography-speci/f_ic:\n",
            "inter-city similarity is consistently larger than intra-city similarity.\n",
            "As such, for several classes we can expect learning to transfer from\n",
            "one geography to another. /T_hus, while it is not news that some\n",
            "cities are more “similar” than others (Barcelona is visually closer to\n",
            "Athens than it is to Berlin), the methodology in this paper allows\n",
            "for a more quantitative and practical comparison of similarity.\n",
            "By leveraging satellite data (available virtually world-wide), this\n",
            "approach may allow for a low-cost way to analyze urban envi-\n",
            "ronments in locations where ground truth information on urban\n",
            "planning is not available. As future directions of this work, we\n",
            "plan to i)continue to develop more rigorous ways to compare and\n",
            "benchmark urban neighborhoods, going deeper to physical ele-\n",
            "ments (vegetation, buildings, roads etc.); ii)improve and further\n",
            "curate the open Urban Environments dataset; and iii)extend this\n",
            "type of analysis to more cities across other geographical locations.\n",
            "A PRACTICAL TRAINING DETAILS.\n",
            "We split our training data into a training set (80% of the data) and a\n",
            "validation set (the remaining 20%). /T_his is separate from the data\n",
            "sampled for the ground truth raster grids for each city, which we\n",
            "only used at test time. We implemented the architectures in the\n",
            "open-source deep learning framework Keras7(with a TensorFlow8\n",
            "backend). In all our experiments, we used popular data augmenta-\n",
            "tion techniques, including random horizontal and vertical /f_lipping\n",
            "of the input images, random shearing (up to 0 .1 radians), random\n",
            "scaling (up to 120%), random rotations (by at most 15 degrees either\n",
            "direction). Input images were 224 ×224×3 pixels in size (RGB\n",
            "bands). For all experiments, we used stochastic gradient descent\n",
            "(with its Adadelta variant) to optimize the network loss function (a\n",
            "standard multi-class cross-entropy), starting with a learning rate of\n",
            "0.1, and halving the rate each 10 epochs. We trained our networks\n",
            "for at most 100 epochs, with 2000 samples in each epoch, stopping\n",
            "the learning process when the accuracy on the validation set did\n",
            "not improve for more than 10 epochs. Given the inherent imbalance\n",
            "of the classes, we explicitly enforced that the minibatches used for\n",
            "training were relatively balanced by a weighted sampling proce-\n",
            "dure. For training, we used a cluster of 4 NVIDIA K80 GPUs, and\n",
            "tested our models on a cluster of 48 CPUs.\n",
            "7h/t_tps://github.com/fchollet/keras\n",
            "8www.tensor/f_low.orgREFERENCES\n",
            "[1]Saikat Basu, Sangram Ganguly, Supratik Mukhopadhyay, Robert DiBiano,\n",
            "Manohar Karki, and Ramakrishna R. Nemani. 2015. DeepSat - A Learning\n",
            "framework for Satellite Imagery. CoRR abs/1509.03602 (2015).\n",
            "[2]Marco Castelluccio, Giovanni Poggi, Carlo Sansone, and Luisa Verdoliva. 2015.\n",
            "Land Use Classi/f_ication in Remote Sensing Images by Convolutional Neural\n",
            "Networks. CoRR abs/1508.00092 (2015). h/t_tp://arxiv.org/abs/1508.00092\n",
            "[3]Dragos Costea and Marius Leordeanu. 2016. Aerial image geolocalization from\n",
            "recognition and matching of roads and intersections. CoRR abs/1605.08323 (2016).\n",
            "h/t_tp://arxiv.org/abs/1605.08323\n",
            "[4]Marco De Nadai, Radu Laurentiu Vieriu, Gloria Zen, Stefan Dragicevic, Nikhil\n",
            "Naik, Michele Caraviello, Cesar Augusto Hidalgo, Nicu Sebe, and Bruno Lepri.\n",
            "2016. Are Safer Looking Neighborhoods More Lively?: A Multimodal Investiga-\n",
            "tion into Urban Life. In Proceedings of the 2016 ACM on Multimedia Conference\n",
            "(MM ’16) . ACM, New York, NY, USA, 1127–1135. DOI: h/t_tp://dx.doi.org/10.1145/\n",
            "2964284.2964312\n",
            "[5]Abhimanyu Dubey, Nikhil Naik, Devi Parikh, Ramesh Raskar, and C ´esar A.\n",
            "Hidalgo. 2016. Deep Learning the City: /Q_uantifying Urban Perception at a Global\n",
            "Scale . Springer International Publishing, Cham, 196–212. DOI: h/t_tp://dx.doi.org/\n",
            "10.1007/978-3-319-46448-0 12\n",
            "[6]Sebastian Grauwin, Stanislav Sobolevsky, Simon Moritz, Istv ´an G´odor, and Carlo\n",
            "Ra/t_ti. 2015. Towards a Comparative Science of Cities: Using Mobile Traﬃc Records\n",
            "in New York, London, and Hong Kong . Springer International Publishing, Cham,\n",
            "363–387. DOI: h/t_tp://dx.doi.org/10.1007/978-3-319-11469-9 15\n",
            "[7]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual\n",
            "Learning for Image Recognition. In 2016 IEEE Conference on Computer Vision and\n",
            "Pa/t_tern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016 . 770–778.\n",
            "DOI: h/t_tp://dx.doi.org/10.1109/CVPR.2016.90\n",
            "[8]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Identity Map-\n",
            "pings in Deep Residual Networks. In ECCV (4) (Lecture Notes in Computer Science) ,\n",
            "Vol. 9908. Springer, 630–645.\n",
            "[9]Neal Jean, Marshall Burke, Michael Xie, W Ma/t_thew Davis, David B Lobell, and\n",
            "Stefano Ermon. 2016. Combining satellite imagery and machine learning to\n",
            "predict poverty. Science 353, 6301 (2016), 790–794.\n",
            "[10] Maxime Lenormand, Miguel Picornell, Oliva G. Cant ´u-Ros, /T_homas Louail, Ri-\n",
            "cardo Herranz, Marc Barthelemy, Enrique Fr ´ıas-Mart ´ınez, Maxi San Miguel, and\n",
            "Jos´e J. Ramasco. 2015. Comparing and modelling land use organization in cities.\n",
            "Royal Society Open Science 2, 12 (2015). DOI: h/t_tp://dx.doi.org/10.1098/rsos.150449\n",
            "arXiv:h/t_tp://rsos.royalsocietypublishing.org/content/2/12/150449.full.pdf\n",
            "[11] Qingshan Liu, Renlong Hang, Huihui Song, and Zhi Li. 2016. Learning Multi-\n",
            "Scale Deep Features for High-Resolution Satellite Image Classi/f_ication. CoRR\n",
            "abs/1611.03591 (2016). h/t_tp://arxiv.org/abs/1611.03591\n",
            "[12] D. Marmanis, K. Schindler, J. D. Wegner, S. Galliani, M. Datcu, and U. Stilla.\n",
            "2016. Classi/f_ication With an Edge: Improving Semantic Image Segmentation\n",
            "with Boundary Detection. ArXiv e-prints (Dec. 2016). arXiv:cs.CV/1612.01337\n",
            "[13] Volodymyr Mnih. 2013. Machine learning for aerial image labeling . Ph.D. Disser-\n",
            "tation. University of Toronto.\n",
            "[14] Nikhil Naik, Ramesh Raskar, and Csar A. Hidalgo. 2016. Cities Are Physical Too:\n",
            "Using Computer Vision to Measure the /Q_uality and Impact of Urban Appearance.\n",
            "American Economic Review 106, 5 (May 2016), 128–32. DOI: h/t_tp://dx.doi.org/10.1257/aer.p20161030\n",
            "[15] M. Papadomanolaki, M. Vakalopoulou, S. Zagoruyko, and K. Karantzalos. 2016.\n",
            "Benchmarking Deep Learning Frameworks for the Classi/f_ication of Very High\n",
            "Resolution Satellite Multispectral Data. ISPRS Annals of Photogrammetry, Remote\n",
            "Sensing and Spatial Information Sciences (June 2016), 83–88. DOI: h/t_tp://dx.doi.\n",
            "org/10.5194/isprs-annals-III-7-83-2016\n",
            "[16] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. /T_hirion, O. Grisel, M.\n",
            "Blondel, P. Pre/t_tenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour-\n",
            "napeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine\n",
            "Learning in Python. Journal of Machine Learning Research 12 (2011), 2825–2830.\n",
            "[17] O. A. B. Pena/t_ti, K. Nogueira, and J. A. dos Santos. 2015. Do deep features\n",
            "generalize from everyday objects to remote sensing and aerial scenes domains?.\n",
            "In2015 IEEE Conference on Computer Vision and Pa/t_tern Recognition Workshops\n",
            "(CVPRW) . 44–51.DOI: h/t_tp://dx.doi.org/10.1109/CVPRW.2015.7301382\n",
            "[18] A. Romero, C. Ga/t_ta, and G. Camps-Valls. 2016. Unsupervised Deep Fea-\n",
            "ture Extraction for Remote Sensing Image Classi/f_ication. IEEE Transactions\n",
            "on Geoscience and Remote Sensing 54, 3 (March 2016), 1349–1362. DOI: h/t_tp:\n",
            "//dx.doi.org/10.1109/TGRS.2015.2478379\n",
            "[19] Karen Simonyan and Andrew Zisserman. 2014. Very Deep Convolutional\n",
            "Networks for Large-Scale Image Recognition. CoRR abs/1409.1556 (2014).\n",
            "h/t_tp://arxiv.org/abs/1409.1556\n",
            "[20] Jameson L. Toole, Michael Ulm, Marta C. Gonz ´alez, and Dietmar Bauer. 2012.\n",
            "Inferring Land Use from Mobile Phone Activity. In Proceedings of the ACM\n",
            "SIGKDD International Workshop on Urban Computing (UrbComp ’12) . ACM, New\n",
            "York, NY, USA, 1–8. DOI: h/t_tp://dx.doi.org/10.1145/2346496.2346498\n",
            "[21] Nagesh Kumar Uba. 2016. Land Use and Land Cover Classi/f_ication Using Deep\n",
            "Learning Techniques . Master’s thesis. Arizona State University.\n",
            "[22] European Union. 2011. Urban Atlas. Urban Atlas is a product commissioned by\n",
            "DG REGIO and provided by the Copernicus programme. h/t_tp://www.eea.europa.\n",
            "eu/data-and-maps/data/urban-atlas/. (2011).\n",
            "[23] L.J.P van der Maaten and G.E. Hinton. 2008. Visualizing High-Dimensional Data\n",
            "Using t-SNE. Journal of Machine Learning Research 9: 2579-2605 (2008).\n",
            "[24] Sco/t_t Workman, Richard Souvenir, and Nathan Jacobs. 2015. Wide-Area Image\n",
            "Geolocalization with Aerial Reference Imagery. CoRR abs/1510.03743 (2015).\n",
            "[25] Yi Yang and Shawn Newsam. 2010. Bag-of-visual-words and Spatial Extensions\n",
            "for Land-use Classi/f_ication. In Proceedings of the 18th SIGSPATIAL International\n",
            "Conference on Advances in Geographic Information Systems (GIS ’10) . ACM, New\n",
            "York, NY, USA, 270–279. DOI: h/t_tp://dx.doi.org/10.1145/1869790.1869829\n",
            "[26] Jun Yue, Wenzhi Zhao, Shanjun Mao, and Hui Liu. 2015. Spectral-spatial classi/f_ica-\n",
            "tion of hyperspectral images using deep convolutional neural networks. Remote\n",
            "Sensing Le/t_ters 6, 6 (2015), 468–477. DOI: h/t_tp://dx.doi.org/10.1080/2150704X.\n",
            "2015.1047045 arXiv:h/t_tp://dx.doi.org/10.1080/2150704X.2015.1047045\n",
            "[27] Menghua Zhai, Zachary Bessinger, Sco/t_t Workman, and Nathan Jacobs. 2016.\n",
            "Predicting Ground-Level Scene Layout from Aerial Imagery. CoRR abs/1612.02709\n",
            "(2016).\n",
            "[28] Yi Zhu and Shawn Newsam. 2015. Land Use Classi/f_ication Using Convolutional\n",
            "Neural Networks Applied to Ground-level Images. In Proceedings of the 23rd\n",
            "SIGSPATIAL International Conference on Advances in Geographic Information\n",
            "Systems (SIGSPATIAL ’15) . ACM, New York, NY, USA, Article 61, 4 pages. DOI:\n",
            "h/t_tp://dx.doi.org/10.1145/2820783.2820851\n"
          ]
        }
      ],
      "source": [
        "extracted_text = extract_text_from_pdf(pdf_path)\n",
        "print(extracted_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "2iDhiUNI2Rjy"
      },
      "outputs": [],
      "source": [
        "# Omitting 'References' section from extracted text\n",
        "def remove_references(pdf_text):\n",
        "    references_pattern = re.compile(r'References\\s*[\\r\\n]+.*', re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "    text_without_references = re.sub(references_pattern, '', pdf_text)\n",
        "\n",
        "    return text_without_references"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "nRV4KUpg2Rjz",
        "outputId": "a16c3b28-78f6-4402-ccb9-3d275761252a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Using Convolutional Networks and Satellite Imagery to Identify\\nPa/t_terns in Urban Environments at a Large Scale\\nAdrian Albert∗\\nMassachuse/t_ts Institute of Technology\\nCivil and Environmental Engineering\\n77 Massachuse/t_ts Ave\\nCambridge, MA 02139\\nadalbert@mit.eduJasleen Kaur\\nPhilips Lighting Research North\\nAmerica\\n2 Canal Park\\nCambridge, MA 02141\\njasleen.kaur1@philips.comMarta C. Gonz ´alez\\nMassachuse/t_ts Institute of Technology\\nCivil and Environmental Engineering\\n77 Massachuse/t_ts Ave\\nCambridge, MA 02139\\nmartag@mit.edu\\nABSTRACT\\nUrban planning applications (energy audits, investment, etc.) re-\\nquire an understanding of built infrastructure and its environment,\\ni.e., both low-level, physical features (amount of vegetation, build-\\ning area and geometry etc.), as well as higher-level concepts such\\nas land use classes (which encode expert understanding of socio-\\neconomic end uses). /T_his kind of data is expensive and labor-\\nintensive to obtain, which limits its availability (particularly in\\ndeveloping countries). We analyze pa/t_terns in land use in urban\\nneighborhoods using large-scale satellite imagery data (which is\\navailable worldwide from third-party providers) and state-of-the-\\nart computer vision techniques based on deep convolutional neural\\nnetworks. For supervision, given the limited availability of standard\\nbenchmarks for remote-sensing data, we obtain ground truth land\\nuse class labels carefully sampled from open-source surveys, in\\nparticular the Urban Atlas land classi/f_ication dataset of 20 land use\\nclasses across 300 European cities. We use this data to train and\\ncompare deep architectures which have recently shown good per-\\nformance on standard computer vision tasks (image classi/f_ication\\nand segmentation), including on geospatial data. Furthermore, we\\nshow that the deep representations extracted from satellite imagery\\nof urban environments can be used to compare neighborhoods\\nacross several cities. We make our dataset available for other ma-\\nchine learning researchers to use for remote-sensing applications.\\nCCS CONCEPTS\\n•Computing methodologies →Computer vision; Neural net-\\nworks; •Applied computing →Environmental sciences;\\nKEYWORDS\\nSatellite imagery, land use classi/f_ication, convolutional networks\\n1 INTRODUCTION\\nLand use classi/f_ication is an important input for applications rang-\\ning from urban planning, zoning and the issuing of business per-\\nmits, to real-estate construction and evaluation to infrastructure\\n∗Corresponding author.\\nPermission to make digital or hard copies of part or all of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor pro/f_it or commercial advantage and that copies bear this notice and the full citation\\non the /f_irst page. Copyrights for third-party components of this work must be honored.\\nFor all other uses, contact the owner/author(s).\\nKDD’17, August 13–17, 2017, Halifax, NS, Canada.\\n© 2017 Copyright held by the owner/author(s). 978-1-4503-4887-4/17/08.\\nDOI: h/t_tp://dx.doi.org/10.1145/3097983.3098070development. Urban land use classi/f_ication is typically based on\\nsurveys performed by trained professionals. As such, this task\\nis labor-intensive, infrequent, slow, and costly. As a result, such\\ndata are mostly available in developed countries and big cities that\\nhave the resources and the vision necessary to collect and curate it;\\nthis information is usually not available in many poorer regions,\\nincluding many developing countries [ 9] where it is mostly needed.\\n/T_his paper builds on two recent trends that promise to make\\nthe analysis of urban environments a more democratic and inclu-\\nsive task. On the one hand, recent years have seen signi/f_icant\\nimprovements in satellite technology and its deployment (primar-\\nily through commercial operators), which allows to obtain high\\nand medium-resolution imagery of most urbanized areas of the\\nEarth with an almost daily revisit rate. On the other hand, the\\nrecent breakthroughs in computer vision methods, in particular\\ndeep learning models for image classi/f_ication and object detection,\\nnow make possible to obtain a much more accurate representation\\nof the composition built infrastructure and its environments.\\nOur contributions are to both the applied deep learning literature,\\nand to the incipient study of “smart cities” using remote sensing\\ndata. We contrast state-of-the-art convolutional architectures (the\\nVGG-16 [ 19] and ResNet [ 7] networks) to train classi/f_iers that recog-\\nnize broad land use classes from satellite imagery. We then use the\\nfeatures extracted from the model to perform a large-scale compar-\\nison of urban environments. For this, we construct a novel dataset\\nfor land use classi/f_ication, pairing carefully sampled locations with\\nground truth land use class labels obtained from the Urban Atlas\\nsurvey [ 22] with satellite imagery obtained from Google Maps’s\\nstatic API. Our dataset - which we have made available publicly\\nfor other researchers - covers, for now, 10 cities in Europe (chosen\\nout of the original 300) with 10 land use classes (from the original\\n20). As the Urban Atlas is a widely-used, standardized dataset for\\nland use classi/f_ication, we hope that making this dataset available\\nwill encourage the development analyses and algorithms for ana-\\nlyzing the built infrastructure in urban environments. Moreover,\\ngiven that satellite imagery is available virtually everywhere on\\nthe globe, the methods presented here allow for automated, rapid\\nclassi/f_ication of urban environments that can potentially be applied\\nto locations where survey and zoning data is not available.\\nLand use classi/f_ication refers to the combination of physical\\nland a/t_tributes and what cultural and socio-economic function land\\nserves (which is a subjective judgement by experts) [ 2]. In this paper,\\nwe take the view that land use classes are just a useful discretization\\nof a more continuous spectrum of pa/t_terns in the organization of\\nurban environments. /T_his viewpoint is illustrated in Figure 2: while\\narXiv:1704.02965v2  [cs.CV]  13 Sep 2017Figure 1: Urban land use maps for six example cities. We compare the ground truth ( top row ) with the predicted land use maps,\\neither from using separate data collected from the same city ( middle row ), or using data from all other cities ( bo/t_tom row ).\\nFigure 2: Le/f_t: Comparing urban environments via deep hi-\\nerarchical representations of satellite image samples. Right:\\napproach outline - data collection, classi/f_ication, feature ex-\\ntraction, clustering, validation.\\nsome a/t_tributes (e.g., amount of built structures or vegetation) are\\ndirectly interpretable, some others may not be. Nevertheless, these\\npa/t_terns in/f_luence, and are in/f_luenced by, socio-economic factors\\n(e.g., economic activity), resource use (energy), and dynamic human\\nbehavior (e.g., mobility, building occupancy). We see the work\\non cheaply curating a large-scale land use classi/f_ication dataset\\nand comparing neighborhoods using deep representations that\\nthis paper puts forth as a necessary /f_irst step towards a granular\\nunderstanding of urban environments in data-poor regions.\\nSubsequently, in Section 2 we review related studies that apply\\ndeep learning methods and other machine learning techniques\\nto problems of land use classi/f_ication, object detection, and image\\nsegmentation in aerial imagery. In Section 3 we describe the datasetwe curated based on the Urban Atlas survey. Section 4 reviews the\\ndeep learning architectures we used. Section 5 describes model\\nvalidation and analysis results. We conclude in Section 6.\\nAll the code used to acquire, process, and analyze the data, as\\nwell as to train the models discussed in this paper is available at\\nh/t_tp://www.github.com/adrianalbert/urban-environments.\\n2 LITERATURE\\n/T_he literature on the use of remote sensing data for applications in\\nland use cover, urban planning, environmental science, and others,\\nhas a long and rich history. /T_his paper however is concerned more\\nnarrowly with newer work that employs widely-available data\\nand machine learning models - and in particular deep learning\\narchitectures - to study urban environments.\\nDeep learning methods have only recently started to be deployed\\nto the analysis of satellite imagery. As such, land use classi/f_ication\\nusing these tools is still a very incipient literature. Probably the /f_irst\\nstudies (yet currently only 1-2 years old) include the application\\nof convolutional neural networks to land use classi/f_ication [ 2] us-\\ning the UC Merced land use dataset [ 25] (of 2100 images spanning\\n21 classes) and the classi/f_ication of agricultural images of coﬀee\\nplantations [ 17]. Similar early studies on land use classi/f_ication\\nthat employ deep learning techniques are [ 21], [18], and [ 15]. In\\n[11], a spatial pyramid pooling technique is employed for land use\\nclassi/f_ication using satellite imagery. /T_he authors of these studies\\nadapted architectures pre-trained to recognize natural images from\\nthe ImageNet dataset (such as the VGG16 [ 19], which we also use),\\nand /f_ine-tuned them on their (much smaller) land use data. More\\nrecent studies use the DeepSat land use benchmark dataset [ 1],\\nwhich we also use and describe in more detail in Section 2.1. An-\\nother topic that is closely related to ours is remote-sensing image\\nsegmentation and object detection, where modern deep learning\\nmodels have also started to be applied. Some of the earliest work\\nthat develops and applies deep neural networks for this tasks is thatof [13]. Examples of recent studies include [ 26] and [ 12], where the\\nauthors propose a semantic image segmentation technique com-\\nbining texture features and boundary detection in an end-to-end\\ntrainable architecture.\\nRemote-sensing data and deep learning methods have been put\\nto use to other related ends, e.g., geo-localization of ground-level\\nphotos via satellite images [ 3,24] or predicting ground-level scene\\nimages from corresponding aerial imagery [ 27]. Other applications\\nhave included predicting survey estimates on poverty levels in\\nseveral countries in Africa by /f_irst learning to predict levels of night\\nlights (considered as proxies of economic activity and measured\\nby satellites) from day-time, visual-range imagery from Google\\nMaps, then transferring the learning from this la/t_ter task to the\\nformer [ 9]. Our work takes a similar approach, in that we aim to use\\nremote-sensing data (which is widely-available for most parts of\\nthe world) to infer land use types in those locations where ground\\ntruth surveys are not available.\\nUrban environments have been analyzed using other types of\\nimagery data that have become recently available. In [ 4,14], the\\nauthors propose to use the same type of imagery from Google Street\\nView to measure the relationship between urban appearance and\\nquality of life measures such as perceived safety. For this, they\\nhand-cra/f_t standard image features widely used in the computer\\nvision community, and train a shallow machine learning classi/f_ier\\n(a support vector machine). In a similar fashion, [ 5] trained a\\nconvolutional neural network on ground-level Street View imagery\\npaired with a crowd-sourced mechanism for collecting ground truth\\nlabels to predict subjective perceptions of urban environments such\\nas “beauty”, “wealth”, and “liveliness”.\\nLand use classi/f_ication has been studied with other new data\\nsources in recent years. For example, ground-level imagery has been\\nemployed to accurately predict land use classes on an university\\ncampus [ 28]. Another related literature strand is work that uses\\nmobile phone call records to extract spatial and temporal mobility\\npa/t_terns, which are then used to infer land use classes for several\\ncities [ 6,10,20]. Our work builds on some of the ideas for sampling\\ngeospatial data presented there.\\n2.1 Existing land use benchmark datasets\\nPublic benchmark data for land use classi/f_ication using aerial im-\\nagery are still in relatively short supply. Presently there are two\\nsuch datasets that we are aware of, discussed below.\\nUC Merced. /T_his dataset was published in 2010 [ 25] and con-\\ntains 2100 256 ×256, 1 m/pxaerial RGB images over 21 land use\\nclasses. It is considered a “solved problem”, as modern neural net-\\nwork based classi/f_iers [2] have achieved >95% accuracy on it.\\nDeepSat. /T_he DeepSat [ 1] dataset1was released in 2015. It\\ncontains two benchmarks: the Sat-4 data of 500 ,000 images over 4\\nland use classes ( barren land, trees, grassland, other ), and the Sat-6\\ndata of 405 ,000 images over 6 land use classes ( barren land, trees,\\ngrassland, roads, buildings, water bodies ). All the samples are 28 ×28\\nin size at a 1 m/pxspatial resolution and contain 4 channels (red,\\ngreen, blue, and NIR - near infrared). Currently less than two years\\nold, this dataset is already a “solved problem”, with previous studies\\n[15] (and our own experiments) achieving classi/f_ication accuracies\\n1Available at h/t_tp://csc.lsu.edu/ ∼saikat/deepsat/.of over 99% using convolutional architectures. While useful as input\\nfor pre-training more complex models, (e.g., image segmentation),\\nthis dataset does not allow to take the further steps for detailed\\nland use analysis and comparison of urban environments across\\ncities, which gap we hope our dataset will address.\\nOther open-source eﬀorts. /T_here are several other projects\\nthat we are aware of related to land use classi/f_ication using open-\\nsource data. /T_he TerraPa/t_tern2project uses satellite imagery from\\nGoogle Maps (just like we do) paired with truth labels over a large\\nnumber (450) of detailed classes obtained using the Open Street\\nMap API3. (Open Street Maps is a comprehensive, open-access,\\ncrowd-sourced mapping system.) /T_he project’s intended use is as\\na search tool for satellite imagery, and as such, the classes they\\nemploy are very speci/f_ic, e.g., baseball diamonds, churches, or\\nroundabouts. /T_he authors use a ResNet architecture [ 7] to train a\\nclassi/f_ication model, which they use to embed images in a high-\\ndimensional feature space, where “similar” images to an input image\\ncan be identi/f_ied. A second open-source project related to ours is\\nthe DeepOSM4, in which the authors take the same approach of\\npairing OpenStreetMap labels with satellite imagery obtained from\\nGoogle Maps, and use a convolutional architecture for classi/f_ication.\\n/T_hese are excellent starting points from a practical standpoint,\\nallowing interested researchers to quickly familiarize themselves\\nwith programming aspects of data collection, API calls, etc.\\n3 THE URBAN ENVIRONMENTS DATASET\\n3.1 Urban Atlas: a standard in land use analysis\\n/T_he Urban Atlas [ 22] is an open-source, standardized land use\\ndataset that covers ∼300 European cities of 100 ,000 inhabitants or\\nmore, distributed relatively evenly across major geographical and\\ngeopolitical regions. /T_he dataset was created between 2005-2011 as\\npart of a major eﬀort by the European Union to provide a uniform\\nframework for the geospatial analysis of urban areas in Europe.\\nLand use classi/f_ication is encoded via detailed polygons organized\\nin commonly-used GIS/ESRI shape /f_iles. /T_he dataset covers 20\\nstandardized land use classes. In this work we selected classes of\\ninterest and consolidated them into 10 /f_inal classes used for analysis\\n(see Figure 3). Producing the original Urban Atlas dataset required\\nfusing several data sources: high and medium-resolution satellite\\nimagery, topographic maps, navigation and road layout data, and\\nlocal zoning (cadastral) databases. More information on the method-\\nology used by the Urban Atlas researchers can be obtained from\\nthe European Environment Agency5. We chose expressly to use\\nthe Urban Atlas dataset over other sources (described in Section 2.1\\nbecause i)it is a comprehensive and consistent survey at a large\\nscale, which has been extensively curated by experts and used in\\nresearch, planning, and socio-economic work over the past decade,\\nandii)the land use classes re/f_lect higher-level (socio-economic,\\ncultural) functions of the land as used in applications.\\nWe note that there is a wide variance in the distribution of land\\nuse classes across and within the 300 cities. Figure 3 illustrates\\nthe diﬀerences in the distribution in ground truth polygon areas\\n2h/t_tp://www.terrapa/t_tern.com/\\n3h/t_tp://www.openstreetmap.org\\n4h/t_tps://github.com/trailbehind/DeepOSM\\n5h/t_tp://www.eea.europa.eu/data-and-maps/data/urban-atlas/Figure 3: Ground truth land use distribution (by area) for\\nthree example cities in the Urban Environments dataset.\\nfor each of the classes for three example cities (Budapest, Rome,\\nBarcelona) from the dataset (from Eastern, Central, and Western\\nEurope, respectively). /T_his wide disparity in the spatial distribution\\npa/t_terns of diﬀerent land use classes and across diﬀerent cities\\nmotivates us to design a careful sampling procedure for collecting\\ntraining data, described in detail below.\\n3.2 Data sampling and acquisition\\nWe set out to develop a strategy to obtain high-quality samples\\nof the type (satellite image, ground truth label) to use in training\\nconvolutional architectures for image classi/f_ication. Our /f_irst re-\\nquirement is to do this solely with freely-available data sources,\\nas to keep costs very low or close to zero. For this, we chose to\\nuse the Google Maps Static API6as a source of satellite imagery.\\n/T_his service allows for 25 ,000 API requests/day free of charge. For\\na given sampling location given by (latitude, longitude), we ob-\\ntained 224 ×224 images at a zoom level 17 (around 1 .20m/pxspatial\\nresolution, or ∼250m×250mcoverage for an image).\\n/T_he goals of our sampling strategy are twofold. First, we want\\nto ensure that the resulting dataset is as much as possible balanced\\nwith respect to the land use classes. /T_he challenge is that the classes\\nare highly imbalanced among the ground truth polygons in the\\ndataset (e.g., many more polygons are agricultural land and isolated\\nstructures than airports). Second, the satellite images should be\\nrepresentative of the ground truth class associated to them. To this\\nend, we require that the image contain at least 25% (by area) of\\nthe associated ground truth polygon. /T_hus, our strategy to obtain\\ntraining samples is as follows (for a given city):\\n•Sort ground truth polygons in decreasing order according to\\ntheir size, and retain only those polygons with areas larger than\\n1\\n4(224×1.2m)2=0.06km2;\\n•From each decile of the distribution of areas, sample a propor-\\ntionally larger number of polygons, such that some of the smaller\\npolygons also are picked, and more of the larger ones;\\n•For each picked polygon, sample a number of images propor-\\ntional to the area of the polygon, and assign each image the\\npolygon class as ground truth label;\\n6h/t_tps://developers.google.com/maps/documentation/static-maps/\\nFigure 4: Example satellite images for the original land use\\nclasses in the Urban Atlas dataset.\\nExample satellite images for each of the 10 land use classes in\\nthe Urban Environments dataset are given in Figure 4. Note the\\nsigni/f_icant variety (in color schemes, textures, etc) in environments\\ndenoted as having the same land use class. /T_his is because of several\\nfactors, including the time of the year when the image was acquired\\n(e.g., agricultural lands appear diﬀerent in the spring than in the\\nfall), the diﬀerent physical form and appearance of environments\\nthat serve the same socioeconomic or cultural function (e.g., green\\nurban areas may look very diﬀerent in diﬀerent cities or in even\\nin diﬀerent parts of the same city; what counts as “dense urban\\nfabric” in one city may not be dense at all in other cities), and\\nchange in the landscape during the several years that have passed\\nsince the compilation of the Urban Atlas dataset and the time of\\nacquisition of the satellite image (e.g., construction sites may not\\nre/f_lect accurately anymore the reality on the ground).\\nApart from these training images, we constructed ground truth\\nrasters to validate model output for each city. For that, we de/f_ined\\nuniform validation grids of 100 ×100 (25km×25km )around the\\n(geographical) center of a given city of interest. We take a satellite\\nimage sample in each grid cell, and assign to it as label the class\\nof the polygon that has the maximum intersection area with that\\ncell. Examples of land use maps for the six cities we analyze here\\nare given in Figure 1 (top row). /T_here, each grid cell is assigned the\\nclass of the ground truth polygon whose intersection with the cell\\nhas maximum coverage fraction by area. Classes are color-coded\\nfollowing the original Urban Atlas documentation.\\nIn Table 1 we present summaries of the training (le/f_t) and vali-\\ndation (right) datasets we used for the analysis in this paper. /T_he\\nvalidation dataset consists of the images sampled at the centers of\\neach cell in the 25 km×25kmgrid as discussed above. /T_his dataset\\nconsists of ∼140,000 images distributed across 10 urban environ-\\nment classes from 6 cities: Roma (Rome), Madrid, Berlin, Budapest,\\nBarcelona, and Athina (Athens). Because of the high variation in\\nappearance upon visual inspection, we chose to consolidate severalclasses from the original dataset, in particular classes that indicated\\nurban fabric into “High Density Urban Fabric”, “Medium Density\\nUrban Fabric, and “Low Density Urban Fabric”. As mentioned above\\nand illustrated in Figure 3, we did notice a great disparity in the\\nnumbers and distribution of ground truth polygons for other ex-\\nample cities that we investigated in the Urban Atlas dataset. As\\nsuch,for the analysis in this paper, we have chosen cities where\\nenough ground truth polygons were available for each class (that\\nis, at least 50 samples) to allow for statistical comparisons.\\n4 EXPERIMENTAL SETUP\\n4.1 Neural network architectures and training\\nFor all experiments in this paper we compared the VGG-16 [ 19]\\nand ResNet [7, 8] architectures.\\nVGG-16. /T_his architecture [ 19] has become one of the most pop-\\nular models in computer vision for classi/f_ication and segmentation\\ntasks. It consists of 16 trainable layers organized in blocks. It starts\\nwith a 5-block convolutional base of neurons with 3 ×3 receptive\\n/f_ields (alternated with max-pooling layers that eﬀectively increase\\nthe receptive /f_ield of neurons further downstream). Following each\\nconvolutional layer is a ReLU activation function [ 19]. /T_he feature\\nmaps thus obtained are fed into a set of fully-connected layers (a\\ndeep neural network classi/f_ier). See Table 2 for a summary.\\nResNet. /T_his architecture [ 7,8] has achieved state-of-the-art\\nperformance on image classi/f_ication on several popular natural\\nimage benchmark datasets. It consists of blocks of convolutional\\nlayers, each of which is followed by a ReLU non-linearity. As before,\\neach block in the convolutional base is followed by a max-pooling\\noperation. Finally, the output of the last convolutional layer serves\\nas input feature map for a fully-connected layer with a so/f_tmax\\nactivation function. /T_he key diﬀerence in this architecture is that\\nshortcut connections are implemented that skip blocks of convo-\\nlutional layers, allowing the network to learn residual mappings\\nbetween layer input and output. Here we used an implementation\\nwith 50 trainable layers per [7]. See Table 3 for a summary.\\nTransfer learning. As it is common practice in the literature,\\nwe have experimented with training our models on the problem of\\ninterest (urban environment classi/f_ication) starting from architec-\\ntures pre-trained on datasets from other domains ( transfer learning ).\\n/T_his procedure has been shown to yield both be/t_ter performance\\nand faster training times, as the network already has learned to\\nrecognize basic shapes and pa/t_terns that are characteristic of im-\\nages across many domains (e.g., [ 9,12,15]). We have implemented\\nthe following approaches: 1)we used models pre-trained on the\\nImageNet dataset, then further /f_ine-tuned them on the Urban Atlas\\ndataset; and 2)we pre-trained on the DeepSat dataset (See Section\\n2), then further re/f_ined on the Urban Atlas dataset. As expected,\\nthe la/t_ter strategy - /f_irst training a model (itself pre-trained on Ima-\\ngeNet data) on the DeepSat benchmark, and the further re/f_ining\\non the Urban Atlas dataset - yielded the best results, achieving\\nincreases of around 5% accuracy for a given training time.\\nGiven the large amount of variation in the visual appearance\\nof urban environments across diﬀerent cities (because of diﬀerent\\nclimates, diﬀerent architecture styles, various other socio-economic\\nfactors), it is of interest to study to what extent a model learned on\\none geographical location can be applied to a diﬀerent geographicallocation. As such, we perform experiments in which we train a\\nmodel for one (or more) cities, then apply the model to a diﬀerent set\\nof cities. Intuitively, one would expect that, the more neighborhoods\\nand other urban features at one location are similar to those at a\\ndiﬀerent location, the be/t_ter learning would transfer, and the higher\\nthe classi/f_ication accuracy obtained would be. Results for these\\nexperiments are summarized in Figure 6.\\n4.2 Comparing urban environments\\nWe next used the convolutional architectures to extract features\\nfor validation images. As in other recent studies (e.g., [ 9]), we use\\nthe last layer of a network as feature extractor. /T_his amounts to\\nfeature vectors of D=4096 dimensions for the VGG16 architecture\\nandD=2048 dimensions for the ResNet-50 architecture. /T_he\\ncodes x∈RDare the image representations that either network\\nderives as most representative to discriminate the high-level land\\nuse concepts it is trained to predict.\\nWe would like to study how ”similar” diﬀerent classes of urban\\nenvironments are across two example cities (here we picked Berlin\\nand Barcelona, which are fairly diﬀerent from a cultural and archi-\\ntectural standpoint). For this, we focus only on the 25 km×25km,\\n100×100-cell grids around the city center as in Figure 1. To be able\\nto quantify similarity in local urban environments, we construct\\na KD-tree T(using a high-performance implementation available\\nin the Python package scikit-learn [16]) using all the gridded\\nsamples. /T_his data structure allows to /f_ind k-nearest neighbors of a\\nquery image in an eﬃcient way. In this way, the feature space can\\nbe probed in an eﬃcient way.\\n5 RESULTS AND DISCUSSION\\nIn Figure 1 we show model performance on the 100 ×100 (25 km×\\n25km) raster grids we used for testing. /T_he top row shows ground\\ntruth grids, where the class in each cell was assigned as the most\\nprevalent land use class by area (see also Section 3). /T_he bo/t_tom row\\nshows model predictions, where each cell in a raster is painted in\\nthe color corresponding to the maximum probability class estimated\\nby the model (here ResNet-50). Columns in the /f_igure show results\\nfor each of the 6 cities we used in our dataset. Even at a /f_irst visual\\ninspection, the model is able to recreate from satellite imagery\\nqualitatively the urban land use classi/f_ication map.\\nFurther, looking at the individual classes separately and the con-\\n/f_idence of the model in its predictions (the probability distribution\\nover classes computed by the model), the picture is again qualita-\\ntively very encouraging. In Figure 5 we show grayscale raster maps\\nencoding the spatial layout of the class probability distribution for\\none example city, Barcelona. Particularly good qualitative agree-\\nment is observed for agricultural lands, water bodies, industrial,\\npublic, and commercial land, forests, green urban areas, low density\\nurban fabric, airports, and sports and leisure facilities. /T_he model\\nappears to struggle with reconstructing the spatial distribution of\\nroads, which is not unexpected, given that roads typically appear\\nin many other scenes that have a diﬀerent functional classi/f_ication\\nfor urban planning purposes.Table 1: Urban Environments dataset: sample size summary.\\n(a) Dataset used for training & validation ( 80%and20%, respectively)\\nclass/city athina barcelona berlin budapest madrid roma class\\ntotal\\nAgricultural + Semi-\\nnatural areas + Wet-\\nlands4347 2987 7602 2211 4662 4043 25852\\nAirports 382 452 232 138 124 142 1470\\nForests 1806 2438 7397 1550 2685 2057 17933\\nGreen urban areas 990 722 1840 1342 1243 1401 7538\\nHigh Density Urban\\nFabric967 996 8975 6993 2533 3103 23567\\nIndustrial, commer-\\ncial, public, military\\nand pr…1887 2116 4761 1850 3203 2334 16151\\nLow Density Urban\\nFabric1424 1520 2144 575 2794 3689 12146\\nMedium Density Ur-\\nban Fabric2144 1128 6124 1661 1833 2100 14990\\nSports and leisure fa-\\ncilities750 1185 2268 1305 1397 1336 8241\\nWater bodies 537 408 1919 807 805 619 5095\\ncity total 15234 13952 43262 18432 21279 20824 132983(b)25km×25kmground truth test grids (fractions of city total)\\nclass / city athina barcelona berlin budapest madrid roma\\nAgricultural + Semi-\\nnatural areas + Wet-\\nlands0.350 0.261 0.106 0.181 0.395 0.473\\nAirports 0.003 0.030 0.013 0.000 0.044 0.006\\nForests 0.031 0.192 0.087 0.211 0.013 0.019\\nGreen urban areas 0.038 0.030 0.072 0.027 0.125 0.054\\nHigh Density Urban\\nFabric0.389 0.217 0.284 0.365 0.170 0.215\\nIndustrial, commer-\\ncial, public, military\\nand pr…0.109 0.160 0.190 0.096 0.138 0.129\\nLow Density Urban\\nFabric0.016 0.044 0.012 0.006 0.036 0.029\\nMedium Density Ur-\\nban Fabric0.041 0.025 0.129 0.045 0.042 0.047\\nSports and leisure fa-\\ncilities0.017 0.034 0.080 0.025 0.036 0.025\\nWater bodies 0.005 0.006 0.026 0.044 <0.001 0.004\\nFigure 5: Barcelona: ground truth ( top) and predicted probabilities ( bo/t_tom ).\\nTable 2: /T_he VGG16 architecture [19].\\nBlock 1 Block 2 Block 3 Block 4 Block 5 Block 6\\nConv(3,64)\\nConv(3,64)\\nMax-\\nPool(2,2)Conv(3,128)\\nConv(3,128)\\nMax-\\nPool(2,2)Conv(3,256)\\nConv(3,256)\\nConv(3,256)\\nMax-\\nPool(2,2)Conv(3,512)\\nConv(3,512)\\nConv(3,512)\\nMax-\\nPool(2,2)Conv(3,512)\\nConv(3,512)\\nConv(3,512)\\nMax-\\nPool(2,2)FC(4096)\\nFC(4096)\\nFC(Nclasses )\\nSo/f_tMax\\nTable 3: /T_he ResNet-50 architecture [7].\\nBlock 1 Block 2 Block 3 Block 4 Block 5 Block 6\\nConv(7,64)\\nMax-\\nPool(3,2)3x[Conv(1,64)\\nConv(3,64)\\nConv(3,256)]4x[Conv(1,128)\\nConv(3,128)\\nConv(1,512)]6x[Conv(1,256)\\nConv(3,256)\\nConv(1,1024)]3x[Conv(1,512)\\nConv(3,512)\\nConv(1,2048)]FC(Nclasses )\\nSo/f_tMax\\n5.1 Classi/f_ication results\\nWe performed experiments training the two architectures described\\nin Section 4 on datasets for each of the 6 cities considered, and for\\na combined dataset ( all) of all the cities. /T_he diagonal in Figure 6\\nsummarizes the (validation set) classi/f_ication performance for each\\nmodel. All /f_igures are averages computed over balanced subsets\\nof 2000 samples each. While accuracies or ∼0.70−0.80 may not\\nlook as impressive as those obtained by convolutional architectures\\non well-studied benchmarks and other classi/f_ication tasks (e.g.,natural images from ImageNet or small aerial patches from Deep-\\nSat), this only a/t_tests to the diﬃculty of the task of understanding\\nhigh-level, subjective concepts of urban planning in complex ur-\\nban environments. First, satellite imagery typically contains much\\nmore semantic variation than natural images (as also noted, e.g.,\\nin [2,13]), i.e., there is no “central” concept that the image is of\\n(unlike the image of a cat or a /f_lower). Second, the type of labels we\\nuse for supervision are higher-level concepts (such as “low density\\nurban fabric”, or “sports and leisure facilities”), which are much\\nless speci/f_ic than more physical land features e.g., “buildings” or\\n”trees” (which are classes used in the DeepSat dataset). Moreover,\\ntop-down imagery poses speci/f_ic challenges to convolutional archi-\\ntectures, as these models are inherently not rotationally-symmetric.\\nUrban environments, especially from from a top-down point of\\nview, come in many complex layouts, for which rotations are ir-\\nrelevant. Nevertheless, these results are encouraging, especially\\nsince this is a harder problem by focusing on wider-area images and\\non higher-level, subjective concepts used in urban planning rather\\nthan on the standard, lower-level physical features such as in [ 1] or\\n[17]. /T_his suggests that such models may be useful feature extrac-\\ntors. Moreover, as more researchers tackle problems with the aid of\\nsatellite imagery (which is still a relatively under-researched source\\nof data in the machine learning community), more open-sourceFigure 6: Transferability (classi/f_ication accuracy) of models\\nlearned at one location and applied at another. Training on\\na more diverse set of cities ( all) yields encouraging results\\ncompared with just pairwise training/testing.\\ndatasets (like this one) are released, performance will certainly im-\\nprove. For the remainder of this section we report results using\\nthe ResNet-50 architecture [ 7], as it consistently yielded (if only\\nslightly) be/t_ter classi/f_ication results in our experiments than the\\nVGG-16 architecture.\\nTransfer learning and classi/f_ication performance. Next,\\nwe investigated how models trained in one se/t_ting (city or set of\\ncities) perform when applied to other geographical locations. Figure\\n6 summarizes these experiments. In general, performance is poor\\nwhen training on samples from a given city and testing on samples\\nfrom a diﬀerent city (the oﬀ-diagonal terms). /T_his is expected, as\\nthese environments can be very diﬀerent in appearance for cities as\\ndiﬀerent as e.g., Budapest and Barcelona. However, we notice that\\na more diverse set ( all) yields be/t_ter performance when applied at\\ndiﬀerent locations than models trained on individual cities. /T_his is\\nencouraging for our purpose of analyzing the high level “similarity”\\nof urban neighborhoods via satellite imagery.\\nWe next looked at per-class model performance to understand\\nwhat types of environments are harder for the model to distin-\\nguish. Figure 7 shows such an example analysis for three example\\ncities, of which a pair is “similar” according to Figure 6 (Rome and\\nBarcelona), and another dissimilar (Rome and Budapest). /T_he le/f_t\\npanel shows model performance when training on samples from\\nBarcelona, and predicting on test samples from Barcelona (intra-\\ncity). /T_he middle panel shows training on Rome, and predicting\\non test samples in Barcelona, which can be assumed to be “sim-\\nilar” to Rome from a cultural and architectural standpoint (both\\nLatin cities in warm climates). /T_he right /f_igure shows training on\\nBarcelona, and predicting on test samples in Budapest, which can\\nbe assumed a rather diﬀerent city from a cultural and architectural\\nstandpoint. For all cases, the classes that the model most struggles\\nwith are “High Density Urban Fabric”, “Low Density Urban Fabric,\\nand “Medium Density Urban Fabric”. Considerable overlap can be\\nnoticed between these classes - which is not surprising given the\\nhighly subjective nature of these concepts. Other examples where\\nthe model performance is lower is forests and low-density urban\\nareas being sometimes misclassifed as “green urban areas”, which,again, is not surprising. /T_his is especially apparent in the cross-city\\ncase, where the model struggles with telling apart these classes. For\\nboth the case of training and testing on “diﬀerent cities” (Budapest\\nand Barcelona) and on “similar” cities (Rome and Barcelona), we\\nnote that airports and forests are relatively easier to distinguish.\\nHowever, more subjective, high-level urban-planning concepts such\\nas “high density urban fabric” are harder to infer (and more easily\\nconfused with “medium density” or “low density” urban fabric) in\\nthe case of more similar cities (Rome and Barcelona) rather than\\ndissimilar cities (Budapest and Barcelona). Urban environments\\ncontaining sports and leisure facilities and green areas are under\\nthis view more similar between Rome and Barcelona than they are\\nbetween Budapest and Barcelona.\\nChoosing the spatial scale: sensitivity analysis. So far, we\\nhave presented results assuming that tiles of 250 mis an appropriate\\nspatial scale for this analysis. Our intuition suggested that tiles of\\nthis size have enough variation and information to be recognized\\n(even by humans) as belonging to one of the high-level concepts\\nof land use classes that we study in this paper. However, one\\ncan /f_ind arguments in favor of smaller tile sizes, e.g., in many\\ncities the size of a typical city block is 100 m. /T_hus, we trained\\nmodels at diﬀerent spatial scales and computed test-set accuracy\\nvalues for three example cities, Barcelona, Roma, and Budapest\\n- see Figure 8. It is apparent that, for all example cities, smaller\\nspatial scales (50 m, 100m, 150m) that we analyzed yield poorer\\nperformance than the scale we chose for the analysis in this paper\\n(250m). /T_his is likely because images at smaller scales do not capture\\nenough variation in urban form (number and type of buildings,\\nrelative amount of vegetation, roads etc.) to allow for discriminating\\nbetween concepts that are fairly high-level. /T_his is in contrast with a\\nbenchmark such as DeepSat [ 1] that focuses on lower-level, physical\\nconcepts (“trees”, “buildings”, etc.). /T_here, a good spatial scale is\\nby necessity smaller (28 mfor DeepSat), as variation in appearance\\nand compositional elements is unwanted.\\n5.2 Comparing urban environments\\nFinally, we set to understand, at least on an initial qualitative level,\\nhow “similar” urban environments are to one another, across formal\\nland use classes and geographies. Our /f_irst experiment was to\\nproject sample images for each class and city in this analysis to\\nlower-dimensional manifolds, using the t-SNE algorithm [ 23]. /T_his\\nserves the purpose of both visualization (as t-SNE is widely used\\nfor visualizing high-dimensional data), as well as for providing an\\ninitial, coarse continuous representation of urban land use classes.\\nIn our experiments, we used balanced samples of size N=6000, or\\n100 samples for each of the 10 classes for each city. We extracted\\nfeatures for each of these samples using the allmodels (trained\\non a train set with samples across all cities except for the test one).\\nFigure 9 visualizes such t-SNE embeddings for the six cities in\\nour analysis. For most cities, classes such as low density urban\\nfabric, forests, and water bodies are well-resolved, while sports\\nand leisure facilities seem to consistently blend into other types of\\nenvironments (which is not surprising, given that these types of\\nfacilities can be found within many types of locations that have a\\ndiﬀerent formal urban planning class assigned). Intriguing diﬀer-\\nences emerge in this picture among the cities. For example, greenFigure 7: Example classi/f_ication confusion matrix for land use inference. Le/f_t: training and testing on Barcelona; Middle :\\ntraining on Rome, testing on Barcelona; Right: training on Rome, predicting on Budapest.\\nFigure 8: Sensitivity of training patch size vs test accuracy.\\nFigure 9: t-SNE visualization (the /f_irst 2 dimensions) of ur-\\nban environments (satellite image samples) across six cities.\\nurban spaces seem fairly well resolved for most cities. Commercial\\nneighborhoods in Barcelona seem more integrated with the other\\ntypes of environments in the city, whereas for Berlin they appear\\nmore distinct. Urban water bodies are more embedded with urban\\nparks for Barcelona than for other cities. Such reasoning (with\\nmore rigorous quantitative analysis) can serve as coarse way to\\nbenchmark and compare neighborhoods as input to further analysis\\nabout e.g., energy use, livelihood, or traﬃc in urban environments.\\nFigure 10: Comparing urban environments across cities\\n(with reference to Barcelona) We show relative inter-city\\nsimilarity measures computed as the sum of squares across\\nthe clusters in Figure 9.\\nWe further illustrate how “similar” the six cities we used through-\\nout this analysis are starting oﬀ the embeddings plots in Figure 9.\\nFor each land use class, we compute intra-city sum of squares in\\nthe 2-d t-SNE embedding, and display the results in Figure 10. Note\\nthat the distances are always shown with Barcelona as a reference\\npoint (chosen arbitrarily). For each panel, the normalization is with\\nrespect to the largest inter-city distance for that land use class. /T_his\\nvisualization aids quick understanding of similarity between urban\\nenvironments. For example, agricultural lands in Barcelona are\\nmost dissimilar to those in Budapest. Airports in Barcelona are\\nmost similar to those in Athens, and most dissimilar to those in\\nBerlin and Budapest. Barcelona’s forests and parks are most dissim-\\nilar to Budapest’s. Water bodies in Barcelona are very dissimilar to\\nall other cities. /T_his point is enforced by Figure 11 below, which\\nsuggests that areas marked as water bodies in Barcelona are ocean\\nwaterfronts, whereas this class for all other cities represents rivers\\nor lakes.Figure 11: Samples from three urban environments across\\nour 6 example cities. We sampled the 2-d t-SNE embedding\\nof Figure 9 and queried for the closest real sample to the\\ncentroid using an eﬃcient KD-tree search.\\nFinally, we explore the feature maps extracted by the convolu-\\ntional model in order to illustrate how “similar” the six cities we\\nused throughout this analysis are across three example environ-\\nments, green urban areas, water bodies, and medium density urban\\nfabric. For each city and land use class, we start oﬀ the centroid of\\nthe point cloud in the 2-d space of Figure 9, and /f_ind the nearest\\nseveral samples using the KD-tree method described in Section 4.\\nWe present the results in Figure 11. Visual inspection indicates\\nthat the model has learned useful feature maps about urban envi-\\nronments: the sample image patches show a very good qualitative\\nagreement with the region of the space where they’re sampled from,\\nindicated by the land use class of neighboring points. /Q_ualitatively,\\nit is clear that the features extracted from the top layer of the con-\\nvolutional model allow a comparison between urban environments\\nby high-level concepts used in urban planning.\\n6 CONCLUSIONS\\n/T_his paper has investigated the use of convolutional neural net-\\nworks for analyzing urban environments through satellite imagery\\nat the scale of entire cities. Given the current relative dearth of\\nlabeled satellite imagery in the machine learning community, we\\nhave constructed an open dataset of over 140 ,000 samples over 10\\nconsistent land use classes from 6 cities in Europe. As we continue\\nto improve, curate, and expand this dataset, we hope that it can help\\nother researchers in machine learning, smart cities, urban planning,\\nand related /f_ields in their work on understanding cities.\\nWe set out to study similarity and variability across urban envi-\\nronments, as being able to quantify such pa/t_terns will enable richer\\napplications in topics such as urban energy analysis, infrastructure\\nbenchmarking, and socio-economic composition of communities.\\nWe formulated this as a two-step task: /f_irst predicting urban land\\nuse classes from satellite imagery, then turning this (rigid) clas-\\nsi/f_ication into a continuous spectrum by embedding the features\\nextracted from the convolutional classi/f_ier into a lower-dimensional\\nmanifold. We show that the classi/f_ication task achieves encour-\\naging results, given the large variety in physical appearance of\\nurban environments having the same functional class. Moreover,we exemplify how the features extracted from the convolutional\\nnetwork allow for identifying “neighbors” of any given query im-\\nage, allowing a rich comparison analysis of urban environments by\\ntheir visual composition.\\n/T_he analysis in this paper shows that some types urban envi-\\nronments are easier to infer than others, both in the intra- and\\ninter-city cases. For example, in all our experiments, the models\\nhad most trouble telling apart “high”, “medium”, and “low” den-\\nsity urban environments, a/t_testing to the subjectivity of such a\\nhigh-level classi/f_ication for urban planning purposes. However,\\nagricultural lands, forests, and airports tend to be visually similar\\nacross diﬀerent cities - and the amount of relative dissimilarity can\\nbe quanti/f_ied using the methods in this paper. Green urban areas\\n(parks) are generally similar to forests or to leisure facilities, and\\nthe models do be/t_ter in the intra-city case than predicting across\\ncities. How industrial areas look is again less geography-speci/f_ic:\\ninter-city similarity is consistently larger than intra-city similarity.\\nAs such, for several classes we can expect learning to transfer from\\none geography to another. /T_hus, while it is not news that some\\ncities are more “similar” than others (Barcelona is visually closer to\\nAthens than it is to Berlin), the methodology in this paper allows\\nfor a more quantitative and practical comparison of similarity.\\nBy leveraging satellite data (available virtually world-wide), this\\napproach may allow for a low-cost way to analyze urban envi-\\nronments in locations where ground truth information on urban\\nplanning is not available. As future directions of this work, we\\nplan to i)continue to develop more rigorous ways to compare and\\nbenchmark urban neighborhoods, going deeper to physical ele-\\nments (vegetation, buildings, roads etc.); ii)improve and further\\ncurate the open Urban Environments dataset; and iii)extend this\\ntype of analysis to more cities across other geographical locations.\\nA PRACTICAL TRAINING DETAILS.\\nWe split our training data into a training set (80% of the data) and a\\nvalidation set (the remaining 20%). /T_his is separate from the data\\nsampled for the ground truth raster grids for each city, which we\\nonly used at test time. We implemented the architectures in the\\nopen-source deep learning framework Keras7(with a TensorFlow8\\nbackend). In all our experiments, we used popular data augmenta-\\ntion techniques, including random horizontal and vertical /f_lipping\\nof the input images, random shearing (up to 0 .1 radians), random\\nscaling (up to 120%), random rotations (by at most 15 degrees either\\ndirection). Input images were 224 ×224×3 pixels in size (RGB\\nbands). For all experiments, we used stochastic gradient descent\\n(with its Adadelta variant) to optimize the network loss function (a\\nstandard multi-class cross-entropy), starting with a learning rate of\\n0.1, and halving the rate each 10 epochs. We trained our networks\\nfor at most 100 epochs, with 2000 samples in each epoch, stopping\\nthe learning process when the accuracy on the validation set did\\nnot improve for more than 10 epochs. Given the inherent imbalance\\nof the classes, we explicitly enforced that the minibatches used for\\ntraining were relatively balanced by a weighted sampling proce-\\ndure. For training, we used a cluster of 4 NVIDIA K80 GPUs, and\\ntested our models on a cluster of 48 CPUs.\\n7h/t_tps://github.com/fchollet/keras\\n8www.tensor/f_low.org'"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "extracted_text = remove_references(extracted_text)\n",
        "extracted_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "Uw7T0x_h2Rjz",
        "outputId": "d7f9e419-8ac2-4b23-9977-ef3fbcbaad15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Convolutional Networks and Satellite Imagery to Identify\n",
            "Pa/t_terns in Urban Environments at a Large Scale\n",
            "Adrian Albert\n",
            "Massachuse/t_ts Institute of Technology\n",
            "Civil and Environmental Engineering\n",
            "77 Massachuse/t_ts Ave\n",
            "Cambridge, MA 02139\n",
            "adalbert@mit.eduJasleen Kaur\n",
            "Philips Lighting Research North\n",
            "America\n",
            "2 Canal Park\n",
            "Cambridge, MA 02141\n",
            "jasleen.kaur1@philips.comMarta C. Gonz alez\n",
            "Massachuse/t_ts Institute of Technology\n",
            "Civil and Environmental Engineering\n",
            "77 Massachuse/t_ts Ave\n",
            "Cambridge, MA 02139\n",
            "martag@mit.edu\n",
            "ABSTRACT\n",
            "Urban planning applications (energy audits, investment, etc.) re-\n",
            "quire an understanding of built infrastructure and its environment,\n",
            "i.e., both low-level, physical features (amount of vegetation, build-\n",
            "ing area and geometry etc.), as well as higher-level concepts such\n",
            "as land use classes (which encode expert understanding of socio-\n",
            "economic end uses). /T_his kind of data is expensive and labor-\n",
            "intensive to obtain, which limits its availability (particularly in\n",
            "developing countries). We analyze pa/t_terns in land use in urban\n",
            "neighborhoods using large-scale satellite imagery data (which is\n",
            "available worldwide from third-party providers) and state-of-the-\n",
            "art computer vision techniques based on deep convolutional neural\n",
            "networks. For supervision, given the limited availability of standard\n",
            "benchmarks for remote-sensing data, we obtain ground truth land\n",
            "use class labels carefully sampled from open-source surveys, in\n",
            "particular the Urban Atlas land classi/f_ication dataset of 20 land use\n",
            "classes across 300 European cities. We use this data to train and\n",
            "compare deep architectures which have recently shown good per-\n",
            "formance on standard computer vision tasks (image classi/f_ication\n",
            "and segmentation), including on geospatial data. Furthermore, we\n",
            "show that the deep representations extracted from satellite imagery\n",
            "of urban environments can be used to compare neighborhoods\n",
            "across several cities. We make our dataset available for other ma-\n",
            "chine learning researchers to use for remote-sensing applications.\n",
            "CCS CONCEPTS\n",
            "Computing methodologies Computer vision; Neural net-\n",
            "works; Applied computing Environmental sciences;\n",
            "KEYWORDS\n",
            "Satellite imagery, land use classi/f_ication, convolutional networks\n",
            "1 INTRODUCTION\n",
            "Land use classi/f_ication is an important input for applications rang-\n",
            "ing from urban planning, zoning and the issuing of business per-\n",
            "mits, to real-estate construction and evaluation to infrastructure\n",
            "Corresponding author.\n",
            "Permission to make digital or hard copies of part or all of this work for personal or\n",
            "classroom use is granted without fee provided that copies are not made or distributed\n",
            "for pro/f_it or commercial advantage and that copies bear this notice and the full citation\n",
            "on the /f_irst page. Copyrights for third-party components of this work must be honored.\n",
            "For all other uses, contact the owner/author(s).\n",
            "KDD17, August 1317, 2017, Halifax, NS, Canada.\n",
            " 2017 Copyright held by the owner/author(s). 978-1-4503-4887-4/17/08.\n",
            "DOI:  Urban land use classi/f_ication is typically based on\n",
            "surveys performed by trained professionals. As such, this task\n",
            "is labor-intensive, infrequent, slow, and costly. As a result, such\n",
            "data are mostly available in developed countries and big cities that\n",
            "have the resources and the vision necessary to collect and curate it;\n",
            "this information is usually not available in many poorer regions,\n",
            "including many developing countries [ 9] where it is mostly needed.\n",
            "/T_his paper builds on two recent trends that promise to make\n",
            "the analysis of urban environments a more democratic and inclu-\n",
            "sive task. On the one hand, recent years have seen signi/f_icant\n",
            "improvements in satellite technology and its deployment (primar-\n",
            "ily through commercial operators), which allows to obtain high\n",
            "and medium-resolution imagery of most urbanized areas of the\n",
            "Earth with an almost daily revisit rate. On the other hand, the\n",
            "recent breakthroughs in computer vision methods, in particular\n",
            "deep learning models for image classi/f_ication and object detection,\n",
            "now make possible to obtain a much more accurate representation\n",
            "of the composition built infrastructure and its environments.\n",
            "Our contributions are to both the applied deep learning literature,\n",
            "and to the incipient study of smart cities using remote sensing\n",
            "data. We contrast state-of-the-art convolutional architectures (the\n",
            "VGG-16 [ 19] and ResNet [ 7] networks) to train classi/f_iers that recog-\n",
            "nize broad land use classes from satellite imagery. We then use the\n",
            "features extracted from the model to perform a large-scale compar-\n",
            "ison of urban environments. For this, we construct a novel dataset\n",
            "for land use classi/f_ication, pairing carefully sampled locations with\n",
            "ground truth land use class labels obtained from the Urban Atlas\n",
            "survey [ 22] with satellite imagery obtained from Google Mapss\n",
            "static API. Our dataset - which we have made available publicly\n",
            "for other researchers - covers, for now, 10 cities in Europe (chosen\n",
            "out of the original 300) with 10 land use classes (from the original\n",
            "20). As the Urban Atlas is a widely-used, standardized dataset for\n",
            "land use classi/f_ication, we hope that making this dataset available\n",
            "will encourage the development analyses and algorithms for ana-\n",
            "lyzing the built infrastructure in urban environments. Moreover,\n",
            "given that satellite imagery is available virtually everywhere on\n",
            "the globe, the methods presented here allow for automated, rapid\n",
            "classi/f_ication of urban environments that can potentially be applied\n",
            "to locations where survey and zoning data is not available.\n",
            "Land use classi/f_ication refers to the combination of physical\n",
            "land a/t_tributes and what cultural and socio-economic function land\n",
            "serves (which is a subjective judgement by experts) [ 2]. In this paper,\n",
            "we take the view that land use classes are just a useful discretization\n",
            "of a more continuous spectrum of pa/t_terns in the organization of\n",
            "urban environments. /T_his viewpoint is illustrated in Figure 2: while\n",
            "arXiv:1704.02965v2  [cs.CV]  13 Sep 2017Figure 1: Urban land use maps for six example cities. We compare the ground truth ( top row ) with the predicted land use maps,\n",
            "either from using separate data collected from the same city ( middle row ), or using data from all other cities ( bo/t_tom row ).\n",
            "Figure 2: Le/f_t: Comparing urban environments via deep hi-\n",
            "erarchical representations of satellite image samples. Right:\n",
            "approach outline - data collection, classi/f_ication, feature ex-\n",
            "traction, clustering, validation.\n",
            "some a/t_tributes (e.g., amount of built structures or vegetation) are\n",
            "directly interpretable, some others may not be. Nevertheless, these\n",
            "pa/t_terns in/f_luence, and are in/f_luenced by, socio-economic factors\n",
            "(e.g., economic activity), resource use (energy), and dynamic human\n",
            "behavior (e.g., mobility, building occupancy). We see the work\n",
            "on cheaply curating a large-scale land use classi/f_ication dataset\n",
            "and comparing neighborhoods using deep representations that\n",
            "this paper puts forth as a necessary /f_irst step towards a granular\n",
            "understanding of urban environments in data-poor regions.\n",
            "Subsequently, in Section 2 we review related studies that apply\n",
            "deep learning methods and other machine learning techniques\n",
            "to problems of land use classi/f_ication, object detection, and image\n",
            "segmentation in aerial imagery. In Section 3 we describe the datasetwe curated based on the Urban Atlas survey. Section 4 reviews the\n",
            "deep learning architectures we used. Section 5 describes model\n",
            "validation and analysis results. We conclude in Section 6.\n",
            "All the code used to acquire, process, and analyze the data, as\n",
            "well as to train the models discussed in this paper is available at\n",
            "\n",
            "2 LITERATURE\n",
            "/T_he literature on the use of remote sensing data for applications in\n",
            "land use cover, urban planning, environmental science, and others,\n",
            "has a long and rich history. /T_his paper however is concerned more\n",
            "narrowly with newer work that employs widely-available data\n",
            "and machine learning models - and in particular deep learning\n",
            "architectures - to study urban environments.\n",
            "Deep learning methods have only recently started to be deployed\n",
            "to the analysis of satellite imagery. As such, land use classi/f_ication\n",
            "using these tools is still a very incipient literature. Probably the /f_irst\n",
            "studies (yet currently only 1-2 years old) include the application\n",
            "of convolutional neural networks to land use classi/f_ication [ 2] us-\n",
            "ing the UC Merced land use dataset [ 25] (of 2100 images spanning\n",
            "21 classes) and the classi/f_ication of agricultural images of coee\n",
            "plantations [ 17]. Similar early studies on land use classi/f_ication\n",
            "that employ deep learning techniques are [ 21], [18], and [ 15]. In\n",
            "[11], a spatial pyramid pooling technique is employed for land use\n",
            "classi/f_ication using satellite imagery. /T_he authors of these studies\n",
            "adapted architectures pre-trained to recognize natural images from\n",
            "the ImageNet dataset (such as the VGG16 [ 19], which we also use),\n",
            "and /f_ine-tuned them on their (much smaller) land use data. More\n",
            "recent studies use the DeepSat land use benchmark dataset [ 1],\n",
            "which we also use and describe in more detail in Section 2.1. An-\n",
            "other topic that is closely related to ours is remote-sensing image\n",
            "segmentation and object detection, where modern deep learning\n",
            "models have also started to be applied. Some of the earliest work\n",
            "that develops and applies deep neural networks for this tasks is thatof [13]. Examples of recent studies include [ 26] and [ 12], where the\n",
            "authors propose a semantic image segmentation technique com-\n",
            "bining texture features and boundary detection in an end-to-end\n",
            "trainable architecture.\n",
            "Remote-sensing data and deep learning methods have been put\n",
            "to use to other related ends, e.g., geo-localization of ground-level\n",
            "photos via satellite images [ 3,24] or predicting ground-level scene\n",
            "images from corresponding aerial imagery [ 27]. Other applications\n",
            "have included predicting survey estimates on poverty levels in\n",
            "several countries in Africa by /f_irst learning to predict levels of night\n",
            "lights (considered as proxies of economic activity and measured\n",
            "by satellites) from day-time, visual-range imagery from Google\n",
            "Maps, then transferring the learning from this la/t_ter task to the\n",
            "former [ 9]. Our work takes a similar approach, in that we aim to use\n",
            "remote-sensing data (which is widely-available for most parts of\n",
            "the world) to infer land use types in those locations where ground\n",
            "truth surveys are not available.\n",
            "Urban environments have been analyzed using other types of\n",
            "imagery data that have become recently available. In [ 4,14], the\n",
            "authors propose to use the same type of imagery from Google Street\n",
            "View to measure the relationship between urban appearance and\n",
            "quality of life measures such as perceived safety. For this, they\n",
            "hand-cra/f_t standard image features widely used in the computer\n",
            "vision community, and train a shallow machine learning classi/f_ier\n",
            "(a support vector machine). In a similar fashion, [ 5] trained a\n",
            "convolutional neural network on ground-level Street View imagery\n",
            "paired with a crowd-sourced mechanism for collecting ground truth\n",
            "labels to predict subjective perceptions of urban environments such\n",
            "as beauty, wealth, and liveliness.\n",
            "Land use classi/f_ication has been studied with other new data\n",
            "sources in recent years. For example, ground-level imagery has been\n",
            "employed to accurately predict land use classes on an university\n",
            "campus [ 28]. Another related literature strand is work that uses\n",
            "mobile phone call records to extract spatial and temporal mobility\n",
            "pa/t_terns, which are then used to infer land use classes for several\n",
            "cities [ 6,10,20]. Our work builds on some of the ideas for sampling\n",
            "geospatial data presented there.\n",
            "2.1 Existing land use benchmark datasets\n",
            "Public benchmark data for land use classi/f_ication using aerial im-\n",
            "agery are still in relatively short supply. Presently there are two\n",
            "such datasets that we are aware of, discussed below.\n",
            "UC Merced. /T_his dataset was published in 2010 [ 25] and con-\n",
            "tains 2100 256 256, 1 m/pxaerial RGB images over 21 land use\n",
            "classes. It is considered a solved problem, as modern neural net-\n",
            "work based classi/f_iers [2] have achieved >95% accuracy on it.\n",
            "DeepSat. /T_he DeepSat [ 1] dataset1was released in 2015. It\n",
            "contains two benchmarks: the Sat-4 data of 500 ,000 images over 4\n",
            "land use classes ( barren land, trees, grassland, other ), and the Sat-6\n",
            "data of 405 ,000 images over 6 land use classes ( barren land, trees,\n",
            "grassland, roads, buildings, water bodies ). All the samples are 28 28\n",
            "in size at a 1 m/pxspatial resolution and contain 4 channels (red,\n",
            "green, blue, and NIR - near infrared). Currently less than two years\n",
            "old, this dataset is already a solved problem, with previous studies\n",
            "[15] (and our own experiments) achieving classi/f_ication accuracies\n",
            "1Available at  saikat/deepsat/.of over 99% using convolutional architectures. While useful as input\n",
            "for pre-training more complex models, (e.g., image segmentation),\n",
            "this dataset does not allow to take the further steps for detailed\n",
            "land use analysis and comparison of urban environments across\n",
            "cities, which gap we hope our dataset will address.\n",
            "Other open-source eorts. /T_here are several other projects\n",
            "that we are aware of related to land use classi/f_ication using open-\n",
            "source data. /T_he TerraPa/t_tern2project uses satellite imagery from\n",
            "Google Maps (just like we do) paired with truth labels over a large\n",
            "number (450) of detailed classes obtained using the Open Street\n",
            "Map API3. (Open Street Maps is a comprehensive, open-access,\n",
            "crowd-sourced mapping system.) /T_he projects intended use is as\n",
            "a search tool for satellite imagery, and as such, the classes they\n",
            "employ are very speci/f_ic, e.g., baseball diamonds, churches, or\n",
            "roundabouts. /T_he authors use a ResNet architecture [ 7] to train a\n",
            "classi/f_ication model, which they use to embed images in a high-\n",
            "dimensional feature space, where similar images to an input image\n",
            "can be identi/f_ied. A second open-source project related to ours is\n",
            "the DeepOSM4, in which the authors take the same approach of\n",
            "pairing OpenStreetMap labels with satellite imagery obtained from\n",
            "Google Maps, and use a convolutional architecture for classi/f_ication.\n",
            "/T_hese are excellent starting points from a practical standpoint,\n",
            "allowing interested researchers to quickly familiarize themselves\n",
            "with programming aspects of data collection, API calls, etc.\n",
            "3 THE URBAN ENVIRONMENTS DATASET\n",
            "3.1 Urban Atlas: a standard in land use analysis\n",
            "/T_he Urban Atlas [ 22] is an open-source, standardized land use\n",
            "dataset that covers 300 European cities of 100 ,000 inhabitants or\n",
            "more, distributed relatively evenly across major geographical and\n",
            "geopolitical regions. /T_he dataset was created between 2005-2011 as\n",
            "part of a major eort by the European Union to provide a uniform\n",
            "framework for the geospatial analysis of urban areas in Europe.\n",
            "Land use classi/f_ication is encoded via detailed polygons organized\n",
            "in commonly-used GIS/ESRI shape /f_iles. /T_he dataset covers 20\n",
            "standardized land use classes. In this work we selected classes of\n",
            "interest and consolidated them into 10 /f_inal classes used for analysis\n",
            "(see Figure 3). Producing the original Urban Atlas dataset required\n",
            "fusing several data sources: high and medium-resolution satellite\n",
            "imagery, topographic maps, navigation and road layout data, and\n",
            "local zoning (cadastral) databases. More information on the method-\n",
            "ology used by the Urban Atlas researchers can be obtained from\n",
            "the European Environment Agency5. We chose expressly to use\n",
            "the Urban Atlas dataset over other sources (described in Section 2.1\n",
            "because i)it is a comprehensive and consistent survey at a large\n",
            "scale, which has been extensively curated by experts and used in\n",
            "research, planning, and socio-economic work over the past decade,\n",
            "andii)the land use classes re/f_lect higher-level (socio-economic,\n",
            "cultural) functions of the land as used in applications.\n",
            "We note that there is a wide variance in the distribution of land\n",
            "use classes across and within the 300 cities. Figure 3 illustrates\n",
            "the dierences in the distribution in ground truth polygon areas\n",
            "2\n",
            "3\n",
            "4\n",
            "5 3: Ground truth land use distribution (by area) for\n",
            "three example cities in the Urban Environments dataset.\n",
            "for each of the classes for three example cities (Budapest, Rome,\n",
            "Barcelona) from the dataset (from Eastern, Central, and Western\n",
            "Europe, respectively). /T_his wide disparity in the spatial distribution\n",
            "pa/t_terns of dierent land use classes and across dierent cities\n",
            "motivates us to design a careful sampling procedure for collecting\n",
            "training data, described in detail below.\n",
            "3.2 Data sampling and acquisition\n",
            "We set out to develop a strategy to obtain high-quality samples\n",
            "of the type (satellite image, ground truth label) to use in training\n",
            "convolutional architectures for image classi/f_ication. Our /f_irst re-\n",
            "quirement is to do this solely with freely-available data sources,\n",
            "as to keep costs very low or close to zero. For this, we chose to\n",
            "use the Google Maps Static API6as a source of satellite imagery.\n",
            "/T_his service allows for 25 ,000 API requests/day free of charge. For\n",
            "a given sampling location given by (latitude, longitude), we ob-\n",
            "tained 224 224 images at a zoom level 17 (around 1 .20m/pxspatial\n",
            "resolution, or 250m250mcoverage for an image).\n",
            "/T_he goals of our sampling strategy are twofold. First, we want\n",
            "to ensure that the resulting dataset is as much as possible balanced\n",
            "with respect to the land use classes. /T_he challenge is that the classes\n",
            "are highly imbalanced among the ground truth polygons in the\n",
            "dataset (e.g., many more polygons are agricultural land and isolated\n",
            "structures than airports). Second, the satellite images should be\n",
            "representative of the ground truth class associated to them. To this\n",
            "end, we require that the image contain at least 25% (by area) of\n",
            "the associated ground truth polygon. /T_hus, our strategy to obtain\n",
            "training samples is as follows (for a given city):\n",
            "Sort ground truth polygons in decreasing order according to\n",
            "their size, and retain only those polygons with areas larger than\n",
            "1\n",
            "4(2241.2m)2=0.06km2;\n",
            "From each decile of the distribution of areas, sample a propor-\n",
            "tionally larger number of polygons, such that some of the smaller\n",
            "polygons also are picked, and more of the larger ones;\n",
            "For each picked polygon, sample a number of images propor-\n",
            "tional to the area of the polygon, and assign each image the\n",
            "polygon class as ground truth label;\n",
            "6\n",
            "Figure 4: Example satellite images for the original land use\n",
            "classes in the Urban Atlas dataset.\n",
            "Example satellite images for each of the 10 land use classes in\n",
            "the Urban Environments dataset are given in Figure 4. Note the\n",
            "signi/f_icant variety (in color schemes, textures, etc) in environments\n",
            "denoted as having the same land use class. /T_his is because of several\n",
            "factors, including the time of the year when the image was acquired\n",
            "(e.g., agricultural lands appear dierent in the spring than in the\n",
            "fall), the dierent physical form and appearance of environments\n",
            "that serve the same socioeconomic or cultural function (e.g., green\n",
            "urban areas may look very dierent in dierent cities or in even\n",
            "in dierent parts of the same city; what counts as dense urban\n",
            "fabric in one city may not be dense at all in other cities), and\n",
            "change in the landscape during the several years that have passed\n",
            "since the compilation of the Urban Atlas dataset and the time of\n",
            "acquisition of the satellite image (e.g., construction sites may not\n",
            "re/f_lect accurately anymore the reality on the ground).\n",
            "Apart from these training images, we constructed ground truth\n",
            "rasters to validate model output for each city. For that, we de/f_ined\n",
            "uniform validation grids of 100 100 (25km25km )around the\n",
            "(geographical) center of a given city of interest. We take a satellite\n",
            "image sample in each grid cell, and assign to it as label the class\n",
            "of the polygon that has the maximum intersection area with that\n",
            "cell. Examples of land use maps for the six cities we analyze here\n",
            "are given in Figure 1 (top row). /T_here, each grid cell is assigned the\n",
            "class of the ground truth polygon whose intersection with the cell\n",
            "has maximum coverage fraction by area. Classes are color-coded\n",
            "following the original Urban Atlas documentation.\n",
            "In Table 1 we present summaries of the training (le/f_t) and vali-\n",
            "dation (right) datasets we used for the analysis in this paper. /T_he\n",
            "validation dataset consists of the images sampled at the centers of\n",
            "each cell in the 25 km25kmgrid as discussed above. /T_his dataset\n",
            "consists of 140,000 images distributed across 10 urban environ-\n",
            "ment classes from 6 cities: Roma (Rome), Madrid, Berlin, Budapest,\n",
            "Barcelona, and Athina (Athens). Because of the high variation in\n",
            "appearance upon visual inspection, we chose to consolidate severalclasses from the original dataset, in particular classes that indicated\n",
            "urban fabric into High Density Urban Fabric, Medium Density\n",
            "Urban Fabric, and Low Density Urban Fabric. As mentioned above\n",
            "and illustrated in Figure 3, we did notice a great disparity in the\n",
            "numbers and distribution of ground truth polygons for other ex-\n",
            "ample cities that we investigated in the Urban Atlas dataset. As\n",
            "such,for the analysis in this paper, we have chosen cities where\n",
            "enough ground truth polygons were available for each class (that\n",
            "is, at least 50 samples) to allow for statistical comparisons.\n",
            "4 EXPERIMENTAL SETUP\n",
            "4.1 Neural network architectures and training\n",
            "For all experiments in this paper we compared the VGG-16 [ 19]\n",
            "and ResNet [7, 8] architectures.\n",
            "VGG-16. /T_his architecture [ 19] has become one of the most pop-\n",
            "ular models in computer vision for classi/f_ication and segmentation\n",
            "tasks. It consists of 16 trainable layers organized in blocks. It starts\n",
            "with a 5-block convolutional base of neurons with 3 3 receptive\n",
            "/f_ields (alternated with max-pooling layers that eectively increase\n",
            "the receptive /f_ield of neurons further downstream). Following each\n",
            "convolutional layer is a ReLU activation function [ 19]. /T_he feature\n",
            "maps thus obtained are fed into a set of fully-connected layers (a\n",
            "deep neural network classi/f_ier). See Table 2 for a summary.\n",
            "ResNet. /T_his architecture [ 7,8] has achieved state-of-the-art\n",
            "performance on image classi/f_ication on several popular natural\n",
            "image benchmark datasets. It consists of blocks of convolutional\n",
            "layers, each of which is followed by a ReLU non-linearity. As before,\n",
            "each block in the convolutional base is followed by a max-pooling\n",
            "operation. Finally, the output of the last convolutional layer serves\n",
            "as input feature map for a fully-connected layer with a so/f_tmax\n",
            "activation function. /T_he key dierence in this architecture is that\n",
            "shortcut connections are implemented that skip blocks of convo-\n",
            "lutional layers, allowing the network to learn residual mappings\n",
            "between layer input and output. Here we used an implementation\n",
            "with 50 trainable layers per [7]. See Table 3 for a summary.\n",
            "Transfer learning. As it is common practice in the literature,\n",
            "we have experimented with training our models on the problem of\n",
            "interest (urban environment classi/f_ication) starting from architec-\n",
            "tures pre-trained on datasets from other domains ( transfer learning ).\n",
            "/T_his procedure has been shown to yield both be/t_ter performance\n",
            "and faster training times, as the network already has learned to\n",
            "recognize basic shapes and pa/t_terns that are characteristic of im-\n",
            "ages across many domains (e.g., [ 9,12,15]). We have implemented\n",
            "the following approaches: 1)we used models pre-trained on the\n",
            "ImageNet dataset, then further /f_ine-tuned them on the Urban Atlas\n",
            "dataset; and 2)we pre-trained on the DeepSat dataset (See Section\n",
            "2), then further re/f_ined on the Urban Atlas dataset. As expected,\n",
            "the la/t_ter strategy - /f_irst training a model (itself pre-trained on Ima-\n",
            "geNet data) on the DeepSat benchmark, and the further re/f_ining\n",
            "on the Urban Atlas dataset - yielded the best results, achieving\n",
            "increases of around 5% accuracy for a given training time.\n",
            "Given the large amount of variation in the visual appearance\n",
            "of urban environments across dierent cities (because of dierent\n",
            "climates, dierent architecture styles, various other socio-economic\n",
            "factors), it is of interest to study to what extent a model learned on\n",
            "one geographical location can be applied to a dierent geographicallocation. As such, we perform experiments in which we train a\n",
            "model for one (or more) cities, then apply the model to a dierent set\n",
            "of cities. Intuitively, one would expect that, the more neighborhoods\n",
            "and other urban features at one location are similar to those at a\n",
            "dierent location, the be/t_ter learning would transfer, and the higher\n",
            "the classi/f_ication accuracy obtained would be. Results for these\n",
            "experiments are summarized in Figure 6.\n",
            "4.2 Comparing urban environments\n",
            "We next used the convolutional architectures to extract features\n",
            "for validation images. As in other recent studies (e.g., [ 9]), we use\n",
            "the last layer of a network as feature extractor. /T_his amounts to\n",
            "feature vectors of D=4096 dimensions for the VGG16 architecture\n",
            "andD=2048 dimensions for the ResNet-50 architecture. /T_he\n",
            "codes xRDare the image representations that either network\n",
            "derives as most representative to discriminate the high-level land\n",
            "use concepts it is trained to predict.\n",
            "We would like to study how similar dierent classes of urban\n",
            "environments are across two example cities (here we picked Berlin\n",
            "and Barcelona, which are fairly dierent from a cultural and archi-\n",
            "tectural standpoint). For this, we focus only on the 25 km25km,\n",
            "100100-cell grids around the city center as in Figure 1. To be able\n",
            "to quantify similarity in local urban environments, we construct\n",
            "a KD-tree T(using a high-performance implementation available\n",
            "in the Python package scikit-learn [16]) using all the gridded\n",
            "samples. /T_his data structure allows to /f_ind k-nearest neighbors of a\n",
            "query image in an ecient way. In this way, the feature space can\n",
            "be probed in an ecient way.\n",
            "5 RESULTS AND DISCUSSION\n",
            "In Figure 1 we show model performance on the 100 100 (25 km\n",
            "25km) raster grids we used for testing. /T_he top row shows ground\n",
            "truth grids, where the class in each cell was assigned as the most\n",
            "prevalent land use class by area (see also Section 3). /T_he bo/t_tom row\n",
            "shows model predictions, where each cell in a raster is painted in\n",
            "the color corresponding to the maximum probability class estimated\n",
            "by the model (here ResNet-50). Columns in the /f_igure show results\n",
            "for each of the 6 cities we used in our dataset. Even at a /f_irst visual\n",
            "inspection, the model is able to recreate from satellite imagery\n",
            "qualitatively the urban land use classi/f_ication map.\n",
            "Further, looking at the individual classes separately and the con-\n",
            "/f_idence of the model in its predictions (the probability distribution\n",
            "over classes computed by the model), the picture is again qualita-\n",
            "tively very encouraging. In Figure 5 we show grayscale raster maps\n",
            "encoding the spatial layout of the class probability distribution for\n",
            "one example city, Barcelona. Particularly good qualitative agree-\n",
            "ment is observed for agricultural lands, water bodies, industrial,\n",
            "public, and commercial land, forests, green urban areas, low density\n",
            "urban fabric, airports, and sports and leisure facilities. /T_he model\n",
            "appears to struggle with reconstructing the spatial distribution of\n",
            "roads, which is not unexpected, given that roads typically appear\n",
            "in many other scenes that have a dierent functional classi/f_ication\n",
            "for urban planning purposes.Table 1: Urban Environments dataset: sample size summary.\n",
            "(a) Dataset used for training & validation ( 80%and20%, respectively)\n",
            "class/city athina barcelona berlin budapest madrid roma class\n",
            "total\n",
            "Agricultural + Semi-\n",
            "natural areas + Wet-\n",
            "lands4347 2987 7602 2211 4662 4043 25852\n",
            "Airports 382 452 232 138 124 142 1470\n",
            "Forests 1806 2438 7397 1550 2685 2057 17933\n",
            "Green urban areas 990 722 1840 1342 1243 1401 7538\n",
            "High Density Urban\n",
            "Fabric967 996 8975 6993 2533 3103 23567\n",
            "Industrial, commer-\n",
            "cial, public, military\n",
            "and pr1887 2116 4761 1850 3203 2334 16151\n",
            "Low Density Urban\n",
            "Fabric1424 1520 2144 575 2794 3689 12146\n",
            "Medium Density Ur-\n",
            "ban Fabric2144 1128 6124 1661 1833 2100 14990\n",
            "Sports and leisure fa-\n",
            "cilities750 1185 2268 1305 1397 1336 8241\n",
            "Water bodies 537 408 1919 807 805 619 5095\n",
            "city total 15234 13952 43262 18432 21279 20824 132983(b)25km25kmground truth test grids (fractions of city total)\n",
            "class / city athina barcelona berlin budapest madrid roma\n",
            "Agricultural + Semi-\n",
            "natural areas + Wet-\n",
            "lands0.350 0.261 0.106 0.181 0.395 0.473\n",
            "Airports 0.003 0.030 0.013 0.000 0.044 0.006\n",
            "Forests 0.031 0.192 0.087 0.211 0.013 0.019\n",
            "Green urban areas 0.038 0.030 0.072 0.027 0.125 0.054\n",
            "High Density Urban\n",
            "Fabric0.389 0.217 0.284 0.365 0.170 0.215\n",
            "Industrial, commer-\n",
            "cial, public, military\n",
            "and pr0.109 0.160 0.190 0.096 0.138 0.129\n",
            "Low Density Urban\n",
            "Fabric0.016 0.044 0.012 0.006 0.036 0.029\n",
            "Medium Density Ur-\n",
            "ban Fabric0.041 0.025 0.129 0.045 0.042 0.047\n",
            "Sports and leisure fa-\n",
            "cilities0.017 0.034 0.080 0.025 0.036 0.025\n",
            "Water bodies 0.005 0.006 0.026 0.044 <0.001 0.004\n",
            "Figure 5: Barcelona: ground truth ( top) and predicted probabilities ( bo/t_tom ).\n",
            "Table 2: /T_he VGG16 architecture [19].\n",
            "Block 1 Block 2 Block 3 Block 4 Block 5 Block 6\n",
            "Conv(3,64)\n",
            "Conv(3,64)\n",
            "Max-\n",
            "Pool(2,2)Conv(3,128)\n",
            "Conv(3,128)\n",
            "Max-\n",
            "Pool(2,2)Conv(3,256)\n",
            "Conv(3,256)\n",
            "Conv(3,256)\n",
            "Max-\n",
            "Pool(2,2)Conv(3,512)\n",
            "Conv(3,512)\n",
            "Conv(3,512)\n",
            "Max-\n",
            "Pool(2,2)Conv(3,512)\n",
            "Conv(3,512)\n",
            "Conv(3,512)\n",
            "Max-\n",
            "Pool(2,2)FC(4096)\n",
            "FC(4096)\n",
            "FC(Nclasses )\n",
            "So/f_tMax\n",
            "Table 3: /T_he ResNet-50 architecture [7].\n",
            "Block 1 Block 2 Block 3 Block 4 Block 5 Block 6\n",
            "Conv(7,64)\n",
            "Max-\n",
            "Pool(3,2)3x[Conv(1,64)\n",
            "Conv(3,64)\n",
            "Conv(3,256)]4x[Conv(1,128)\n",
            "Conv(3,128)\n",
            "Conv(1,512)]6x[Conv(1,256)\n",
            "Conv(3,256)\n",
            "Conv(1,1024)]3x[Conv(1,512)\n",
            "Conv(3,512)\n",
            "Conv(1,2048)]FC(Nclasses )\n",
            "So/f_tMax\n",
            "5.1 Classi/f_ication results\n",
            "We performed experiments training the two architectures described\n",
            "in Section 4 on datasets for each of the 6 cities considered, and for\n",
            "a combined dataset ( all) of all the cities. /T_he diagonal in Figure 6\n",
            "summarizes the (validation set) classi/f_ication performance for each\n",
            "model. All /f_igures are averages computed over balanced subsets\n",
            "of 2000 samples each. While accuracies or 0.700.80 may not\n",
            "look as impressive as those obtained by convolutional architectures\n",
            "on well-studied benchmarks and other classi/f_ication tasks (e.g.,natural images from ImageNet or small aerial patches from Deep-\n",
            "Sat), this only a/t_tests to the diculty of the task of understanding\n",
            "high-level, subjective concepts of urban planning in complex ur-\n",
            "ban environments. First, satellite imagery typically contains much\n",
            "more semantic variation than natural images (as also noted, e.g.,\n",
            "in [2,13]), i.e., there is no central concept that the image is of\n",
            "(unlike the image of a cat or a /f_lower). Second, the type of labels we\n",
            "use for supervision are higher-level concepts (such as low density\n",
            "urban fabric, or sports and leisure facilities), which are much\n",
            "less speci/f_ic than more physical land features e.g., buildings or\n",
            "trees (which are classes used in the DeepSat dataset). Moreover,\n",
            "top-down imagery poses speci/f_ic challenges to convolutional archi-\n",
            "tectures, as these models are inherently not rotationally-symmetric.\n",
            "Urban environments, especially from from a top-down point of\n",
            "view, come in many complex layouts, for which rotations are ir-\n",
            "relevant. Nevertheless, these results are encouraging, especially\n",
            "since this is a harder problem by focusing on wider-area images and\n",
            "on higher-level, subjective concepts used in urban planning rather\n",
            "than on the standard, lower-level physical features such as in [ 1] or\n",
            "[17]. /T_his suggests that such models may be useful feature extrac-\n",
            "tors. Moreover, as more researchers tackle problems with the aid of\n",
            "satellite imagery (which is still a relatively under-researched source\n",
            "of data in the machine learning community), more open-sourceFigure 6: Transferability (classi/f_ication accuracy) of models\n",
            "learned at one location and applied at another. Training on\n",
            "a more diverse set of cities ( all) yields encouraging results\n",
            "compared with just pairwise training/testing.\n",
            "datasets (like this one) are released, performance will certainly im-\n",
            "prove. For the remainder of this section we report results using\n",
            "the ResNet-50 architecture [ 7], as it consistently yielded (if only\n",
            "slightly) be/t_ter classi/f_ication results in our experiments than the\n",
            "VGG-16 architecture.\n",
            "Transfer learning and classi/f_ication performance. Next,\n",
            "we investigated how models trained in one se/t_ting (city or set of\n",
            "cities) perform when applied to other geographical locations. Figure\n",
            "6 summarizes these experiments. In general, performance is poor\n",
            "when training on samples from a given city and testing on samples\n",
            "from a dierent city (the o-diagonal terms). /T_his is expected, as\n",
            "these environments can be very dierent in appearance for cities as\n",
            "dierent as e.g., Budapest and Barcelona. However, we notice that\n",
            "a more diverse set ( all) yields be/t_ter performance when applied at\n",
            "dierent locations than models trained on individual cities. /T_his is\n",
            "encouraging for our purpose of analyzing the high level similarity\n",
            "of urban neighborhoods via satellite imagery.\n",
            "We next looked at per-class model performance to understand\n",
            "what types of environments are harder for the model to distin-\n",
            "guish. Figure 7 shows such an example analysis for three example\n",
            "cities, of which a pair is similar according to Figure 6 (Rome and\n",
            "Barcelona), and another dissimilar (Rome and Budapest). /T_he le/f_t\n",
            "panel shows model performance when training on samples from\n",
            "Barcelona, and predicting on test samples from Barcelona (intra-\n",
            "city). /T_he middle panel shows training on Rome, and predicting\n",
            "on test samples in Barcelona, which can be assumed to be sim-\n",
            "ilar to Rome from a cultural and architectural standpoint (both\n",
            "Latin cities in warm climates). /T_he right /f_igure shows training on\n",
            "Barcelona, and predicting on test samples in Budapest, which can\n",
            "be assumed a rather dierent city from a cultural and architectural\n",
            "standpoint. For all cases, the classes that the model most struggles\n",
            "with are High Density Urban Fabric, Low Density Urban Fabric,\n",
            "and Medium Density Urban Fabric. Considerable overlap can be\n",
            "noticed between these classes - which is not surprising given the\n",
            "highly subjective nature of these concepts. Other examples where\n",
            "the model performance is lower is forests and low-density urban\n",
            "areas being sometimes misclassifed as green urban areas, which,again, is not surprising. /T_his is especially apparent in the cross-city\n",
            "case, where the model struggles with telling apart these classes. For\n",
            "both the case of training and testing on dierent cities (Budapest\n",
            "and Barcelona) and on similar cities (Rome and Barcelona), we\n",
            "note that airports and forests are relatively easier to distinguish.\n",
            "However, more subjective, high-level urban-planning concepts such\n",
            "as high density urban fabric are harder to infer (and more easily\n",
            "confused with medium density or low density urban fabric) in\n",
            "the case of more similar cities (Rome and Barcelona) rather than\n",
            "dissimilar cities (Budapest and Barcelona). Urban environments\n",
            "containing sports and leisure facilities and green areas are under\n",
            "this view more similar between Rome and Barcelona than they are\n",
            "between Budapest and Barcelona.\n",
            "Choosing the spatial scale: sensitivity analysis. So far, we\n",
            "have presented results assuming that tiles of 250 mis an appropriate\n",
            "spatial scale for this analysis. Our intuition suggested that tiles of\n",
            "this size have enough variation and information to be recognized\n",
            "(even by humans) as belonging to one of the high-level concepts\n",
            "of land use classes that we study in this paper. However, one\n",
            "can /f_ind arguments in favor of smaller tile sizes, e.g., in many\n",
            "cities the size of a typical city block is 100 m. /T_hus, we trained\n",
            "models at dierent spatial scales and computed test-set accuracy\n",
            "values for three example cities, Barcelona, Roma, and Budapest\n",
            "- see Figure 8. It is apparent that, for all example cities, smaller\n",
            "spatial scales (50 m, 100m, 150m) that we analyzed yield poorer\n",
            "performance than the scale we chose for the analysis in this paper\n",
            "(250m). /T_his is likely because images at smaller scales do not capture\n",
            "enough variation in urban form (number and type of buildings,\n",
            "relative amount of vegetation, roads etc.) to allow for discriminating\n",
            "between concepts that are fairly high-level. /T_his is in contrast with a\n",
            "benchmark such as DeepSat [ 1] that focuses on lower-level, physical\n",
            "concepts (trees, buildings, etc.). /T_here, a good spatial scale is\n",
            "by necessity smaller (28 mfor DeepSat), as variation in appearance\n",
            "and compositional elements is unwanted.\n",
            "5.2 Comparing urban environments\n",
            "Finally, we set to understand, at least on an initial qualitative level,\n",
            "how similar urban environments are to one another, across formal\n",
            "land use classes and geographies. Our /f_irst experiment was to\n",
            "project sample images for each class and city in this analysis to\n",
            "lower-dimensional manifolds, using the t-SNE algorithm [ 23]. /T_his\n",
            "serves the purpose of both visualization (as t-SNE is widely used\n",
            "for visualizing high-dimensional data), as well as for providing an\n",
            "initial, coarse continuous representation of urban land use classes.\n",
            "In our experiments, we used balanced samples of size N=6000, or\n",
            "100 samples for each of the 10 classes for each city. We extracted\n",
            "features for each of these samples using the allmodels (trained\n",
            "on a train set with samples across all cities except for the test one).\n",
            "Figure 9 visualizes such t-SNE embeddings for the six cities in\n",
            "our analysis. For most cities, classes such as low density urban\n",
            "fabric, forests, and water bodies are well-resolved, while sports\n",
            "and leisure facilities seem to consistently blend into other types of\n",
            "environments (which is not surprising, given that these types of\n",
            "facilities can be found within many types of locations that have a\n",
            "dierent formal urban planning class assigned). Intriguing dier-\n",
            "ences emerge in this picture among the cities. For example, greenFigure 7: Example classi/f_ication confusion matrix for land use inference. Le/f_t: training and testing on Barcelona; Middle :\n",
            "training on Rome, testing on Barcelona; Right: training on Rome, predicting on Budapest.\n",
            "Figure 8: Sensitivity of training patch size vs test accuracy.\n",
            "Figure 9: t-SNE visualization (the /f_irst 2 dimensions) of ur-\n",
            "ban environments (satellite image samples) across six cities.\n",
            "urban spaces seem fairly well resolved for most cities. Commercial\n",
            "neighborhoods in Barcelona seem more integrated with the other\n",
            "types of environments in the city, whereas for Berlin they appear\n",
            "more distinct. Urban water bodies are more embedded with urban\n",
            "parks for Barcelona than for other cities. Such reasoning (with\n",
            "more rigorous quantitative analysis) can serve as coarse way to\n",
            "benchmark and compare neighborhoods as input to further analysis\n",
            "about e.g., energy use, livelihood, or trac in urban environments.\n",
            "Figure 10: Comparing urban environments across cities\n",
            "(with reference to Barcelona) We show relative inter-city\n",
            "similarity measures computed as the sum of squares across\n",
            "the clusters in Figure 9.\n",
            "We further illustrate how similar the six cities we used through-\n",
            "out this analysis are starting o the embeddings plots in Figure 9.\n",
            "For each land use class, we compute intra-city sum of squares in\n",
            "the 2-d t-SNE embedding, and display the results in Figure 10. Note\n",
            "that the distances are always shown with Barcelona as a reference\n",
            "point (chosen arbitrarily). For each panel, the normalization is with\n",
            "respect to the largest inter-city distance for that land use class. /T_his\n",
            "visualization aids quick understanding of similarity between urban\n",
            "environments. For example, agricultural lands in Barcelona are\n",
            "most dissimilar to those in Budapest. Airports in Barcelona are\n",
            "most similar to those in Athens, and most dissimilar to those in\n",
            "Berlin and Budapest. Barcelonas forests and parks are most dissim-\n",
            "ilar to Budapests. Water bodies in Barcelona are very dissimilar to\n",
            "all other cities. /T_his point is enforced by Figure 11 below, which\n",
            "suggests that areas marked as water bodies in Barcelona are ocean\n",
            "waterfronts, whereas this class for all other cities represents rivers\n",
            "or lakes.Figure 11: Samples from three urban environments across\n",
            "our 6 example cities. We sampled the 2-d t-SNE embedding\n",
            "of Figure 9 and queried for the closest real sample to the\n",
            "centroid using an ecient KD-tree search.\n",
            "Finally, we explore the feature maps extracted by the convolu-\n",
            "tional model in order to illustrate how similar the six cities we\n",
            "used throughout this analysis are across three example environ-\n",
            "ments, green urban areas, water bodies, and medium density urban\n",
            "fabric. For each city and land use class, we start o the centroid of\n",
            "the point cloud in the 2-d space of Figure 9, and /f_ind the nearest\n",
            "several samples using the KD-tree method described in Section 4.\n",
            "We present the results in Figure 11. Visual inspection indicates\n",
            "that the model has learned useful feature maps about urban envi-\n",
            "ronments: the sample image patches show a very good qualitative\n",
            "agreement with the region of the space where theyre sampled from,\n",
            "indicated by the land use class of neighboring points. /Q_ualitatively,\n",
            "it is clear that the features extracted from the top layer of the con-\n",
            "volutional model allow a comparison between urban environments\n",
            "by high-level concepts used in urban planning.\n",
            "6 CONCLUSIONS\n",
            "/T_his paper has investigated the use of convolutional neural net-\n",
            "works for analyzing urban environments through satellite imagery\n",
            "at the scale of entire cities. Given the current relative dearth of\n",
            "labeled satellite imagery in the machine learning community, we\n",
            "have constructed an open dataset of over 140 ,000 samples over 10\n",
            "consistent land use classes from 6 cities in Europe. As we continue\n",
            "to improve, curate, and expand this dataset, we hope that it can help\n",
            "other researchers in machine learning, smart cities, urban planning,\n",
            "and related /f_ields in their work on understanding cities.\n",
            "We set out to study similarity and variability across urban envi-\n",
            "ronments, as being able to quantify such pa/t_terns will enable richer\n",
            "applications in topics such as urban energy analysis, infrastructure\n",
            "benchmarking, and socio-economic composition of communities.\n",
            "We formulated this as a two-step task: /f_irst predicting urban land\n",
            "use classes from satellite imagery, then turning this (rigid) clas-\n",
            "si/f_ication into a continuous spectrum by embedding the features\n",
            "extracted from the convolutional classi/f_ier into a lower-dimensional\n",
            "manifold. We show that the classi/f_ication task achieves encour-\n",
            "aging results, given the large variety in physical appearance of\n",
            "urban environments having the same functional class. Moreover,we exemplify how the features extracted from the convolutional\n",
            "network allow for identifying neighbors of any given query im-\n",
            "age, allowing a rich comparison analysis of urban environments by\n",
            "their visual composition.\n",
            "/T_he analysis in this paper shows that some types urban envi-\n",
            "ronments are easier to infer than others, both in the intra- and\n",
            "inter-city cases. For example, in all our experiments, the models\n",
            "had most trouble telling apart high, medium, and low den-\n",
            "sity urban environments, a/t_testing to the subjectivity of such a\n",
            "high-level classi/f_ication for urban planning purposes. However,\n",
            "agricultural lands, forests, and airports tend to be visually similar\n",
            "across dierent cities - and the amount of relative dissimilarity can\n",
            "be quanti/f_ied using the methods in this paper. Green urban areas\n",
            "(parks) are generally similar to forests or to leisure facilities, and\n",
            "the models do be/t_ter in the intra-city case than predicting across\n",
            "cities. How industrial areas look is again less geography-speci/f_ic:\n",
            "inter-city similarity is consistently larger than intra-city similarity.\n",
            "As such, for several classes we can expect learning to transfer from\n",
            "one geography to another. /T_hus, while it is not news that some\n",
            "cities are more similar than others (Barcelona is visually closer to\n",
            "Athens than it is to Berlin), the methodology in this paper allows\n",
            "for a more quantitative and practical comparison of similarity.\n",
            "By leveraging satellite data (available virtually world-wide), this\n",
            "approach may allow for a low-cost way to analyze urban envi-\n",
            "ronments in locations where ground truth information on urban\n",
            "planning is not available. As future directions of this work, we\n",
            "plan to i)continue to develop more rigorous ways to compare and\n",
            "benchmark urban neighborhoods, going deeper to physical ele-\n",
            "ments (vegetation, buildings, roads etc.); ii)improve and further\n",
            "curate the open Urban Environments dataset; and iii)extend this\n",
            "type of analysis to more cities across other geographical locations.\n",
            "A PRACTICAL TRAINING DETAILS.\n",
            "We split our training data into a training set (80% of the data) and a\n",
            "validation set (the remaining 20%). /T_his is separate from the data\n",
            "sampled for the ground truth raster grids for each city, which we\n",
            "only used at test time. We implemented the architectures in the\n",
            "open-source deep learning framework Keras7(with a TensorFlow8\n",
            "backend). In all our experiments, we used popular data augmenta-\n",
            "tion techniques, including random horizontal and vertical /f_lipping\n",
            "of the input images, random shearing (up to 0 .1 radians), random\n",
            "scaling (up to 120%), random rotations (by at most 15 degrees either\n",
            "direction). Input images were 224 2243 pixels in size (RGB\n",
            "bands). For all experiments, we used stochastic gradient descent\n",
            "(with its Adadelta variant) to optimize the network loss function (a\n",
            "standard multi-class cross-entropy), starting with a learning rate of\n",
            "0.1, and halving the rate each 10 epochs. We trained our networks\n",
            "for at most 100 epochs, with 2000 samples in each epoch, stopping\n",
            "the learning process when the accuracy on the validation set did\n",
            "not improve for more than 10 epochs. Given the inherent imbalance\n",
            "of the classes, we explicitly enforced that the minibatches used for\n",
            "training were relatively balanced by a weighted sampling proce-\n",
            "dure. For training, we used a cluster of 4 NVIDIA K80 GPUs, and\n",
            "tested our models on a cluster of 48 CPUs.\n",
            "7\n",
            "8www.tensor/f_low.org\n"
          ]
        }
      ],
      "source": [
        "# Removing non-ascii characters, urls from extracted text\n",
        "\n",
        "text_without_non_ascii = re.sub(r\"[^\\x00-\\x7F]\", \"\", extracted_text)\n",
        "text_without_non_ascii = re.sub(r\",.-/:\",\"\",text_without_non_ascii)\n",
        "cleaned_text = re.sub(r\"h/t_tps?://[^\\s]+\",\"\",text_without_non_ascii)\n",
        "\n",
        "print(cleaned_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RomL4FpaC-Wu",
        "outputId": "2bcd4247-2ead-4713-949e-a6b6942d6010"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "execution_count": 150,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Copying the header names within a new variable\n",
        "\n",
        "sections = new_output\n",
        "len(sections)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvDA0BdZC-Wu",
        "outputId": "ef2b7482-4f26-43d0-b378-f7c0c49266df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Markers found in the text. ABSTRACT\n",
            "Markers found in the text. 1 INTRODUCTION\n",
            "Markers found in the text. 2 LITERATURE\n",
            "Markers found in the text. 3 THE URBAN ENVIRONMENTS DATASET\n",
            "Markers found in the text. 4 EXPERIMENTAL SETUP\n",
            "Markers found in the text. 5 RESULTS AND DISCUSSION\n",
            "Markers found in the text. 6 CONCLUSIONS\n"
          ]
        }
      ],
      "source": [
        "# Extraction of text under individual sections of pdfs using the header names\n",
        "\n",
        "section_extraction = []\n",
        "\n",
        "# Initialize an empty list to store the sections found in the text\n",
        "updated_sections = []\n",
        "\n",
        "for i in range(len(sections)-1):\n",
        "    start_index = cleaned_text.find(sections[i])\n",
        "    end_index = cleaned_text.find(sections[i+1])\n",
        "\n",
        "    if start_index != -1 and end_index != -1:\n",
        "        extraction = cleaned_text[start_index:end_index].strip()\n",
        "        print(\"Markers found in the text.\",sections[i])\n",
        "        section_extraction.append(extraction)\n",
        "        updated_sections.append(sections[i])\n",
        "    else:\n",
        "        print(\"Markers not found in the text.\",sections[i])\n",
        "\n",
        "\n",
        "# Extract the last section separately\n",
        "last_start_index = cleaned_text.find(sections[-1])\n",
        "if last_start_index != -1:\n",
        "    last_extraction = cleaned_text[last_start_index:].strip()\n",
        "    print(\"Markers found in the text.\", sections[-1])\n",
        "    section_extraction.append(last_extraction)\n",
        "    updated_sections.append(sections[-1])\n",
        "else:\n",
        "    print(\"Markers not found in the text.\", sections[-1])\n",
        "\n",
        "# Update the sections list with only the sections found in the text\n",
        "sections = updated_sections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ABSTRACT',\n",
              " '1 INTRODUCTION',\n",
              " '2 LITERATURE',\n",
              " '3 THE URBAN ENVIRONMENTS DATASET',\n",
              " '4 EXPERIMENTAL SETUP',\n",
              " '5 RESULTS AND DISCUSSION',\n",
              " '6 CONCLUSIONS']"
            ]
          },
          "execution_count": 152,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOM4NXbZC-Wu",
        "outputId": "2c514c1f-683b-440e-fbc2-ad12adac6dc9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ABSTRACT\\nUrban planning applications (energy audits, investment, etc.) re-\\nquire an understanding of built infrastructure and its environment,\\ni.e., both low-level, physical features (amount of vegetation, build-\\ning area and geometry etc.), as well as higher-level concepts such\\nas land use classes (which encode expert understanding of socio-\\neconomic end uses). /T_his kind of data is expensive and labor-\\nintensive to obtain, which limits its availability (particularly in\\ndeveloping countries). We analyze pa/t_terns in land use in urban\\nneighborhoods using large-scale satellite imagery data (which is\\navailable worldwide from third-party providers) and state-of-the-\\nart computer vision techniques based on deep convolutional neural\\nnetworks. For supervision, given the limited availability of standard\\nbenchmarks for remote-sensing data, we obtain ground truth land\\nuse class labels carefully sampled from open-source surveys, in\\nparticular the Urban Atlas land classi/f_ication dataset of 20 land use\\nclasses across 300 European cities. We use this data to train and\\ncompare deep architectures which have recently shown good per-\\nformance on standard computer vision tasks (image classi/f_ication\\nand segmentation), including on geospatial data. Furthermore, we\\nshow that the deep representations extracted from satellite imagery\\nof urban environments can be used to compare neighborhoods\\nacross several cities. We make our dataset available for other ma-\\nchine learning researchers to use for remote-sensing applications.\\nCCS CONCEPTS\\nComputing methodologies Computer vision; Neural net-\\nworks; Applied computing Environmental sciences;\\nKEYWORDS\\nSatellite imagery, land use classi/f_ication, convolutional networks',\n",
              " '1 INTRODUCTION\\nLand use classi/f_ication is an important input for applications rang-\\ning from urban planning, zoning and the issuing of business per-\\nmits, to real-estate construction and evaluation to infrastructure\\nCorresponding author.\\nPermission to make digital or hard copies of part or all of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor pro/f_it or commercial advantage and that copies bear this notice and the full citation\\non the /f_irst page. Copyrights for third-party components of this work must be honored.\\nFor all other uses, contact the owner/author(s).\\nKDD17, August 1317, 2017, Halifax, NS, Canada.\\n 2017 Copyright held by the owner/author(s). 978-1-4503-4887-4/17/08.\\nDOI:  Urban land use classi/f_ication is typically based on\\nsurveys performed by trained professionals. As such, this task\\nis labor-intensive, infrequent, slow, and costly. As a result, such\\ndata are mostly available in developed countries and big cities that\\nhave the resources and the vision necessary to collect and curate it;\\nthis information is usually not available in many poorer regions,\\nincluding many developing countries [ 9] where it is mostly needed.\\n/T_his paper builds on two recent trends that promise to make\\nthe analysis of urban environments a more democratic and inclu-\\nsive task. On the one hand, recent years have seen signi/f_icant\\nimprovements in satellite technology and its deployment (primar-\\nily through commercial operators), which allows to obtain high\\nand medium-resolution imagery of most urbanized areas of the\\nEarth with an almost daily revisit rate. On the other hand, the\\nrecent breakthroughs in computer vision methods, in particular\\ndeep learning models for image classi/f_ication and object detection,\\nnow make possible to obtain a much more accurate representation\\nof the composition built infrastructure and its environments.\\nOur contributions are to both the applied deep learning literature,\\nand to the incipient study of smart cities using remote sensing\\ndata. We contrast state-of-the-art convolutional architectures (the\\nVGG-16 [ 19] and ResNet [ 7] networks) to train classi/f_iers that recog-\\nnize broad land use classes from satellite imagery. We then use the\\nfeatures extracted from the model to perform a large-scale compar-\\nison of urban environments. For this, we construct a novel dataset\\nfor land use classi/f_ication, pairing carefully sampled locations with\\nground truth land use class labels obtained from the Urban Atlas\\nsurvey [ 22] with satellite imagery obtained from Google Mapss\\nstatic API. Our dataset - which we have made available publicly\\nfor other researchers - covers, for now, 10 cities in Europe (chosen\\nout of the original 300) with 10 land use classes (from the original\\n20). As the Urban Atlas is a widely-used, standardized dataset for\\nland use classi/f_ication, we hope that making this dataset available\\nwill encourage the development analyses and algorithms for ana-\\nlyzing the built infrastructure in urban environments. Moreover,\\ngiven that satellite imagery is available virtually everywhere on\\nthe globe, the methods presented here allow for automated, rapid\\nclassi/f_ication of urban environments that can potentially be applied\\nto locations where survey and zoning data is not available.\\nLand use classi/f_ication refers to the combination of physical\\nland a/t_tributes and what cultural and socio-economic function land\\nserves (which is a subjective judgement by experts) [ 2]. In this paper,\\nwe take the view that land use classes are just a useful discretization\\nof a more continuous spectrum of pa/t_terns in the organization of\\nurban environments. /T_his viewpoint is illustrated in Figure 2: while\\narXiv:1704.02965v2  [cs.CV]  13 Sep 2017Figure 1: Urban land use maps for six example cities. We compare the ground truth ( top row ) with the predicted land use maps,\\neither from using separate data collected from the same city ( middle row ), or using data from all other cities ( bo/t_tom row ).\\nFigure 2: Le/f_t: Comparing urban environments via deep hi-\\nerarchical representations of satellite image samples. Right:\\napproach outline - data collection, classi/f_ication, feature ex-\\ntraction, clustering, validation.\\nsome a/t_tributes (e.g., amount of built structures or vegetation) are\\ndirectly interpretable, some others may not be. Nevertheless, these\\npa/t_terns in/f_luence, and are in/f_luenced by, socio-economic factors\\n(e.g., economic activity), resource use (energy), and dynamic human\\nbehavior (e.g., mobility, building occupancy). We see the work\\non cheaply curating a large-scale land use classi/f_ication dataset\\nand comparing neighborhoods using deep representations that\\nthis paper puts forth as a necessary /f_irst step towards a granular\\nunderstanding of urban environments in data-poor regions.\\nSubsequently, in Section 2 we review related studies that apply\\ndeep learning methods and other machine learning techniques\\nto problems of land use classi/f_ication, object detection, and image\\nsegmentation in aerial imagery. In Section 3 we describe the datasetwe curated based on the Urban Atlas survey. Section 4 reviews the\\ndeep learning architectures we used. Section 5 describes model\\nvalidation and analysis results. We conclude in Section 6.\\nAll the code used to acquire, process, and analyze the data, as\\nwell as to train the models discussed in this paper is available at',\n",
              " '2 LITERATURE\\n/T_he literature on the use of remote sensing data for applications in\\nland use cover, urban planning, environmental science, and others,\\nhas a long and rich history. /T_his paper however is concerned more\\nnarrowly with newer work that employs widely-available data\\nand machine learning models - and in particular deep learning\\narchitectures - to study urban environments.\\nDeep learning methods have only recently started to be deployed\\nto the analysis of satellite imagery. As such, land use classi/f_ication\\nusing these tools is still a very incipient literature. Probably the /f_irst\\nstudies (yet currently only 1-2 years old) include the application\\nof convolutional neural networks to land use classi/f_ication [ 2] us-\\ning the UC Merced land use dataset [ 25] (of 2100 images spanning\\n21 classes) and the classi/f_ication of agricultural images of coee\\nplantations [ 17]. Similar early studies on land use classi/f_ication\\nthat employ deep learning techniques are [ 21], [18], and [ 15]. In\\n[11], a spatial pyramid pooling technique is employed for land use\\nclassi/f_ication using satellite imagery. /T_he authors of these studies\\nadapted architectures pre-trained to recognize natural images from\\nthe ImageNet dataset (such as the VGG16 [ 19], which we also use),\\nand /f_ine-tuned them on their (much smaller) land use data. More\\nrecent studies use the DeepSat land use benchmark dataset [ 1],\\nwhich we also use and describe in more detail in Section 2.1. An-\\nother topic that is closely related to ours is remote-sensing image\\nsegmentation and object detection, where modern deep learning\\nmodels have also started to be applied. Some of the earliest work\\nthat develops and applies deep neural networks for this tasks is thatof [13]. Examples of recent studies include [ 26] and [ 12], where the\\nauthors propose a semantic image segmentation technique com-\\nbining texture features and boundary detection in an end-to-end\\ntrainable architecture.\\nRemote-sensing data and deep learning methods have been put\\nto use to other related ends, e.g., geo-localization of ground-level\\nphotos via satellite images [ 3,24] or predicting ground-level scene\\nimages from corresponding aerial imagery [ 27]. Other applications\\nhave included predicting survey estimates on poverty levels in\\nseveral countries in Africa by /f_irst learning to predict levels of night\\nlights (considered as proxies of economic activity and measured\\nby satellites) from day-time, visual-range imagery from Google\\nMaps, then transferring the learning from this la/t_ter task to the\\nformer [ 9]. Our work takes a similar approach, in that we aim to use\\nremote-sensing data (which is widely-available for most parts of\\nthe world) to infer land use types in those locations where ground\\ntruth surveys are not available.\\nUrban environments have been analyzed using other types of\\nimagery data that have become recently available. In [ 4,14], the\\nauthors propose to use the same type of imagery from Google Street\\nView to measure the relationship between urban appearance and\\nquality of life measures such as perceived safety. For this, they\\nhand-cra/f_t standard image features widely used in the computer\\nvision community, and train a shallow machine learning classi/f_ier\\n(a support vector machine). In a similar fashion, [ 5] trained a\\nconvolutional neural network on ground-level Street View imagery\\npaired with a crowd-sourced mechanism for collecting ground truth\\nlabels to predict subjective perceptions of urban environments such\\nas beauty, wealth, and liveliness.\\nLand use classi/f_ication has been studied with other new data\\nsources in recent years. For example, ground-level imagery has been\\nemployed to accurately predict land use classes on an university\\ncampus [ 28]. Another related literature strand is work that uses\\nmobile phone call records to extract spatial and temporal mobility\\npa/t_terns, which are then used to infer land use classes for several\\ncities [ 6,10,20]. Our work builds on some of the ideas for sampling\\ngeospatial data presented there.\\n2.1 Existing land use benchmark datasets\\nPublic benchmark data for land use classi/f_ication using aerial im-\\nagery are still in relatively short supply. Presently there are two\\nsuch datasets that we are aware of, discussed below.\\nUC Merced. /T_his dataset was published in 2010 [ 25] and con-\\ntains 2100 256 256, 1 m/pxaerial RGB images over 21 land use\\nclasses. It is considered a solved problem, as modern neural net-\\nwork based classi/f_iers [2] have achieved >95% accuracy on it.\\nDeepSat. /T_he DeepSat [ 1] dataset1was released in 2015. It\\ncontains two benchmarks: the Sat-4 data of 500 ,000 images over 4\\nland use classes ( barren land, trees, grassland, other ), and the Sat-6\\ndata of 405 ,000 images over 6 land use classes ( barren land, trees,\\ngrassland, roads, buildings, water bodies ). All the samples are 28 28\\nin size at a 1 m/pxspatial resolution and contain 4 channels (red,\\ngreen, blue, and NIR - near infrared). Currently less than two years\\nold, this dataset is already a solved problem, with previous studies\\n[15] (and our own experiments) achieving classi/f_ication accuracies\\n1Available at  saikat/deepsat/.of over 99% using convolutional architectures. While useful as input\\nfor pre-training more complex models, (e.g., image segmentation),\\nthis dataset does not allow to take the further steps for detailed\\nland use analysis and comparison of urban environments across\\ncities, which gap we hope our dataset will address.\\nOther open-source eorts. /T_here are several other projects\\nthat we are aware of related to land use classi/f_ication using open-\\nsource data. /T_he TerraPa/t_tern2project uses satellite imagery from\\nGoogle Maps (just like we do) paired with truth labels over a large\\nnumber (450) of detailed classes obtained using the Open Street\\nMap API3. (Open Street Maps is a comprehensive, open-access,\\ncrowd-sourced mapping system.) /T_he projects intended use is as\\na search tool for satellite imagery, and as such, the classes they\\nemploy are very speci/f_ic, e.g., baseball diamonds, churches, or\\nroundabouts. /T_he authors use a ResNet architecture [ 7] to train a\\nclassi/f_ication model, which they use to embed images in a high-\\ndimensional feature space, where similar images to an input image\\ncan be identi/f_ied. A second open-source project related to ours is\\nthe DeepOSM4, in which the authors take the same approach of\\npairing OpenStreetMap labels with satellite imagery obtained from\\nGoogle Maps, and use a convolutional architecture for classi/f_ication.\\n/T_hese are excellent starting points from a practical standpoint,\\nallowing interested researchers to quickly familiarize themselves\\nwith programming aspects of data collection, API calls, etc.',\n",
              " '3 THE URBAN ENVIRONMENTS DATASET\\n3.1 Urban Atlas: a standard in land use analysis\\n/T_he Urban Atlas [ 22] is an open-source, standardized land use\\ndataset that covers 300 European cities of 100 ,000 inhabitants or\\nmore, distributed relatively evenly across major geographical and\\ngeopolitical regions. /T_he dataset was created between 2005-2011 as\\npart of a major eort by the European Union to provide a uniform\\nframework for the geospatial analysis of urban areas in Europe.\\nLand use classi/f_ication is encoded via detailed polygons organized\\nin commonly-used GIS/ESRI shape /f_iles. /T_he dataset covers 20\\nstandardized land use classes. In this work we selected classes of\\ninterest and consolidated them into 10 /f_inal classes used for analysis\\n(see Figure 3). Producing the original Urban Atlas dataset required\\nfusing several data sources: high and medium-resolution satellite\\nimagery, topographic maps, navigation and road layout data, and\\nlocal zoning (cadastral) databases. More information on the method-\\nology used by the Urban Atlas researchers can be obtained from\\nthe European Environment Agency5. We chose expressly to use\\nthe Urban Atlas dataset over other sources (described in Section 2.1\\nbecause i)it is a comprehensive and consistent survey at a large\\nscale, which has been extensively curated by experts and used in\\nresearch, planning, and socio-economic work over the past decade,\\nandii)the land use classes re/f_lect higher-level (socio-economic,\\ncultural) functions of the land as used in applications.\\nWe note that there is a wide variance in the distribution of land\\nuse classes across and within the 300 cities. Figure 3 illustrates\\nthe dierences in the distribution in ground truth polygon areas\\n2\\n3\\n4\\n5 3: Ground truth land use distribution (by area) for\\nthree example cities in the Urban Environments dataset.\\nfor each of the classes for three example cities (Budapest, Rome,\\nBarcelona) from the dataset (from Eastern, Central, and Western\\nEurope, respectively). /T_his wide disparity in the spatial distribution\\npa/t_terns of dierent land use classes and across dierent cities\\nmotivates us to design a careful sampling procedure for collecting\\ntraining data, described in detail below.\\n3.2 Data sampling and acquisition\\nWe set out to develop a strategy to obtain high-quality samples\\nof the type (satellite image, ground truth label) to use in training\\nconvolutional architectures for image classi/f_ication. Our /f_irst re-\\nquirement is to do this solely with freely-available data sources,\\nas to keep costs very low or close to zero. For this, we chose to\\nuse the Google Maps Static API6as a source of satellite imagery.\\n/T_his service allows for 25 ,000 API requests/day free of charge. For\\na given sampling location given by (latitude, longitude), we ob-\\ntained 224 224 images at a zoom level 17 (around 1 .20m/pxspatial\\nresolution, or 250m250mcoverage for an image).\\n/T_he goals of our sampling strategy are twofold. First, we want\\nto ensure that the resulting dataset is as much as possible balanced\\nwith respect to the land use classes. /T_he challenge is that the classes\\nare highly imbalanced among the ground truth polygons in the\\ndataset (e.g., many more polygons are agricultural land and isolated\\nstructures than airports). Second, the satellite images should be\\nrepresentative of the ground truth class associated to them. To this\\nend, we require that the image contain at least 25% (by area) of\\nthe associated ground truth polygon. /T_hus, our strategy to obtain\\ntraining samples is as follows (for a given city):\\nSort ground truth polygons in decreasing order according to\\ntheir size, and retain only those polygons with areas larger than\\n1\\n4(2241.2m)2=0.06km2;\\nFrom each decile of the distribution of areas, sample a propor-\\ntionally larger number of polygons, such that some of the smaller\\npolygons also are picked, and more of the larger ones;\\nFor each picked polygon, sample a number of images propor-\\ntional to the area of the polygon, and assign each image the\\npolygon class as ground truth label;\\n6\\nFigure 4: Example satellite images for the original land use\\nclasses in the Urban Atlas dataset.\\nExample satellite images for each of the 10 land use classes in\\nthe Urban Environments dataset are given in Figure 4. Note the\\nsigni/f_icant variety (in color schemes, textures, etc) in environments\\ndenoted as having the same land use class. /T_his is because of several\\nfactors, including the time of the year when the image was acquired\\n(e.g., agricultural lands appear dierent in the spring than in the\\nfall), the dierent physical form and appearance of environments\\nthat serve the same socioeconomic or cultural function (e.g., green\\nurban areas may look very dierent in dierent cities or in even\\nin dierent parts of the same city; what counts as dense urban\\nfabric in one city may not be dense at all in other cities), and\\nchange in the landscape during the several years that have passed\\nsince the compilation of the Urban Atlas dataset and the time of\\nacquisition of the satellite image (e.g., construction sites may not\\nre/f_lect accurately anymore the reality on the ground).\\nApart from these training images, we constructed ground truth\\nrasters to validate model output for each city. For that, we de/f_ined\\nuniform validation grids of 100 100 (25km25km )around the\\n(geographical) center of a given city of interest. We take a satellite\\nimage sample in each grid cell, and assign to it as label the class\\nof the polygon that has the maximum intersection area with that\\ncell. Examples of land use maps for the six cities we analyze here\\nare given in Figure 1 (top row). /T_here, each grid cell is assigned the\\nclass of the ground truth polygon whose intersection with the cell\\nhas maximum coverage fraction by area. Classes are color-coded\\nfollowing the original Urban Atlas documentation.\\nIn Table 1 we present summaries of the training (le/f_t) and vali-\\ndation (right) datasets we used for the analysis in this paper. /T_he\\nvalidation dataset consists of the images sampled at the centers of\\neach cell in the 25 km25kmgrid as discussed above. /T_his dataset\\nconsists of 140,000 images distributed across 10 urban environ-\\nment classes from 6 cities: Roma (Rome), Madrid, Berlin, Budapest,\\nBarcelona, and Athina (Athens). Because of the high variation in\\nappearance upon visual inspection, we chose to consolidate severalclasses from the original dataset, in particular classes that indicated\\nurban fabric into High Density Urban Fabric, Medium Density\\nUrban Fabric, and Low Density Urban Fabric. As mentioned above\\nand illustrated in Figure 3, we did notice a great disparity in the\\nnumbers and distribution of ground truth polygons for other ex-\\nample cities that we investigated in the Urban Atlas dataset. As\\nsuch,for the analysis in this paper, we have chosen cities where\\nenough ground truth polygons were available for each class (that\\nis, at least 50 samples) to allow for statistical comparisons.',\n",
              " '4 EXPERIMENTAL SETUP\\n4.1 Neural network architectures and training\\nFor all experiments in this paper we compared the VGG-16 [ 19]\\nand ResNet [7, 8] architectures.\\nVGG-16. /T_his architecture [ 19] has become one of the most pop-\\nular models in computer vision for classi/f_ication and segmentation\\ntasks. It consists of 16 trainable layers organized in blocks. It starts\\nwith a 5-block convolutional base of neurons with 3 3 receptive\\n/f_ields (alternated with max-pooling layers that eectively increase\\nthe receptive /f_ield of neurons further downstream). Following each\\nconvolutional layer is a ReLU activation function [ 19]. /T_he feature\\nmaps thus obtained are fed into a set of fully-connected layers (a\\ndeep neural network classi/f_ier). See Table 2 for a summary.\\nResNet. /T_his architecture [ 7,8] has achieved state-of-the-art\\nperformance on image classi/f_ication on several popular natural\\nimage benchmark datasets. It consists of blocks of convolutional\\nlayers, each of which is followed by a ReLU non-linearity. As before,\\neach block in the convolutional base is followed by a max-pooling\\noperation. Finally, the output of the last convolutional layer serves\\nas input feature map for a fully-connected layer with a so/f_tmax\\nactivation function. /T_he key dierence in this architecture is that\\nshortcut connections are implemented that skip blocks of convo-\\nlutional layers, allowing the network to learn residual mappings\\nbetween layer input and output. Here we used an implementation\\nwith 50 trainable layers per [7]. See Table 3 for a summary.\\nTransfer learning. As it is common practice in the literature,\\nwe have experimented with training our models on the problem of\\ninterest (urban environment classi/f_ication) starting from architec-\\ntures pre-trained on datasets from other domains ( transfer learning ).\\n/T_his procedure has been shown to yield both be/t_ter performance\\nand faster training times, as the network already has learned to\\nrecognize basic shapes and pa/t_terns that are characteristic of im-\\nages across many domains (e.g., [ 9,12,15]). We have implemented\\nthe following approaches: 1)we used models pre-trained on the\\nImageNet dataset, then further /f_ine-tuned them on the Urban Atlas\\ndataset; and 2)we pre-trained on the DeepSat dataset (See Section\\n2), then further re/f_ined on the Urban Atlas dataset. As expected,\\nthe la/t_ter strategy - /f_irst training a model (itself pre-trained on Ima-\\ngeNet data) on the DeepSat benchmark, and the further re/f_ining\\non the Urban Atlas dataset - yielded the best results, achieving\\nincreases of around 5% accuracy for a given training time.\\nGiven the large amount of variation in the visual appearance\\nof urban environments across dierent cities (because of dierent\\nclimates, dierent architecture styles, various other socio-economic\\nfactors), it is of interest to study to what extent a model learned on\\none geographical location can be applied to a dierent geographicallocation. As such, we perform experiments in which we train a\\nmodel for one (or more) cities, then apply the model to a dierent set\\nof cities. Intuitively, one would expect that, the more neighborhoods\\nand other urban features at one location are similar to those at a\\ndierent location, the be/t_ter learning would transfer, and the higher\\nthe classi/f_ication accuracy obtained would be. Results for these\\nexperiments are summarized in Figure 6.\\n4.2 Comparing urban environments\\nWe next used the convolutional architectures to extract features\\nfor validation images. As in other recent studies (e.g., [ 9]), we use\\nthe last layer of a network as feature extractor. /T_his amounts to\\nfeature vectors of D=4096 dimensions for the VGG16 architecture\\nandD=2048 dimensions for the ResNet-50 architecture. /T_he\\ncodes xRDare the image representations that either network\\nderives as most representative to discriminate the high-level land\\nuse concepts it is trained to predict.\\nWe would like to study how similar dierent classes of urban\\nenvironments are across two example cities (here we picked Berlin\\nand Barcelona, which are fairly dierent from a cultural and archi-\\ntectural standpoint). For this, we focus only on the 25 km25km,\\n100100-cell grids around the city center as in Figure 1. To be able\\nto quantify similarity in local urban environments, we construct\\na KD-tree T(using a high-performance implementation available\\nin the Python package scikit-learn [16]) using all the gridded\\nsamples. /T_his data structure allows to /f_ind k-nearest neighbors of a\\nquery image in an ecient way. In this way, the feature space can\\nbe probed in an ecient way.',\n",
              " '5 RESULTS AND DISCUSSION\\nIn Figure 1 we show model performance on the 100 100 (25 km\\n25km) raster grids we used for testing. /T_he top row shows ground\\ntruth grids, where the class in each cell was assigned as the most\\nprevalent land use class by area (see also Section 3). /T_he bo/t_tom row\\nshows model predictions, where each cell in a raster is painted in\\nthe color corresponding to the maximum probability class estimated\\nby the model (here ResNet-50). Columns in the /f_igure show results\\nfor each of the 6 cities we used in our dataset. Even at a /f_irst visual\\ninspection, the model is able to recreate from satellite imagery\\nqualitatively the urban land use classi/f_ication map.\\nFurther, looking at the individual classes separately and the con-\\n/f_idence of the model in its predictions (the probability distribution\\nover classes computed by the model), the picture is again qualita-\\ntively very encouraging. In Figure 5 we show grayscale raster maps\\nencoding the spatial layout of the class probability distribution for\\none example city, Barcelona. Particularly good qualitative agree-\\nment is observed for agricultural lands, water bodies, industrial,\\npublic, and commercial land, forests, green urban areas, low density\\nurban fabric, airports, and sports and leisure facilities. /T_he model\\nappears to struggle with reconstructing the spatial distribution of\\nroads, which is not unexpected, given that roads typically appear\\nin many other scenes that have a dierent functional classi/f_ication\\nfor urban planning purposes.Table 1: Urban Environments dataset: sample size summary.\\n(a) Dataset used for training & validation ( 80%and20%, respectively)\\nclass/city athina barcelona berlin budapest madrid roma class\\ntotal\\nAgricultural + Semi-\\nnatural areas + Wet-\\nlands4347 2987 7602 2211 4662 4043 25852\\nAirports 382 452 232 138 124 142 1470\\nForests 1806 2438 7397 1550 2685 2057 17933\\nGreen urban areas 990 722 1840 1342 1243 1401 7538\\nHigh Density Urban\\nFabric967 996 8975 6993 2533 3103 23567\\nIndustrial, commer-\\ncial, public, military\\nand pr1887 2116 4761 1850 3203 2334 16151\\nLow Density Urban\\nFabric1424 1520 2144 575 2794 3689 12146\\nMedium Density Ur-\\nban Fabric2144 1128 6124 1661 1833 2100 14990\\nSports and leisure fa-\\ncilities750 1185 2268 1305 1397 1336 8241\\nWater bodies 537 408 1919 807 805 619 5095\\ncity total 15234 13952 43262 18432 21279 20824 132983(b)25km25kmground truth test grids (fractions of city total)\\nclass / city athina barcelona berlin budapest madrid roma\\nAgricultural + Semi-\\nnatural areas + Wet-\\nlands0.350 0.261 0.106 0.181 0.395 0.473\\nAirports 0.003 0.030 0.013 0.000 0.044 0.006\\nForests 0.031 0.192 0.087 0.211 0.013 0.019\\nGreen urban areas 0.038 0.030 0.072 0.027 0.125 0.054\\nHigh Density Urban\\nFabric0.389 0.217 0.284 0.365 0.170 0.215\\nIndustrial, commer-\\ncial, public, military\\nand pr0.109 0.160 0.190 0.096 0.138 0.129\\nLow Density Urban\\nFabric0.016 0.044 0.012 0.006 0.036 0.029\\nMedium Density Ur-\\nban Fabric0.041 0.025 0.129 0.045 0.042 0.047\\nSports and leisure fa-\\ncilities0.017 0.034 0.080 0.025 0.036 0.025\\nWater bodies 0.005 0.006 0.026 0.044 <0.001 0.004\\nFigure 5: Barcelona: ground truth ( top) and predicted probabilities ( bo/t_tom ).\\nTable 2: /T_he VGG16 architecture [19].\\nBlock 1 Block 2 Block 3 Block 4 Block 5 Block 6\\nConv(3,64)\\nConv(3,64)\\nMax-\\nPool(2,2)Conv(3,128)\\nConv(3,128)\\nMax-\\nPool(2,2)Conv(3,256)\\nConv(3,256)\\nConv(3,256)\\nMax-\\nPool(2,2)Conv(3,512)\\nConv(3,512)\\nConv(3,512)\\nMax-\\nPool(2,2)Conv(3,512)\\nConv(3,512)\\nConv(3,512)\\nMax-\\nPool(2,2)FC(4096)\\nFC(4096)\\nFC(Nclasses )\\nSo/f_tMax\\nTable 3: /T_he ResNet-50 architecture [7].\\nBlock 1 Block 2 Block 3 Block 4 Block 5 Block 6\\nConv(7,64)\\nMax-\\nPool(3,2)3x[Conv(1,64)\\nConv(3,64)\\nConv(3,256)]4x[Conv(1,128)\\nConv(3,128)\\nConv(1,512)]6x[Conv(1,256)\\nConv(3,256)\\nConv(1,1024)]3x[Conv(1,512)\\nConv(3,512)\\nConv(1,2048)]FC(Nclasses )\\nSo/f_tMax\\n5.1 Classi/f_ication results\\nWe performed experiments training the two architectures described\\nin Section 4 on datasets for each of the 6 cities considered, and for\\na combined dataset ( all) of all the cities. /T_he diagonal in Figure 6\\nsummarizes the (validation set) classi/f_ication performance for each\\nmodel. All /f_igures are averages computed over balanced subsets\\nof 2000 samples each. While accuracies or 0.700.80 may not\\nlook as impressive as those obtained by convolutional architectures\\non well-studied benchmarks and other classi/f_ication tasks (e.g.,natural images from ImageNet or small aerial patches from Deep-\\nSat), this only a/t_tests to the diculty of the task of understanding\\nhigh-level, subjective concepts of urban planning in complex ur-\\nban environments. First, satellite imagery typically contains much\\nmore semantic variation than natural images (as also noted, e.g.,\\nin [2,13]), i.e., there is no central concept that the image is of\\n(unlike the image of a cat or a /f_lower). Second, the type of labels we\\nuse for supervision are higher-level concepts (such as low density\\nurban fabric, or sports and leisure facilities), which are much\\nless speci/f_ic than more physical land features e.g., buildings or\\ntrees (which are classes used in the DeepSat dataset). Moreover,\\ntop-down imagery poses speci/f_ic challenges to convolutional archi-\\ntectures, as these models are inherently not rotationally-symmetric.\\nUrban environments, especially from from a top-down point of\\nview, come in many complex layouts, for which rotations are ir-\\nrelevant. Nevertheless, these results are encouraging, especially\\nsince this is a harder problem by focusing on wider-area images and\\non higher-level, subjective concepts used in urban planning rather\\nthan on the standard, lower-level physical features such as in [ 1] or\\n[17]. /T_his suggests that such models may be useful feature extrac-\\ntors. Moreover, as more researchers tackle problems with the aid of\\nsatellite imagery (which is still a relatively under-researched source\\nof data in the machine learning community), more open-sourceFigure 6: Transferability (classi/f_ication accuracy) of models\\nlearned at one location and applied at another. Training on\\na more diverse set of cities ( all) yields encouraging results\\ncompared with just pairwise training/testing.\\ndatasets (like this one) are released, performance will certainly im-\\nprove. For the remainder of this section we report results using\\nthe ResNet-50 architecture [ 7], as it consistently yielded (if only\\nslightly) be/t_ter classi/f_ication results in our experiments than the\\nVGG-16 architecture.\\nTransfer learning and classi/f_ication performance. Next,\\nwe investigated how models trained in one se/t_ting (city or set of\\ncities) perform when applied to other geographical locations. Figure\\n6 summarizes these experiments. In general, performance is poor\\nwhen training on samples from a given city and testing on samples\\nfrom a dierent city (the o-diagonal terms). /T_his is expected, as\\nthese environments can be very dierent in appearance for cities as\\ndierent as e.g., Budapest and Barcelona. However, we notice that\\na more diverse set ( all) yields be/t_ter performance when applied at\\ndierent locations than models trained on individual cities. /T_his is\\nencouraging for our purpose of analyzing the high level similarity\\nof urban neighborhoods via satellite imagery.\\nWe next looked at per-class model performance to understand\\nwhat types of environments are harder for the model to distin-\\nguish. Figure 7 shows such an example analysis for three example\\ncities, of which a pair is similar according to Figure 6 (Rome and\\nBarcelona), and another dissimilar (Rome and Budapest). /T_he le/f_t\\npanel shows model performance when training on samples from\\nBarcelona, and predicting on test samples from Barcelona (intra-\\ncity). /T_he middle panel shows training on Rome, and predicting\\non test samples in Barcelona, which can be assumed to be sim-\\nilar to Rome from a cultural and architectural standpoint (both\\nLatin cities in warm climates). /T_he right /f_igure shows training on\\nBarcelona, and predicting on test samples in Budapest, which can\\nbe assumed a rather dierent city from a cultural and architectural\\nstandpoint. For all cases, the classes that the model most struggles\\nwith are High Density Urban Fabric, Low Density Urban Fabric,\\nand Medium Density Urban Fabric. Considerable overlap can be\\nnoticed between these classes - which is not surprising given the\\nhighly subjective nature of these concepts. Other examples where\\nthe model performance is lower is forests and low-density urban\\nareas being sometimes misclassifed as green urban areas, which,again, is not surprising. /T_his is especially apparent in the cross-city\\ncase, where the model struggles with telling apart these classes. For\\nboth the case of training and testing on dierent cities (Budapest\\nand Barcelona) and on similar cities (Rome and Barcelona), we\\nnote that airports and forests are relatively easier to distinguish.\\nHowever, more subjective, high-level urban-planning concepts such\\nas high density urban fabric are harder to infer (and more easily\\nconfused with medium density or low density urban fabric) in\\nthe case of more similar cities (Rome and Barcelona) rather than\\ndissimilar cities (Budapest and Barcelona). Urban environments\\ncontaining sports and leisure facilities and green areas are under\\nthis view more similar between Rome and Barcelona than they are\\nbetween Budapest and Barcelona.\\nChoosing the spatial scale: sensitivity analysis. So far, we\\nhave presented results assuming that tiles of 250 mis an appropriate\\nspatial scale for this analysis. Our intuition suggested that tiles of\\nthis size have enough variation and information to be recognized\\n(even by humans) as belonging to one of the high-level concepts\\nof land use classes that we study in this paper. However, one\\ncan /f_ind arguments in favor of smaller tile sizes, e.g., in many\\ncities the size of a typical city block is 100 m. /T_hus, we trained\\nmodels at dierent spatial scales and computed test-set accuracy\\nvalues for three example cities, Barcelona, Roma, and Budapest\\n- see Figure 8. It is apparent that, for all example cities, smaller\\nspatial scales (50 m, 100m, 150m) that we analyzed yield poorer\\nperformance than the scale we chose for the analysis in this paper\\n(250m). /T_his is likely because images at smaller scales do not capture\\nenough variation in urban form (number and type of buildings,\\nrelative amount of vegetation, roads etc.) to allow for discriminating\\nbetween concepts that are fairly high-level. /T_his is in contrast with a\\nbenchmark such as DeepSat [ 1] that focuses on lower-level, physical\\nconcepts (trees, buildings, etc.). /T_here, a good spatial scale is\\nby necessity smaller (28 mfor DeepSat), as variation in appearance\\nand compositional elements is unwanted.\\n5.2 Comparing urban environments\\nFinally, we set to understand, at least on an initial qualitative level,\\nhow similar urban environments are to one another, across formal\\nland use classes and geographies. Our /f_irst experiment was to\\nproject sample images for each class and city in this analysis to\\nlower-dimensional manifolds, using the t-SNE algorithm [ 23]. /T_his\\nserves the purpose of both visualization (as t-SNE is widely used\\nfor visualizing high-dimensional data), as well as for providing an\\ninitial, coarse continuous representation of urban land use classes.\\nIn our experiments, we used balanced samples of size N=6000, or\\n100 samples for each of the 10 classes for each city. We extracted\\nfeatures for each of these samples using the allmodels (trained\\non a train set with samples across all cities except for the test one).\\nFigure 9 visualizes such t-SNE embeddings for the six cities in\\nour analysis. For most cities, classes such as low density urban\\nfabric, forests, and water bodies are well-resolved, while sports\\nand leisure facilities seem to consistently blend into other types of\\nenvironments (which is not surprising, given that these types of\\nfacilities can be found within many types of locations that have a\\ndierent formal urban planning class assigned). Intriguing dier-\\nences emerge in this picture among the cities. For example, greenFigure 7: Example classi/f_ication confusion matrix for land use inference. Le/f_t: training and testing on Barcelona; Middle :\\ntraining on Rome, testing on Barcelona; Right: training on Rome, predicting on Budapest.\\nFigure 8: Sensitivity of training patch size vs test accuracy.\\nFigure 9: t-SNE visualization (the /f_irst 2 dimensions) of ur-\\nban environments (satellite image samples) across six cities.\\nurban spaces seem fairly well resolved for most cities. Commercial\\nneighborhoods in Barcelona seem more integrated with the other\\ntypes of environments in the city, whereas for Berlin they appear\\nmore distinct. Urban water bodies are more embedded with urban\\nparks for Barcelona than for other cities. Such reasoning (with\\nmore rigorous quantitative analysis) can serve as coarse way to\\nbenchmark and compare neighborhoods as input to further analysis\\nabout e.g., energy use, livelihood, or trac in urban environments.\\nFigure 10: Comparing urban environments across cities\\n(with reference to Barcelona) We show relative inter-city\\nsimilarity measures computed as the sum of squares across\\nthe clusters in Figure 9.\\nWe further illustrate how similar the six cities we used through-\\nout this analysis are starting o the embeddings plots in Figure 9.\\nFor each land use class, we compute intra-city sum of squares in\\nthe 2-d t-SNE embedding, and display the results in Figure 10. Note\\nthat the distances are always shown with Barcelona as a reference\\npoint (chosen arbitrarily). For each panel, the normalization is with\\nrespect to the largest inter-city distance for that land use class. /T_his\\nvisualization aids quick understanding of similarity between urban\\nenvironments. For example, agricultural lands in Barcelona are\\nmost dissimilar to those in Budapest. Airports in Barcelona are\\nmost similar to those in Athens, and most dissimilar to those in\\nBerlin and Budapest. Barcelonas forests and parks are most dissim-\\nilar to Budapests. Water bodies in Barcelona are very dissimilar to\\nall other cities. /T_his point is enforced by Figure 11 below, which\\nsuggests that areas marked as water bodies in Barcelona are ocean\\nwaterfronts, whereas this class for all other cities represents rivers\\nor lakes.Figure 11: Samples from three urban environments across\\nour 6 example cities. We sampled the 2-d t-SNE embedding\\nof Figure 9 and queried for the closest real sample to the\\ncentroid using an ecient KD-tree search.\\nFinally, we explore the feature maps extracted by the convolu-\\ntional model in order to illustrate how similar the six cities we\\nused throughout this analysis are across three example environ-\\nments, green urban areas, water bodies, and medium density urban\\nfabric. For each city and land use class, we start o the centroid of\\nthe point cloud in the 2-d space of Figure 9, and /f_ind the nearest\\nseveral samples using the KD-tree method described in Section 4.\\nWe present the results in Figure 11. Visual inspection indicates\\nthat the model has learned useful feature maps about urban envi-\\nronments: the sample image patches show a very good qualitative\\nagreement with the region of the space where theyre sampled from,\\nindicated by the land use class of neighboring points. /Q_ualitatively,\\nit is clear that the features extracted from the top layer of the con-\\nvolutional model allow a comparison between urban environments\\nby high-level concepts used in urban planning.',\n",
              " '6 CONCLUSIONS\\n/T_his paper has investigated the use of convolutional neural net-\\nworks for analyzing urban environments through satellite imagery\\nat the scale of entire cities. Given the current relative dearth of\\nlabeled satellite imagery in the machine learning community, we\\nhave constructed an open dataset of over 140 ,000 samples over 10\\nconsistent land use classes from 6 cities in Europe. As we continue\\nto improve, curate, and expand this dataset, we hope that it can help\\nother researchers in machine learning, smart cities, urban planning,\\nand related /f_ields in their work on understanding cities.\\nWe set out to study similarity and variability across urban envi-\\nronments, as being able to quantify such pa/t_terns will enable richer\\napplications in topics such as urban energy analysis, infrastructure\\nbenchmarking, and socio-economic composition of communities.\\nWe formulated this as a two-step task: /f_irst predicting urban land\\nuse classes from satellite imagery, then turning this (rigid) clas-\\nsi/f_ication into a continuous spectrum by embedding the features\\nextracted from the convolutional classi/f_ier into a lower-dimensional\\nmanifold. We show that the classi/f_ication task achieves encour-\\naging results, given the large variety in physical appearance of\\nurban environments having the same functional class. Moreover,we exemplify how the features extracted from the convolutional\\nnetwork allow for identifying neighbors of any given query im-\\nage, allowing a rich comparison analysis of urban environments by\\ntheir visual composition.\\n/T_he analysis in this paper shows that some types urban envi-\\nronments are easier to infer than others, both in the intra- and\\ninter-city cases. For example, in all our experiments, the models\\nhad most trouble telling apart high, medium, and low den-\\nsity urban environments, a/t_testing to the subjectivity of such a\\nhigh-level classi/f_ication for urban planning purposes. However,\\nagricultural lands, forests, and airports tend to be visually similar\\nacross dierent cities - and the amount of relative dissimilarity can\\nbe quanti/f_ied using the methods in this paper. Green urban areas\\n(parks) are generally similar to forests or to leisure facilities, and\\nthe models do be/t_ter in the intra-city case than predicting across\\ncities. How industrial areas look is again less geography-speci/f_ic:\\ninter-city similarity is consistently larger than intra-city similarity.\\nAs such, for several classes we can expect learning to transfer from\\none geography to another. /T_hus, while it is not news that some\\ncities are more similar than others (Barcelona is visually closer to\\nAthens than it is to Berlin), the methodology in this paper allows\\nfor a more quantitative and practical comparison of similarity.\\nBy leveraging satellite data (available virtually world-wide), this\\napproach may allow for a low-cost way to analyze urban envi-\\nronments in locations where ground truth information on urban\\nplanning is not available. As future directions of this work, we\\nplan to i)continue to develop more rigorous ways to compare and\\nbenchmark urban neighborhoods, going deeper to physical ele-\\nments (vegetation, buildings, roads etc.); ii)improve and further\\ncurate the open Urban Environments dataset; and iii)extend this\\ntype of analysis to more cities across other geographical locations.\\nA PRACTICAL TRAINING DETAILS.\\nWe split our training data into a training set (80% of the data) and a\\nvalidation set (the remaining 20%). /T_his is separate from the data\\nsampled for the ground truth raster grids for each city, which we\\nonly used at test time. We implemented the architectures in the\\nopen-source deep learning framework Keras7(with a TensorFlow8\\nbackend). In all our experiments, we used popular data augmenta-\\ntion techniques, including random horizontal and vertical /f_lipping\\nof the input images, random shearing (up to 0 .1 radians), random\\nscaling (up to 120%), random rotations (by at most 15 degrees either\\ndirection). Input images were 224 2243 pixels in size (RGB\\nbands). For all experiments, we used stochastic gradient descent\\n(with its Adadelta variant) to optimize the network loss function (a\\nstandard multi-class cross-entropy), starting with a learning rate of\\n0.1, and halving the rate each 10 epochs. We trained our networks\\nfor at most 100 epochs, with 2000 samples in each epoch, stopping\\nthe learning process when the accuracy on the validation set did\\nnot improve for more than 10 epochs. Given the inherent imbalance\\nof the classes, we explicitly enforced that the minibatches used for\\ntraining were relatively balanced by a weighted sampling proce-\\ndure. For training, we used a cluster of 4 NVIDIA K80 GPUs, and\\ntested our models on a cluster of 48 CPUs.\\n7\\n8www.tensor/f_low.org']"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "section_extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_HRuUDoC-Wv"
      },
      "source": [
        "### Applying LSA for summarizing the section extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "1ktz-IQiC-Wv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "pcyZXjwTC-Wv"
      },
      "outputs": [],
      "source": [
        "# Vectorizing the extracted sentences\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "tfidf_matrix = vectorizer.fit_transform(section_extraction)\n",
        "\n",
        "# 3. Create a TruncatedSVD object for LSA\n",
        "lsa = TruncatedSVD(n_components = 7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2lRc78OC-Wv",
        "outputId": "2fd7109c-2adf-40db-8985-bb75a621dd67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 0.66985382  0.65392816 -0.32679962  0.11499039  0.00801817 -0.01928647\n",
            "  -0.05671672]\n",
            " [ 0.86927604  0.12105766  0.13131812 -0.09678107 -0.12588641  0.24044266\n",
            "   0.35976817]\n",
            " [ 0.8054966   0.26047647  0.45818702 -0.05656148  0.17278491 -0.16539507\n",
            "  -0.11394095]\n",
            " [ 0.86323308 -0.1765504  -0.02084377 -0.10230153 -0.07269376  0.32811443\n",
            "  -0.31593524]\n",
            " [ 0.79878758 -0.2752541   0.03122694  0.53233823  0.00210954 -0.0283697\n",
            "   0.03170469]\n",
            " [ 0.81531223 -0.2807547  -0.23819433 -0.17757784  0.39282417 -0.07014282\n",
            "   0.09455952]\n",
            " [ 0.8422664  -0.180353   -0.0914909  -0.16559004 -0.34944571 -0.3161189\n",
            "  -0.01503322]]\n"
          ]
        }
      ],
      "source": [
        "# 4. Perform LSA on the TF-IDF matrix\n",
        "\n",
        "lsa_matrix = lsa.fit_transform(tfidf_matrix)\n",
        "print(lsa_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9RPKwL0ndZ_",
        "outputId": "f558cbfb-90f8-40ae-d44d-64c8e8c1b27d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Section ABSTRACT sentences:\n",
            " ['ABSTRACT\\nUrban planning applications (energy audits, investment, etc.) re-\\nquire an understanding of built infrastructure and its environment,\\ni.e., both low-level, physical features (amount of vegetation, build-\\ning area and geometry etc.), as well as higher-level concepts such\\nas land use classes (which encode expert understanding of socio-\\neconomic end uses). /T_his kind of data is expensive and labor-\\nintensive to obtain, which limits its availability (particularly in\\ndeveloping countries). We analyze pa/t_terns in land use in urban\\nneighborhoods using large-scale satellite imagery data (which is\\navailable worldwide from third-party providers) and state-of-the-\\nart computer vision techniques based on deep convolutional neural\\nnetworks. For supervision, given the limited availability of standard\\nbenchmarks for remote-sensing data, we obtain ground truth land\\nuse class labels carefully sampled from open-source surveys, in\\nparticular the Urban Atlas land classi/f_ication dataset of 20 land use\\nclasses across 300 European cities. We use this data to train and\\ncompare deep architectures which have recently shown good per-\\nformance on standard computer vision tasks (image classi/f_ication\\nand segmentation), including on geospatial data. Furthermore, we\\nshow that the deep representations extracted from satellite imagery\\nof urban environments can be used to compare neighborhoods\\nacross several cities. We make our dataset available for other ma-\\nchine learning researchers to use for remote-sensing applications.\\nCCS CONCEPTS\\nComputing methodologies Computer vision; Neural net-\\nworks; Applied computing Environmental sciences;\\nKEYWORDS\\nSatellite imagery, land use classi/f_ication, convolutional networks']\n",
            "Section 1 INTRODUCTION sentences:\n",
            " ['1 INTRODUCTION\\nLand use classi/f_ication is an important input for applications rang-\\ning from urban planning, zoning and the issuing of business per-\\nmits, to real-estate construction and evaluation to infrastructure\\nCorresponding author.\\nPermission to make digital or hard copies of part or all of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor pro/f_it or commercial advantage and that copies bear this notice and the full citation\\non the /f_irst page. Copyrights for third-party components of this work must be honored.\\nFor all other uses, contact the owner/author(s).\\nKDD17, August 1317, 2017, Halifax, NS, Canada.\\n 2017 Copyright held by the owner/author(s). 978-1-4503-4887-4/17/08.\\nDOI:  Urban land use classi/f_ication is typically based on\\nsurveys performed by trained professionals. As such, this task\\nis labor-intensive, infrequent, slow, and costly. As a result, such\\ndata are mostly available in developed countries and big cities that\\nhave the resources and the vision necessary to collect and curate it;\\nthis information is usually not available in many poorer regions,\\nincluding many developing countries [ 9] where it is mostly needed.\\n/T_his paper builds on two recent trends that promise to make\\nthe analysis of urban environments a more democratic and inclu-\\nsive task. On the one hand, recent years have seen signi/f_icant\\nimprovements in satellite technology and its deployment (primar-\\nily through commercial operators), which allows to obtain high\\nand medium-resolution imagery of most urbanized areas of the\\nEarth with an almost daily revisit rate. On the other hand, the\\nrecent breakthroughs in computer vision methods, in particular\\ndeep learning models for image classi/f_ication and object detection,\\nnow make possible to obtain a much more accurate representation\\nof the composition built infrastructure and its environments.\\nOur contributions are to both the applied deep learning literature,\\nand to the incipient study of smart cities using remote sensing\\ndata. We contrast state-of-the-art convolutional architectures (the\\nVGG-16 [ 19] and ResNet [ 7] networks) to train classi/f_iers that recog-\\nnize broad land use classes from satellite imagery. We then use the\\nfeatures extracted from the model to perform a large-scale compar-\\nison of urban environments. For this, we construct a novel dataset\\nfor land use classi/f_ication, pairing carefully sampled locations with\\nground truth land use class labels obtained from the Urban Atlas\\nsurvey [ 22] with satellite imagery obtained from Google Mapss\\nstatic API. Our dataset - which we have made available publicly\\nfor other researchers - covers, for now, 10 cities in Europe (chosen\\nout of the original 300) with 10 land use classes (from the original\\n20). As the Urban Atlas is a widely-used, standardized dataset for\\nland use classi/f_ication, we hope that making this dataset available\\nwill encourage the development analyses and algorithms for ana-\\nlyzing the built infrastructure in urban environments. Moreover,\\ngiven that satellite imagery is available virtually everywhere on\\nthe globe, the methods presented here allow for automated, rapid\\nclassi/f_ication of urban environments that can potentially be applied\\nto locations where survey and zoning data is not available.\\nLand use classi/f_ication refers to the combination of physical\\nland a/t_tributes and what cultural and socio-economic function land\\nserves (which is a subjective judgement by experts) [ 2]. In this paper,\\nwe take the view that land use classes are just a useful discretization\\nof a more continuous spectrum of pa/t_terns in the organization of\\nurban environments. /T_his viewpoint is illustrated in Figure 2: while\\narXiv:1704.02965v2  [cs.CV]  13 Sep 2017Figure 1: Urban land use maps for six example cities. We compare the ground truth ( top row ) with the predicted land use maps,\\neither from using separate data collected from the same city ( middle row ), or using data from all other cities ( bo/t_tom row ).\\nFigure 2: Le/f_t: Comparing urban environments via deep hi-\\nerarchical representations of satellite image samples. Right:\\napproach outline - data collection, classi/f_ication, feature ex-\\ntraction, clustering, validation.\\nsome a/t_tributes (e.g., amount of built structures or vegetation) are\\ndirectly interpretable, some others may not be. Nevertheless, these\\npa/t_terns in/f_luence, and are in/f_luenced by, socio-economic factors\\n(e.g., economic activity), resource use (energy), and dynamic human\\nbehavior (e.g., mobility, building occupancy). We see the work\\non cheaply curating a large-scale land use classi/f_ication dataset\\nand comparing neighborhoods using deep representations that\\nthis paper puts forth as a necessary /f_irst step towards a granular\\nunderstanding of urban environments in data-poor regions.\\nSubsequently, in Section 2 we review related studies that apply\\ndeep learning methods and other machine learning techniques\\nto problems of land use classi/f_ication, object detection, and image\\nsegmentation in aerial imagery. In Section 3 we describe the datasetwe curated based on the Urban Atlas survey. Section 4 reviews the\\ndeep learning architectures we used. Section 5 describes model\\nvalidation and analysis results. We conclude in Section 6.\\nAll the code used to acquire, process, and analyze the data, as\\nwell as to train the models discussed in this paper is available at']\n",
            "Section 2 LITERATURE sentences:\n",
            " ['2 LITERATURE\\n/T_he literature on the use of remote sensing data for applications in\\nland use cover, urban planning, environmental science, and others,\\nhas a long and rich history. /T_his paper however is concerned more\\nnarrowly with newer work that employs widely-available data\\nand machine learning models - and in particular deep learning\\narchitectures - to study urban environments.\\nDeep learning methods have only recently started to be deployed\\nto the analysis of satellite imagery. As such, land use classi/f_ication\\nusing these tools is still a very incipient literature. Probably the /f_irst\\nstudies (yet currently only 1-2 years old) include the application\\nof convolutional neural networks to land use classi/f_ication [ 2] us-\\ning the UC Merced land use dataset [ 25] (of 2100 images spanning\\n21 classes) and the classi/f_ication of agricultural images of coee\\nplantations [ 17]. Similar early studies on land use classi/f_ication\\nthat employ deep learning techniques are [ 21], [18], and [ 15]. In\\n[11], a spatial pyramid pooling technique is employed for land use\\nclassi/f_ication using satellite imagery. /T_he authors of these studies\\nadapted architectures pre-trained to recognize natural images from\\nthe ImageNet dataset (such as the VGG16 [ 19], which we also use),\\nand /f_ine-tuned them on their (much smaller) land use data. More\\nrecent studies use the DeepSat land use benchmark dataset [ 1],\\nwhich we also use and describe in more detail in Section 2.1. An-\\nother topic that is closely related to ours is remote-sensing image\\nsegmentation and object detection, where modern deep learning\\nmodels have also started to be applied. Some of the earliest work\\nthat develops and applies deep neural networks for this tasks is thatof [13]. Examples of recent studies include [ 26] and [ 12], where the\\nauthors propose a semantic image segmentation technique com-\\nbining texture features and boundary detection in an end-to-end\\ntrainable architecture.\\nRemote-sensing data and deep learning methods have been put\\nto use to other related ends, e.g., geo-localization of ground-level\\nphotos via satellite images [ 3,24] or predicting ground-level scene\\nimages from corresponding aerial imagery [ 27]. Other applications\\nhave included predicting survey estimates on poverty levels in\\nseveral countries in Africa by /f_irst learning to predict levels of night\\nlights (considered as proxies of economic activity and measured\\nby satellites) from day-time, visual-range imagery from Google\\nMaps, then transferring the learning from this la/t_ter task to the\\nformer [ 9]. Our work takes a similar approach, in that we aim to use\\nremote-sensing data (which is widely-available for most parts of\\nthe world) to infer land use types in those locations where ground\\ntruth surveys are not available.\\nUrban environments have been analyzed using other types of\\nimagery data that have become recently available. In [ 4,14], the\\nauthors propose to use the same type of imagery from Google Street\\nView to measure the relationship between urban appearance and\\nquality of life measures such as perceived safety. For this, they\\nhand-cra/f_t standard image features widely used in the computer\\nvision community, and train a shallow machine learning classi/f_ier\\n(a support vector machine). In a similar fashion, [ 5] trained a\\nconvolutional neural network on ground-level Street View imagery\\npaired with a crowd-sourced mechanism for collecting ground truth\\nlabels to predict subjective perceptions of urban environments such\\nas beauty, wealth, and liveliness.\\nLand use classi/f_ication has been studied with other new data\\nsources in recent years. For example, ground-level imagery has been\\nemployed to accurately predict land use classes on an university\\ncampus [ 28]. Another related literature strand is work that uses\\nmobile phone call records to extract spatial and temporal mobility\\npa/t_terns, which are then used to infer land use classes for several\\ncities [ 6,10,20]. Our work builds on some of the ideas for sampling\\ngeospatial data presented there.\\n2.1 Existing land use benchmark datasets\\nPublic benchmark data for land use classi/f_ication using aerial im-\\nagery are still in relatively short supply. Presently there are two\\nsuch datasets that we are aware of, discussed below.\\nUC Merced. /T_his dataset was published in 2010 [ 25] and con-\\ntains 2100 256 256, 1 m/pxaerial RGB images over 21 land use\\nclasses. It is considered a solved problem, as modern neural net-\\nwork based classi/f_iers [2] have achieved >95% accuracy on it.\\nDeepSat. /T_he DeepSat [ 1] dataset1was released in 2015. It\\ncontains two benchmarks: the Sat-4 data of 500 ,000 images over 4\\nland use classes ( barren land, trees, grassland, other ), and the Sat-6\\ndata of 405 ,000 images over 6 land use classes ( barren land, trees,\\ngrassland, roads, buildings, water bodies ). All the samples are 28 28\\nin size at a 1 m/pxspatial resolution and contain 4 channels (red,\\ngreen, blue, and NIR - near infrared). Currently less than two years\\nold, this dataset is already a solved problem, with previous studies\\n[15] (and our own experiments) achieving classi/f_ication accuracies\\n1Available at  saikat/deepsat/.of over 99% using convolutional architectures. While useful as input\\nfor pre-training more complex models, (e.g., image segmentation),\\nthis dataset does not allow to take the further steps for detailed\\nland use analysis and comparison of urban environments across\\ncities, which gap we hope our dataset will address.\\nOther open-source eorts. /T_here are several other projects\\nthat we are aware of related to land use classi/f_ication using open-\\nsource data. /T_he TerraPa/t_tern2project uses satellite imagery from\\nGoogle Maps (just like we do) paired with truth labels over a large\\nnumber (450) of detailed classes obtained using the Open Street\\nMap API3. (Open Street Maps is a comprehensive, open-access,\\ncrowd-sourced mapping system.) /T_he projects intended use is as\\na search tool for satellite imagery, and as such, the classes they\\nemploy are very speci/f_ic, e.g., baseball diamonds, churches, or\\nroundabouts. /T_he authors use a ResNet architecture [ 7] to train a\\nclassi/f_ication model, which they use to embed images in a high-\\ndimensional feature space, where similar images to an input image\\ncan be identi/f_ied. A second open-source project related to ours is\\nthe DeepOSM4, in which the authors take the same approach of\\npairing OpenStreetMap labels with satellite imagery obtained from\\nGoogle Maps, and use a convolutional architecture for classi/f_ication.\\n/T_hese are excellent starting points from a practical standpoint,\\nallowing interested researchers to quickly familiarize themselves\\nwith programming aspects of data collection, API calls, etc.']\n",
            "Section 3 THE URBAN ENVIRONMENTS DATASET sentences:\n",
            " ['3 THE URBAN ENVIRONMENTS DATASET\\n3.1 Urban Atlas: a standard in land use analysis\\n/T_he Urban Atlas [ 22] is an open-source, standardized land use\\ndataset that covers 300 European cities of 100 ,000 inhabitants or\\nmore, distributed relatively evenly across major geographical and\\ngeopolitical regions. /T_he dataset was created between 2005-2011 as\\npart of a major eort by the European Union to provide a uniform\\nframework for the geospatial analysis of urban areas in Europe.\\nLand use classi/f_ication is encoded via detailed polygons organized\\nin commonly-used GIS/ESRI shape /f_iles. /T_he dataset covers 20\\nstandardized land use classes. In this work we selected classes of\\ninterest and consolidated them into 10 /f_inal classes used for analysis\\n(see Figure 3). Producing the original Urban Atlas dataset required\\nfusing several data sources: high and medium-resolution satellite\\nimagery, topographic maps, navigation and road layout data, and\\nlocal zoning (cadastral) databases. More information on the method-\\nology used by the Urban Atlas researchers can be obtained from\\nthe European Environment Agency5. We chose expressly to use\\nthe Urban Atlas dataset over other sources (described in Section 2.1\\nbecause i)it is a comprehensive and consistent survey at a large\\nscale, which has been extensively curated by experts and used in\\nresearch, planning, and socio-economic work over the past decade,\\nandii)the land use classes re/f_lect higher-level (socio-economic,\\ncultural) functions of the land as used in applications.\\nWe note that there is a wide variance in the distribution of land\\nuse classes across and within the 300 cities. Figure 3 illustrates\\nthe dierences in the distribution in ground truth polygon areas\\n2\\n3\\n4\\n5 3: Ground truth land use distribution (by area) for\\nthree example cities in the Urban Environments dataset.\\nfor each of the classes for three example cities (Budapest, Rome,\\nBarcelona) from the dataset (from Eastern, Central, and Western\\nEurope, respectively). /T_his wide disparity in the spatial distribution\\npa/t_terns of dierent land use classes and across dierent cities\\nmotivates us to design a careful sampling procedure for collecting\\ntraining data, described in detail below.\\n3.2 Data sampling and acquisition\\nWe set out to develop a strategy to obtain high-quality samples\\nof the type (satellite image, ground truth label) to use in training\\nconvolutional architectures for image classi/f_ication. Our /f_irst re-\\nquirement is to do this solely with freely-available data sources,\\nas to keep costs very low or close to zero. For this, we chose to\\nuse the Google Maps Static API6as a source of satellite imagery.\\n/T_his service allows for 25 ,000 API requests/day free of charge. For\\na given sampling location given by (latitude, longitude), we ob-\\ntained 224 224 images at a zoom level 17 (around 1 .20m/pxspatial\\nresolution, or 250m250mcoverage for an image).\\n/T_he goals of our sampling strategy are twofold. First, we want\\nto ensure that the resulting dataset is as much as possible balanced\\nwith respect to the land use classes. /T_he challenge is that the classes\\nare highly imbalanced among the ground truth polygons in the\\ndataset (e.g., many more polygons are agricultural land and isolated\\nstructures than airports). Second, the satellite images should be\\nrepresentative of the ground truth class associated to them. To this\\nend, we require that the image contain at least 25% (by area) of\\nthe associated ground truth polygon. /T_hus, our strategy to obtain\\ntraining samples is as follows (for a given city):\\nSort ground truth polygons in decreasing order according to\\ntheir size, and retain only those polygons with areas larger than\\n1\\n4(2241.2m)2=0.06km2;\\nFrom each decile of the distribution of areas, sample a propor-\\ntionally larger number of polygons, such that some of the smaller\\npolygons also are picked, and more of the larger ones;\\nFor each picked polygon, sample a number of images propor-\\ntional to the area of the polygon, and assign each image the\\npolygon class as ground truth label;\\n6\\nFigure 4: Example satellite images for the original land use\\nclasses in the Urban Atlas dataset.\\nExample satellite images for each of the 10 land use classes in\\nthe Urban Environments dataset are given in Figure 4. Note the\\nsigni/f_icant variety (in color schemes, textures, etc) in environments\\ndenoted as having the same land use class. /T_his is because of several\\nfactors, including the time of the year when the image was acquired\\n(e.g., agricultural lands appear dierent in the spring than in the\\nfall), the dierent physical form and appearance of environments\\nthat serve the same socioeconomic or cultural function (e.g., green\\nurban areas may look very dierent in dierent cities or in even\\nin dierent parts of the same city; what counts as dense urban\\nfabric in one city may not be dense at all in other cities), and\\nchange in the landscape during the several years that have passed\\nsince the compilation of the Urban Atlas dataset and the time of\\nacquisition of the satellite image (e.g., construction sites may not\\nre/f_lect accurately anymore the reality on the ground).\\nApart from these training images, we constructed ground truth\\nrasters to validate model output for each city. For that, we de/f_ined\\nuniform validation grids of 100 100 (25km25km )around the\\n(geographical) center of a given city of interest. We take a satellite\\nimage sample in each grid cell, and assign to it as label the class\\nof the polygon that has the maximum intersection area with that\\ncell. Examples of land use maps for the six cities we analyze here\\nare given in Figure 1 (top row). /T_here, each grid cell is assigned the\\nclass of the ground truth polygon whose intersection with the cell\\nhas maximum coverage fraction by area. Classes are color-coded\\nfollowing the original Urban Atlas documentation.\\nIn Table 1 we present summaries of the training (le/f_t) and vali-\\ndation (right) datasets we used for the analysis in this paper. /T_he\\nvalidation dataset consists of the images sampled at the centers of\\neach cell in the 25 km25kmgrid as discussed above. /T_his dataset\\nconsists of 140,000 images distributed across 10 urban environ-\\nment classes from 6 cities: Roma (Rome), Madrid, Berlin, Budapest,\\nBarcelona, and Athina (Athens). Because of the high variation in\\nappearance upon visual inspection, we chose to consolidate severalclasses from the original dataset, in particular classes that indicated\\nurban fabric into High Density Urban Fabric, Medium Density\\nUrban Fabric, and Low Density Urban Fabric. As mentioned above\\nand illustrated in Figure 3, we did notice a great disparity in the\\nnumbers and distribution of ground truth polygons for other ex-\\nample cities that we investigated in the Urban Atlas dataset. As\\nsuch,for the analysis in this paper, we have chosen cities where\\nenough ground truth polygons were available for each class (that\\nis, at least 50 samples) to allow for statistical comparisons.']\n",
            "Section 4 EXPERIMENTAL SETUP sentences:\n",
            " ['4 EXPERIMENTAL SETUP\\n4.1 Neural network architectures and training\\nFor all experiments in this paper we compared the VGG-16 [ 19]\\nand ResNet [7, 8] architectures.\\nVGG-16. /T_his architecture [ 19] has become one of the most pop-\\nular models in computer vision for classi/f_ication and segmentation\\ntasks. It consists of 16 trainable layers organized in blocks. It starts\\nwith a 5-block convolutional base of neurons with 3 3 receptive\\n/f_ields (alternated with max-pooling layers that eectively increase\\nthe receptive /f_ield of neurons further downstream). Following each\\nconvolutional layer is a ReLU activation function [ 19]. /T_he feature\\nmaps thus obtained are fed into a set of fully-connected layers (a\\ndeep neural network classi/f_ier). See Table 2 for a summary.\\nResNet. /T_his architecture [ 7,8] has achieved state-of-the-art\\nperformance on image classi/f_ication on several popular natural\\nimage benchmark datasets. It consists of blocks of convolutional\\nlayers, each of which is followed by a ReLU non-linearity. As before,\\neach block in the convolutional base is followed by a max-pooling\\noperation. Finally, the output of the last convolutional layer serves\\nas input feature map for a fully-connected layer with a so/f_tmax\\nactivation function. /T_he key dierence in this architecture is that\\nshortcut connections are implemented that skip blocks of convo-\\nlutional layers, allowing the network to learn residual mappings\\nbetween layer input and output. Here we used an implementation\\nwith 50 trainable layers per [7]. See Table 3 for a summary.\\nTransfer learning. As it is common practice in the literature,\\nwe have experimented with training our models on the problem of\\ninterest (urban environment classi/f_ication) starting from architec-\\ntures pre-trained on datasets from other domains ( transfer learning ).\\n/T_his procedure has been shown to yield both be/t_ter performance\\nand faster training times, as the network already has learned to\\nrecognize basic shapes and pa/t_terns that are characteristic of im-\\nages across many domains (e.g., [ 9,12,15]). We have implemented\\nthe following approaches: 1)we used models pre-trained on the\\nImageNet dataset, then further /f_ine-tuned them on the Urban Atlas\\ndataset; and 2)we pre-trained on the DeepSat dataset (See Section\\n2), then further re/f_ined on the Urban Atlas dataset. As expected,\\nthe la/t_ter strategy - /f_irst training a model (itself pre-trained on Ima-\\ngeNet data) on the DeepSat benchmark, and the further re/f_ining\\non the Urban Atlas dataset - yielded the best results, achieving\\nincreases of around 5% accuracy for a given training time.\\nGiven the large amount of variation in the visual appearance\\nof urban environments across dierent cities (because of dierent\\nclimates, dierent architecture styles, various other socio-economic\\nfactors), it is of interest to study to what extent a model learned on\\none geographical location can be applied to a dierent geographicallocation. As such, we perform experiments in which we train a\\nmodel for one (or more) cities, then apply the model to a dierent set\\nof cities. Intuitively, one would expect that, the more neighborhoods\\nand other urban features at one location are similar to those at a\\ndierent location, the be/t_ter learning would transfer, and the higher\\nthe classi/f_ication accuracy obtained would be. Results for these\\nexperiments are summarized in Figure 6.\\n4.2 Comparing urban environments\\nWe next used the convolutional architectures to extract features\\nfor validation images. As in other recent studies (e.g., [ 9]), we use\\nthe last layer of a network as feature extractor. /T_his amounts to\\nfeature vectors of D=4096 dimensions for the VGG16 architecture\\nandD=2048 dimensions for the ResNet-50 architecture. /T_he\\ncodes xRDare the image representations that either network\\nderives as most representative to discriminate the high-level land\\nuse concepts it is trained to predict.\\nWe would like to study how similar dierent classes of urban\\nenvironments are across two example cities (here we picked Berlin\\nand Barcelona, which are fairly dierent from a cultural and archi-\\ntectural standpoint). For this, we focus only on the 25 km25km,\\n100100-cell grids around the city center as in Figure 1. To be able\\nto quantify similarity in local urban environments, we construct\\na KD-tree T(using a high-performance implementation available\\nin the Python package scikit-learn [16]) using all the gridded\\nsamples. /T_his data structure allows to /f_ind k-nearest neighbors of a\\nquery image in an ecient way. In this way, the feature space can\\nbe probed in an ecient way.']\n",
            "Section 5 RESULTS AND DISCUSSION sentences:\n",
            " ['5 RESULTS AND DISCUSSION\\nIn Figure 1 we show model performance on the 100 100 (25 km\\n25km) raster grids we used for testing. /T_he top row shows ground\\ntruth grids, where the class in each cell was assigned as the most\\nprevalent land use class by area (see also Section 3). /T_he bo/t_tom row\\nshows model predictions, where each cell in a raster is painted in\\nthe color corresponding to the maximum probability class estimated\\nby the model (here ResNet-50). Columns in the /f_igure show results\\nfor each of the 6 cities we used in our dataset. Even at a /f_irst visual\\ninspection, the model is able to recreate from satellite imagery\\nqualitatively the urban land use classi/f_ication map.\\nFurther, looking at the individual classes separately and the con-\\n/f_idence of the model in its predictions (the probability distribution\\nover classes computed by the model), the picture is again qualita-\\ntively very encouraging. In Figure 5 we show grayscale raster maps\\nencoding the spatial layout of the class probability distribution for\\none example city, Barcelona. Particularly good qualitative agree-\\nment is observed for agricultural lands, water bodies, industrial,\\npublic, and commercial land, forests, green urban areas, low density\\nurban fabric, airports, and sports and leisure facilities. /T_he model\\nappears to struggle with reconstructing the spatial distribution of\\nroads, which is not unexpected, given that roads typically appear\\nin many other scenes that have a dierent functional classi/f_ication\\nfor urban planning purposes.Table 1: Urban Environments dataset: sample size summary.\\n(a) Dataset used for training & validation ( 80%and20%, respectively)\\nclass/city athina barcelona berlin budapest madrid roma class\\ntotal\\nAgricultural + Semi-\\nnatural areas + Wet-\\nlands4347 2987 7602 2211 4662 4043 25852\\nAirports 382 452 232 138 124 142 1470\\nForests 1806 2438 7397 1550 2685 2057 17933\\nGreen urban areas 990 722 1840 1342 1243 1401 7538\\nHigh Density Urban\\nFabric967 996 8975 6993 2533 3103 23567\\nIndustrial, commer-\\ncial, public, military\\nand pr1887 2116 4761 1850 3203 2334 16151\\nLow Density Urban\\nFabric1424 1520 2144 575 2794 3689 12146\\nMedium Density Ur-\\nban Fabric2144 1128 6124 1661 1833 2100 14990\\nSports and leisure fa-\\ncilities750 1185 2268 1305 1397 1336 8241\\nWater bodies 537 408 1919 807 805 619 5095\\ncity total 15234 13952 43262 18432 21279 20824 132983(b)25km25kmground truth test grids (fractions of city total)\\nclass / city athina barcelona berlin budapest madrid roma\\nAgricultural + Semi-\\nnatural areas + Wet-\\nlands0.350 0.261 0.106 0.181 0.395 0.473\\nAirports 0.003 0.030 0.013 0.000 0.044 0.006\\nForests 0.031 0.192 0.087 0.211 0.013 0.019\\nGreen urban areas 0.038 0.030 0.072 0.027 0.125 0.054\\nHigh Density Urban\\nFabric0.389 0.217 0.284 0.365 0.170 0.215\\nIndustrial, commer-\\ncial, public, military\\nand pr0.109 0.160 0.190 0.096 0.138 0.129\\nLow Density Urban\\nFabric0.016 0.044 0.012 0.006 0.036 0.029\\nMedium Density Ur-\\nban Fabric0.041 0.025 0.129 0.045 0.042 0.047\\nSports and leisure fa-\\ncilities0.017 0.034 0.080 0.025 0.036 0.025\\nWater bodies 0.005 0.006 0.026 0.044 <0.001 0.004\\nFigure 5: Barcelona: ground truth ( top) and predicted probabilities ( bo/t_tom ).\\nTable 2: /T_he VGG16 architecture [19].\\nBlock 1 Block 2 Block 3 Block 4 Block 5 Block 6\\nConv(3,64)\\nConv(3,64)\\nMax-\\nPool(2,2)Conv(3,128)\\nConv(3,128)\\nMax-\\nPool(2,2)Conv(3,256)\\nConv(3,256)\\nConv(3,256)\\nMax-\\nPool(2,2)Conv(3,512)\\nConv(3,512)\\nConv(3,512)\\nMax-\\nPool(2,2)Conv(3,512)\\nConv(3,512)\\nConv(3,512)\\nMax-\\nPool(2,2)FC(4096)\\nFC(4096)\\nFC(Nclasses )\\nSo/f_tMax\\nTable 3: /T_he ResNet-50 architecture [7].\\nBlock 1 Block 2 Block 3 Block 4 Block 5 Block 6\\nConv(7,64)\\nMax-\\nPool(3,2)3x[Conv(1,64)\\nConv(3,64)\\nConv(3,256)]4x[Conv(1,128)\\nConv(3,128)\\nConv(1,512)]6x[Conv(1,256)\\nConv(3,256)\\nConv(1,1024)]3x[Conv(1,512)\\nConv(3,512)\\nConv(1,2048)]FC(Nclasses )\\nSo/f_tMax\\n5.1 Classi/f_ication results\\nWe performed experiments training the two architectures described\\nin Section 4 on datasets for each of the 6 cities considered, and for\\na combined dataset ( all) of all the cities. /T_he diagonal in Figure 6\\nsummarizes the (validation set) classi/f_ication performance for each\\nmodel. All /f_igures are averages computed over balanced subsets\\nof 2000 samples each. While accuracies or 0.700.80 may not\\nlook as impressive as those obtained by convolutional architectures\\non well-studied benchmarks and other classi/f_ication tasks (e.g.,natural images from ImageNet or small aerial patches from Deep-\\nSat), this only a/t_tests to the diculty of the task of understanding\\nhigh-level, subjective concepts of urban planning in complex ur-\\nban environments. First, satellite imagery typically contains much\\nmore semantic variation than natural images (as also noted, e.g.,\\nin [2,13]), i.e., there is no central concept that the image is of\\n(unlike the image of a cat or a /f_lower). Second, the type of labels we\\nuse for supervision are higher-level concepts (such as low density\\nurban fabric, or sports and leisure facilities), which are much\\nless speci/f_ic than more physical land features e.g., buildings or\\ntrees (which are classes used in the DeepSat dataset). Moreover,\\ntop-down imagery poses speci/f_ic challenges to convolutional archi-\\ntectures, as these models are inherently not rotationally-symmetric.\\nUrban environments, especially from from a top-down point of\\nview, come in many complex layouts, for which rotations are ir-\\nrelevant. Nevertheless, these results are encouraging, especially\\nsince this is a harder problem by focusing on wider-area images and\\non higher-level, subjective concepts used in urban planning rather\\nthan on the standard, lower-level physical features such as in [ 1] or\\n[17]. /T_his suggests that such models may be useful feature extrac-\\ntors. Moreover, as more researchers tackle problems with the aid of\\nsatellite imagery (which is still a relatively under-researched source\\nof data in the machine learning community), more open-sourceFigure 6: Transferability (classi/f_ication accuracy) of models\\nlearned at one location and applied at another. Training on\\na more diverse set of cities ( all) yields encouraging results\\ncompared with just pairwise training/testing.\\ndatasets (like this one) are released, performance will certainly im-\\nprove. For the remainder of this section we report results using\\nthe ResNet-50 architecture [ 7], as it consistently yielded (if only\\nslightly) be/t_ter classi/f_ication results in our experiments than the\\nVGG-16 architecture.\\nTransfer learning and classi/f_ication performance. Next,\\nwe investigated how models trained in one se/t_ting (city or set of\\ncities) perform when applied to other geographical locations. Figure\\n6 summarizes these experiments. In general, performance is poor\\nwhen training on samples from a given city and testing on samples\\nfrom a dierent city (the o-diagonal terms). /T_his is expected, as\\nthese environments can be very dierent in appearance for cities as\\ndierent as e.g., Budapest and Barcelona. However, we notice that\\na more diverse set ( all) yields be/t_ter performance when applied at\\ndierent locations than models trained on individual cities. /T_his is\\nencouraging for our purpose of analyzing the high level similarity\\nof urban neighborhoods via satellite imagery.\\nWe next looked at per-class model performance to understand\\nwhat types of environments are harder for the model to distin-\\nguish. Figure 7 shows such an example analysis for three example\\ncities, of which a pair is similar according to Figure 6 (Rome and\\nBarcelona), and another dissimilar (Rome and Budapest). /T_he le/f_t\\npanel shows model performance when training on samples from\\nBarcelona, and predicting on test samples from Barcelona (intra-\\ncity). /T_he middle panel shows training on Rome, and predicting\\non test samples in Barcelona, which can be assumed to be sim-\\nilar to Rome from a cultural and architectural standpoint (both\\nLatin cities in warm climates). /T_he right /f_igure shows training on\\nBarcelona, and predicting on test samples in Budapest, which can\\nbe assumed a rather dierent city from a cultural and architectural\\nstandpoint. For all cases, the classes that the model most struggles\\nwith are High Density Urban Fabric, Low Density Urban Fabric,\\nand Medium Density Urban Fabric. Considerable overlap can be\\nnoticed between these classes - which is not surprising given the\\nhighly subjective nature of these concepts. Other examples where\\nthe model performance is lower is forests and low-density urban\\nareas being sometimes misclassifed as green urban areas, which,again, is not surprising. /T_his is especially apparent in the cross-city\\ncase, where the model struggles with telling apart these classes. For\\nboth the case of training and testing on dierent cities (Budapest\\nand Barcelona) and on similar cities (Rome and Barcelona), we\\nnote that airports and forests are relatively easier to distinguish.\\nHowever, more subjective, high-level urban-planning concepts such\\nas high density urban fabric are harder to infer (and more easily\\nconfused with medium density or low density urban fabric) in\\nthe case of more similar cities (Rome and Barcelona) rather than\\ndissimilar cities (Budapest and Barcelona). Urban environments\\ncontaining sports and leisure facilities and green areas are under\\nthis view more similar between Rome and Barcelona than they are\\nbetween Budapest and Barcelona.\\nChoosing the spatial scale: sensitivity analysis. So far, we\\nhave presented results assuming that tiles of 250 mis an appropriate\\nspatial scale for this analysis. Our intuition suggested that tiles of\\nthis size have enough variation and information to be recognized\\n(even by humans) as belonging to one of the high-level concepts\\nof land use classes that we study in this paper. However, one\\ncan /f_ind arguments in favor of smaller tile sizes, e.g., in many\\ncities the size of a typical city block is 100 m. /T_hus, we trained\\nmodels at dierent spatial scales and computed test-set accuracy\\nvalues for three example cities, Barcelona, Roma, and Budapest\\n- see Figure 8. It is apparent that, for all example cities, smaller\\nspatial scales (50 m, 100m, 150m) that we analyzed yield poorer\\nperformance than the scale we chose for the analysis in this paper\\n(250m). /T_his is likely because images at smaller scales do not capture\\nenough variation in urban form (number and type of buildings,\\nrelative amount of vegetation, roads etc.) to allow for discriminating\\nbetween concepts that are fairly high-level. /T_his is in contrast with a\\nbenchmark such as DeepSat [ 1] that focuses on lower-level, physical\\nconcepts (trees, buildings, etc.). /T_here, a good spatial scale is\\nby necessity smaller (28 mfor DeepSat), as variation in appearance\\nand compositional elements is unwanted.\\n5.2 Comparing urban environments\\nFinally, we set to understand, at least on an initial qualitative level,\\nhow similar urban environments are to one another, across formal\\nland use classes and geographies. Our /f_irst experiment was to\\nproject sample images for each class and city in this analysis to\\nlower-dimensional manifolds, using the t-SNE algorithm [ 23]. /T_his\\nserves the purpose of both visualization (as t-SNE is widely used\\nfor visualizing high-dimensional data), as well as for providing an\\ninitial, coarse continuous representation of urban land use classes.\\nIn our experiments, we used balanced samples of size N=6000, or\\n100 samples for each of the 10 classes for each city. We extracted\\nfeatures for each of these samples using the allmodels (trained\\non a train set with samples across all cities except for the test one).\\nFigure 9 visualizes such t-SNE embeddings for the six cities in\\nour analysis. For most cities, classes such as low density urban\\nfabric, forests, and water bodies are well-resolved, while sports\\nand leisure facilities seem to consistently blend into other types of\\nenvironments (which is not surprising, given that these types of\\nfacilities can be found within many types of locations that have a\\ndierent formal urban planning class assigned). Intriguing dier-\\nences emerge in this picture among the cities. For example, greenFigure 7: Example classi/f_ication confusion matrix for land use inference. Le/f_t: training and testing on Barcelona; Middle :\\ntraining on Rome, testing on Barcelona; Right: training on Rome, predicting on Budapest.\\nFigure 8: Sensitivity of training patch size vs test accuracy.\\nFigure 9: t-SNE visualization (the /f_irst 2 dimensions) of ur-\\nban environments (satellite image samples) across six cities.\\nurban spaces seem fairly well resolved for most cities. Commercial\\nneighborhoods in Barcelona seem more integrated with the other\\ntypes of environments in the city, whereas for Berlin they appear\\nmore distinct. Urban water bodies are more embedded with urban\\nparks for Barcelona than for other cities. Such reasoning (with\\nmore rigorous quantitative analysis) can serve as coarse way to\\nbenchmark and compare neighborhoods as input to further analysis\\nabout e.g., energy use, livelihood, or trac in urban environments.\\nFigure 10: Comparing urban environments across cities\\n(with reference to Barcelona) We show relative inter-city\\nsimilarity measures computed as the sum of squares across\\nthe clusters in Figure 9.\\nWe further illustrate how similar the six cities we used through-\\nout this analysis are starting o the embeddings plots in Figure 9.\\nFor each land use class, we compute intra-city sum of squares in\\nthe 2-d t-SNE embedding, and display the results in Figure 10. Note\\nthat the distances are always shown with Barcelona as a reference\\npoint (chosen arbitrarily). For each panel, the normalization is with\\nrespect to the largest inter-city distance for that land use class. /T_his\\nvisualization aids quick understanding of similarity between urban\\nenvironments. For example, agricultural lands in Barcelona are\\nmost dissimilar to those in Budapest. Airports in Barcelona are\\nmost similar to those in Athens, and most dissimilar to those in\\nBerlin and Budapest. Barcelonas forests and parks are most dissim-\\nilar to Budapests. Water bodies in Barcelona are very dissimilar to\\nall other cities. /T_his point is enforced by Figure 11 below, which\\nsuggests that areas marked as water bodies in Barcelona are ocean\\nwaterfronts, whereas this class for all other cities represents rivers\\nor lakes.Figure 11: Samples from three urban environments across\\nour 6 example cities. We sampled the 2-d t-SNE embedding\\nof Figure 9 and queried for the closest real sample to the\\ncentroid using an ecient KD-tree search.\\nFinally, we explore the feature maps extracted by the convolu-\\ntional model in order to illustrate how similar the six cities we\\nused throughout this analysis are across three example environ-\\nments, green urban areas, water bodies, and medium density urban\\nfabric. For each city and land use class, we start o the centroid of\\nthe point cloud in the 2-d space of Figure 9, and /f_ind the nearest\\nseveral samples using the KD-tree method described in Section 4.\\nWe present the results in Figure 11. Visual inspection indicates\\nthat the model has learned useful feature maps about urban envi-\\nronments: the sample image patches show a very good qualitative\\nagreement with the region of the space where theyre sampled from,\\nindicated by the land use class of neighboring points. /Q_ualitatively,\\nit is clear that the features extracted from the top layer of the con-\\nvolutional model allow a comparison between urban environments\\nby high-level concepts used in urban planning.']\n",
            "Section 6 CONCLUSIONS sentences:\n",
            " ['6 CONCLUSIONS\\n/T_his paper has investigated the use of convolutional neural net-\\nworks for analyzing urban environments through satellite imagery\\nat the scale of entire cities. Given the current relative dearth of\\nlabeled satellite imagery in the machine learning community, we\\nhave constructed an open dataset of over 140 ,000 samples over 10\\nconsistent land use classes from 6 cities in Europe. As we continue\\nto improve, curate, and expand this dataset, we hope that it can help\\nother researchers in machine learning, smart cities, urban planning,\\nand related /f_ields in their work on understanding cities.\\nWe set out to study similarity and variability across urban envi-\\nronments, as being able to quantify such pa/t_terns will enable richer\\napplications in topics such as urban energy analysis, infrastructure\\nbenchmarking, and socio-economic composition of communities.\\nWe formulated this as a two-step task: /f_irst predicting urban land\\nuse classes from satellite imagery, then turning this (rigid) clas-\\nsi/f_ication into a continuous spectrum by embedding the features\\nextracted from the convolutional classi/f_ier into a lower-dimensional\\nmanifold. We show that the classi/f_ication task achieves encour-\\naging results, given the large variety in physical appearance of\\nurban environments having the same functional class. Moreover,we exemplify how the features extracted from the convolutional\\nnetwork allow for identifying neighbors of any given query im-\\nage, allowing a rich comparison analysis of urban environments by\\ntheir visual composition.\\n/T_he analysis in this paper shows that some types urban envi-\\nronments are easier to infer than others, both in the intra- and\\ninter-city cases. For example, in all our experiments, the models\\nhad most trouble telling apart high, medium, and low den-\\nsity urban environments, a/t_testing to the subjectivity of such a\\nhigh-level classi/f_ication for urban planning purposes. However,\\nagricultural lands, forests, and airports tend to be visually similar\\nacross dierent cities - and the amount of relative dissimilarity can\\nbe quanti/f_ied using the methods in this paper. Green urban areas\\n(parks) are generally similar to forests or to leisure facilities, and\\nthe models do be/t_ter in the intra-city case than predicting across\\ncities. How industrial areas look is again less geography-speci/f_ic:\\ninter-city similarity is consistently larger than intra-city similarity.\\nAs such, for several classes we can expect learning to transfer from\\none geography to another. /T_hus, while it is not news that some\\ncities are more similar than others (Barcelona is visually closer to\\nAthens than it is to Berlin), the methodology in this paper allows\\nfor a more quantitative and practical comparison of similarity.\\nBy leveraging satellite data (available virtually world-wide), this\\napproach may allow for a low-cost way to analyze urban envi-\\nronments in locations where ground truth information on urban\\nplanning is not available. As future directions of this work, we\\nplan to i)continue to develop more rigorous ways to compare and\\nbenchmark urban neighborhoods, going deeper to physical ele-\\nments (vegetation, buildings, roads etc.); ii)improve and further\\ncurate the open Urban Environments dataset; and iii)extend this\\ntype of analysis to more cities across other geographical locations.\\nA PRACTICAL TRAINING DETAILS.\\nWe split our training data into a training set (80% of the data) and a\\nvalidation set (the remaining 20%). /T_his is separate from the data\\nsampled for the ground truth raster grids for each city, which we\\nonly used at test time. We implemented the architectures in the\\nopen-source deep learning framework Keras7(with a TensorFlow8\\nbackend). In all our experiments, we used popular data augmenta-\\ntion techniques, including random horizontal and vertical /f_lipping\\nof the input images, random shearing (up to 0 .1 radians), random\\nscaling (up to 120%), random rotations (by at most 15 degrees either\\ndirection). Input images were 224 2243 pixels in size (RGB\\nbands). For all experiments, we used stochastic gradient descent\\n(with its Adadelta variant) to optimize the network loss function (a\\nstandard multi-class cross-entropy), starting with a learning rate of\\n0.1, and halving the rate each 10 epochs. We trained our networks\\nfor at most 100 epochs, with 2000 samples in each epoch, stopping\\nthe learning process when the accuracy on the validation set did\\nnot improve for more than 10 epochs. Given the inherent imbalance\\nof the classes, we explicitly enforced that the minibatches used for\\ntraining were relatively balanced by a weighted sampling proce-\\ndure. For training, we used a cluster of 4 NVIDIA K80 GPUs, and\\ntested our models on a cluster of 48 CPUs.\\n7\\n8www.tensor/f_low.org']\n"
          ]
        }
      ],
      "source": [
        "# Using LSA for summarizing the text of each individual section\n",
        "\n",
        "# Calculate LSA scores for each sentence\n",
        "lsa_scores = np.sum(np.abs(lsa_matrix), axis=1)\n",
        "\n",
        "# Group sentences by section and rank within each group\n",
        "grouped_sentences = {}\n",
        "for sentence, section, score in zip(section_extraction, sections, lsa_scores):\n",
        "    grouped_sentences.setdefault(section, []).append((score, sentence))\n",
        "\n",
        "# Sentence tokenization function\n",
        "\n",
        "def tokenize_sentences(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    return sentences\n",
        "\n",
        "# Tokenize sentences for each section\n",
        "section_sentences = {section: tokenize_sentences(' '.join(sentence[1] for sentence in sentences)) for section, sentences in grouped_sentences.items()}\n",
        "\n",
        "# Choose the top-ranked sentences from each section\n",
        "num_top_sentences = 5\n",
        "top_ranked_sentences = []\n",
        "\n",
        "for section, sentences in grouped_sentences.items():\n",
        "    # Sort sentences by score in descending order\n",
        "    sentences = sorted(sentences, key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    # Choose the top-ranked sentences\n",
        "    top_sentences = [sentence[1] for sentence in sentences[:num_top_sentences]]\n",
        "\n",
        "    # Print the content of each sentence\n",
        "    print(f\"Section {section} sentences:\\n\", top_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LRAdACBhvli",
        "outputId": "dfebfd0a-aa97-4d19-9dee-cbc99ac3d14f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-ranked sentences for Section ABSTRACT:\n",
            "re-\n",
            "quire an understanding of built infrastructure and its environment,\n",
            "i.e., both low-level, physical features (amount of vegetation, build-\n",
            "ing area and geometry etc.\n",
            "We analyze pa/t_terns in land use in urban\n",
            "neighborhoods using large-scale satellite imagery data (which is\n",
            "available worldwide from third-party providers) and state-of-the-\n",
            "art computer vision techniques based on deep convolutional neural\n",
            "networks.\n",
            "We use this data to train and\n",
            "compare deep architectures which have recently shown good per-\n",
            "formance on standard computer vision tasks (image classi/f_ication\n",
            "and segmentation), including on geospatial data.\n",
            "We make our dataset available for other ma-\n",
            "chine learning researchers to use for remote-sensing applications.\n",
            "For supervision, given the limited availability of standard\n",
            "benchmarks for remote-sensing data, we obtain ground truth land\n",
            "use class labels carefully sampled from open-source surveys, in\n",
            "particular the Urban Atlas land classi/f_ication dataset of 20 land use\n",
            "classes across 300 European cities.\n",
            "\n",
            "\n",
            "Top-ranked sentences for Section 1 INTRODUCTION:\n",
            "some a/t_tributes (e.g., amount of built structures or vegetation) are\n",
            "directly interpretable, some others may not be.\n",
            "We contrast state-of-the-art convolutional architectures (the\n",
            "VGG-16 [ 19] and ResNet [ 7] networks) to train classi/f_iers that recog-\n",
            "nize broad land use classes from satellite imagery.\n",
            "We then use the\n",
            "features extracted from the model to perform a large-scale compar-\n",
            "ison of urban environments.\n",
            "We compare the ground truth ( top row ) with the predicted land use maps,\n",
            "either from using separate data collected from the same city ( middle row ), or using data from all other cities ( bo/t_tom row ).\n",
            "We see the work\n",
            "on cheaply curating a large-scale land use classi/f_ication dataset\n",
            "and comparing neighborhoods using deep representations that\n",
            "this paper puts forth as a necessary /f_irst step towards a granular\n",
            "understanding of urban environments in data-poor regions.\n",
            "\n",
            "\n",
            "Top-ranked sentences for Section 2 LITERATURE:\n",
            "While useful as input\n",
            "for pre-training more complex models, (e.g., image segmentation),\n",
            "this dataset does not allow to take the further steps for detailed\n",
            "land use analysis and comparison of urban environments across\n",
            "cities, which gap we hope our dataset will address.\n",
            "Urban environments have been analyzed using other types of\n",
            "imagery data that have become recently available.\n",
            "UC Merced.\n",
            "Similar early studies on land use classi/f_ication\n",
            "that employ deep learning techniques are [ 21], [18], and [ 15].\n",
            "Some of the earliest work\n",
            "that develops and applies deep neural networks for this tasks is thatof [13].\n",
            "\n",
            "\n",
            "Top-ranked sentences for Section 3 THE URBAN ENVIRONMENTS DATASET:\n",
            "for each of the classes for three example cities (Budapest, Rome,\n",
            "Barcelona) from the dataset (from Eastern, Central, and Western\n",
            "Europe, respectively).\n",
            "We chose expressly to use\n",
            "the Urban Atlas dataset over other sources (described in Section 2.1\n",
            "because i)it is a comprehensive and consistent survey at a large\n",
            "scale, which has been extensively curated by experts and used in\n",
            "research, planning, and socio-economic work over the past decade,\n",
            "andii)the land use classes re/f_lect higher-level (socio-economic,\n",
            "cultural) functions of the land as used in applications.\n",
            "We note that there is a wide variance in the distribution of land\n",
            "use classes across and within the 300 cities.\n",
            "We take a satellite\n",
            "image sample in each grid cell, and assign to it as label the class\n",
            "of the polygon that has the maximum intersection area with that\n",
            "cell.\n",
            "To this\n",
            "end, we require that the image contain at least 25% (by area) of\n",
            "the associated ground truth polygon.\n",
            "\n",
            "\n",
            "Top-ranked sentences for Section 4 EXPERIMENTAL SETUP:\n",
            "We have implemented\n",
            "the following approaches: 1)we used models pre-trained on the\n",
            "ImageNet dataset, then further /f_ine-tuned them on the Urban Atlas\n",
            "dataset; and 2)we pre-trained on the DeepSat dataset (See Section\n",
            "2), then further re/f_ined on the Urban Atlas dataset.\n",
            "We would like to study how similar dierent classes of urban\n",
            "environments are across two example cities (here we picked Berlin\n",
            "and Barcelona, which are fairly dierent from a cultural and archi-\n",
            "tectural standpoint).\n",
            "VGG-16.\n",
            "Transfer learning.\n",
            "To be able\n",
            "to quantify similarity in local urban environments, we construct\n",
            "a KD-tree T(using a high-performance implementation available\n",
            "in the Python package scikit-learn [16]) using all the gridded\n",
            "samples.\n",
            "\n",
            "\n",
            "Top-ranked sentences for Section 5 RESULTS AND DISCUSSION:\n",
            "urban spaces seem fairly well resolved for most cities.\n",
            "to allow for discriminating\n",
            "between concepts that are fairly high-level.\n",
            "datasets (like this one) are released, performance will certainly im-\n",
            "prove.\n",
            "While accuracies or 0.700.80 may not\n",
            "look as impressive as those obtained by convolutional architectures\n",
            "on well-studied benchmarks and other classi/f_ication tasks (e.g.,natural images from ImageNet or small aerial patches from Deep-\n",
            "Sat), this only a/t_tests to the diculty of the task of understanding\n",
            "high-level, subjective concepts of urban planning in complex ur-\n",
            "ban environments.\n",
            "We next looked at per-class model performance to understand\n",
            "what types of environments are harder for the model to distin-\n",
            "guish.\n",
            "\n",
            "\n",
            "Top-ranked sentences for Section 6 CONCLUSIONS:\n",
            "We set out to study similarity and variability across urban envi-\n",
            "ronments, as being able to quantify such pa/t_terns will enable richer\n",
            "applications in topics such as urban energy analysis, infrastructure\n",
            "benchmarking, and socio-economic composition of communities.\n",
            "We formulated this as a two-step task: /f_irst predicting urban land\n",
            "use classes from satellite imagery, then turning this (rigid) clas-\n",
            "si/f_ication into a continuous spectrum by embedding the features\n",
            "extracted from the convolutional classi/f_ier into a lower-dimensional\n",
            "manifold.\n",
            "We show that the classi/f_ication task achieves encour-\n",
            "aging results, given the large variety in physical appearance of\n",
            "urban environments having the same functional class.\n",
            "We split our training data into a training set (80% of the data) and a\n",
            "validation set (the remaining 20%).\n",
            "We implemented the architectures in the\n",
            "open-source deep learning framework Keras7(with a TensorFlow8\n",
            "backend).\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Creating a list to store the top-5 sentences of each section\n",
        "\n",
        "# Iterate through each section and extract the top-ranked sentences\n",
        "for section, sentences in section_sentences.items():\n",
        "    # Sort sentences by score in descending order\n",
        "    sentences = sorted(sentences, key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    # Choose the top-ranked sentences\n",
        "    top_sentences = [sentence for sentence in sentences[:num_top_sentences]]\n",
        "\n",
        "    # Append the top sentences to the result list\n",
        "    top_ranked_sentences.extend(top_sentences)\n",
        "\n",
        "    print(f\"Top-ranked sentences for Section {section}:\\n\" + ''.join(sentence + '\\n' for sentence in top_sentences) + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "GXFGDR1N2RkI",
        "outputId": "51ff5b03-06e4-4f76-a4c5-263b35bdde3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-ranked sentences for Section ABSTRACT:\n",
            "re-\n",
            "quire an understanding of built infrastructure and its environment,\n",
            "i.e., both low-level, physical features (amount of vegetation, build-\n",
            "ing area and geometry etc.\n",
            "We analyze pa/t_terns in land use in urban\n",
            "neighborhoods using large-scale satellite imagery data (which is\n",
            "available worldwide from third-party providers) and state-of-the-\n",
            "art computer vision techniques based on deep convolutional neural\n",
            "networks.\n",
            "We use this data to train and\n",
            "compare deep architectures which have recently shown good per-\n",
            "formance on standard computer vision tasks (image classi/f_ication\n",
            "and segmentation), including on geospatial data.\n",
            "We make our dataset available for other ma-\n",
            "chine learning researchers to use for remote-sensing applications.\n",
            "For supervision, given the limited availability of standard\n",
            "benchmarks for remote-sensing data, we obtain ground truth land\n",
            "use class labels carefully sampled from open-source surveys, in\n",
            "particular the Urban Atlas land classi/f_ication dataset of 20 land use\n",
            "classes across 300 European cities.\n",
            "\n",
            "\n",
            "Top-ranked sentences for Section 1 INTRODUCTION:\n",
            "some a/t_tributes (e.g., amount of built structures or vegetation) are\n",
            "directly interpretable, some others may not be.\n",
            "We contrast state-of-the-art convolutional architectures (the\n",
            "VGG-16 [ 19] and ResNet [ 7] networks) to train classi/f_iers that recog-\n",
            "nize broad land use classes from satellite imagery.\n",
            "We then use the\n",
            "features extracted from the model to perform a large-scale compar-\n",
            "ison of urban environments.\n",
            "We compare the ground truth ( top row ) with the predicted land use maps,\n",
            "either from using separate data collected from the same city ( middle row ), or using data from all other cities ( bo/t_tom row ).\n",
            "We see the work\n",
            "on cheaply curating a large-scale land use classi/f_ication dataset\n",
            "and comparing neighborhoods using deep representations that\n",
            "this paper puts forth as a necessary /f_irst step towards a granular\n",
            "understanding of urban environments in data-poor regions.\n",
            "\n",
            "\n",
            "Top-ranked sentences for Section 2 LITERATURE:\n",
            "While useful as input\n",
            "for pre-training more complex models, (e.g., image segmentation),\n",
            "this dataset does not allow to take the further steps for detailed\n",
            "land use analysis and comparison of urban environments across\n",
            "cities, which gap we hope our dataset will address.\n",
            "Urban environments have been analyzed using other types of\n",
            "imagery data that have become recently available.\n",
            "UC Merced.\n",
            "Similar early studies on land use classi/f_ication\n",
            "that employ deep learning techniques are [ 21], [18], and [ 15].\n",
            "Some of the earliest work\n",
            "that develops and applies deep neural networks for this tasks is thatof [13].\n",
            "\n",
            "\n",
            "Top-ranked sentences for Section 3 THE URBAN ENVIRONMENTS DATASET:\n",
            "for each of the classes for three example cities (Budapest, Rome,\n",
            "Barcelona) from the dataset (from Eastern, Central, and Western\n",
            "Europe, respectively).\n",
            "We chose expressly to use\n",
            "the Urban Atlas dataset over other sources (described in Section 2.1\n",
            "because i)it is a comprehensive and consistent survey at a large\n",
            "scale, which has been extensively curated by experts and used in\n",
            "research, planning, and socio-economic work over the past decade,\n",
            "andii)the land use classes re/f_lect higher-level (socio-economic,\n",
            "cultural) functions of the land as used in applications.\n",
            "We note that there is a wide variance in the distribution of land\n",
            "use classes across and within the 300 cities.\n",
            "We take a satellite\n",
            "image sample in each grid cell, and assign to it as label the class\n",
            "of the polygon that has the maximum intersection area with that\n",
            "cell.\n",
            "To this\n",
            "end, we require that the image contain at least 25% (by area) of\n",
            "the associated ground truth polygon.\n",
            "\n",
            "\n",
            "Top-ranked sentences for Section 4 EXPERIMENTAL SETUP:\n",
            "We have implemented\n",
            "the following approaches: 1)we used models pre-trained on the\n",
            "ImageNet dataset, then further /f_ine-tuned them on the Urban Atlas\n",
            "dataset; and 2)we pre-trained on the DeepSat dataset (See Section\n",
            "2), then further re/f_ined on the Urban Atlas dataset.\n",
            "We would like to study how similar dierent classes of urban\n",
            "environments are across two example cities (here we picked Berlin\n",
            "and Barcelona, which are fairly dierent from a cultural and archi-\n",
            "tectural standpoint).\n",
            "VGG-16.\n",
            "Transfer learning.\n",
            "To be able\n",
            "to quantify similarity in local urban environments, we construct\n",
            "a KD-tree T(using a high-performance implementation available\n",
            "in the Python package scikit-learn [16]) using all the gridded\n",
            "samples.\n",
            "\n",
            "\n",
            "Top-ranked sentences for Section 5 RESULTS AND DISCUSSION:\n",
            "urban spaces seem fairly well resolved for most cities.\n",
            "to allow for discriminating\n",
            "between concepts that are fairly high-level.\n",
            "datasets (like this one) are released, performance will certainly im-\n",
            "prove.\n",
            "While accuracies or 0.700.80 may not\n",
            "look as impressive as those obtained by convolutional architectures\n",
            "on well-studied benchmarks and other classi/f_ication tasks (e.g.,natural images from ImageNet or small aerial patches from Deep-\n",
            "Sat), this only a/t_tests to the diculty of the task of understanding\n",
            "high-level, subjective concepts of urban planning in complex ur-\n",
            "ban environments.\n",
            "We next looked at per-class model performance to understand\n",
            "what types of environments are harder for the model to distin-\n",
            "guish.\n",
            "\n",
            "\n",
            "Top-ranked sentences for Section 6 CONCLUSIONS:\n",
            "We set out to study similarity and variability across urban envi-\n",
            "ronments, as being able to quantify such pa/t_terns will enable richer\n",
            "applications in topics such as urban energy analysis, infrastructure\n",
            "benchmarking, and socio-economic composition of communities.\n",
            "We formulated this as a two-step task: /f_irst predicting urban land\n",
            "use classes from satellite imagery, then turning this (rigid) clas-\n",
            "si/f_ication into a continuous spectrum by embedding the features\n",
            "extracted from the convolutional classi/f_ier into a lower-dimensional\n",
            "manifold.\n",
            "We show that the classi/f_ication task achieves encour-\n",
            "aging results, given the large variety in physical appearance of\n",
            "urban environments having the same functional class.\n",
            "We split our training data into a training set (80% of the data) and a\n",
            "validation set (the remaining 20%).\n",
            "We implemented the architectures in the\n",
            "open-source deep learning framework Keras7(with a TensorFlow8\n",
            "backend).\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Creating a dictionary for stroing top-ranked sentences\n",
        "\n",
        "top_ranked_sentences = {}\n",
        "\n",
        "# Iterate through each section and extract the top-ranked sentences\n",
        "for section, sentences in section_sentences.items():\n",
        "    # Sort sentences by score in descending order\n",
        "    sentences = sorted(sentences, key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    # Choose the top-ranked sentences\n",
        "    top_sentences = [sentence for sentence in sentences[:num_top_sentences]]\n",
        "\n",
        "    # Store the top sentences in the dictionary\n",
        "    top_ranked_sentences[section] = top_sentences\n",
        "\n",
        "    # Print the top-ranked sentences for each section\n",
        "    print(f\"Top-ranked sentences for Section {section}:\\n\" + ''.join(sentence + '\\n' for sentence in top_sentences) + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-mwnjUKfomF"
      },
      "source": [
        "### Measuring the similarity score of each section summaries with user query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyHs2RHGQUTO"
      },
      "source": [
        "#### Using TF-IDF embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYl1-lYTpnY6",
        "outputId": "2546a02b-4c6a-4b0b-fe63-332995b0fd0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity score with Section 'ABSTRACT': 0.3116259848185407\n",
            "Similarity score with Section '1 INTRODUCTION': 0.29391739410074685\n",
            "Similarity score with Section '2 LITERATURE': 0.2859426965796846\n",
            "Similarity score with Section '3 THE URBAN ENVIRONMENTS DATASET': 0.1997178783703218\n",
            "Similarity score with Section '4 EXPERIMENTAL SETUP': 0.16393622448566736\n",
            "Similarity score with Section '5 RESULTS AND DISCUSSION': 0.23206590926678103\n",
            "Similarity score with Section '6 CONCLUSIONS': 0.26121236430436706\n"
          ]
        }
      ],
      "source": [
        "# Similarity score of query with each section containing entire text in that section before applying LSA\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def text_similarity(text1, text2):\n",
        "    # Ensure that the input is a string\n",
        "    text1 = str(text1)\n",
        "    text2 = str(text2)\n",
        "\n",
        "    # Tokenize and lemmatize the texts, applying lowercasing to individual words\n",
        "    tokens1 = [word.lower() for word in sent_tokenize(text1)]\n",
        "    tokens2 = [word.lower() for word in sent_tokenize(text2)]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens1 = [lemmatizer.lemmatize(token) for token in tokens1]\n",
        "    tokens2 = [lemmatizer.lemmatize(token) for token in tokens2]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = stopwords.words('english')\n",
        "    tokens1 = [token for token in tokens1 if token not in stop_words]\n",
        "    tokens2 = [token for token in tokens2 if token not in stop_words]\n",
        "\n",
        "    # Join tokens into strings\n",
        "    text1_processed = ' '.join(tokens1)\n",
        "    text2_processed = ' '.join(tokens2)\n",
        "\n",
        "    # Create the TF-IDF vectors\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    vector1 = vectorizer.fit_transform([text1_processed])\n",
        "    vector2 = vectorizer.transform([text2_processed])\n",
        "\n",
        "    # Calculate the cosine similarity (single value for whole texts)\n",
        "    similarity = cosine_similarity(vector1, vector2)[0][0]\n",
        "\n",
        "    return similarity\n",
        "\n",
        "# Example usage:\n",
        "text2 = 'Urban environment analysis using satellite imagery and deep learning.'\n",
        "\n",
        "# Calculate similarity with each section header\n",
        "similarity_scores = {}\n",
        "for section in sections:\n",
        "    similarity_score = text_similarity(section_extraction[sections.index(section)], text2)\n",
        "    similarity_scores[section] = similarity_score\n",
        "\n",
        "# Display similarity scores\n",
        "for section, score in similarity_scores.items():\n",
        "      print(f\"Similarity score with Section '{section}': {score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jzic4Rizo-kO",
        "outputId": "4cc3489d-936e-48d6-e355-b911583cccd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity score with Section ABSTRACT after LSA': 0.28158129534819554\n",
            "Similarity score with Section 1 INTRODUCTION after LSA': 0.2385011978752744\n",
            "Similarity score with Section 2 LITERATURE after LSA': 0.3539191986482828\n",
            "Similarity score with Section 3 THE URBAN ENVIRONMENTS DATASET after LSA': 0.21350420507344955\n",
            "Similarity score with Section 4 EXPERIMENTAL SETUP after LSA': 0.31372790256907934\n",
            "Similarity score with Section 5 RESULTS AND DISCUSSION after LSA': 0.17213259316477417\n",
            "Similarity score with Section 6 CONCLUSIONS after LSA': 0.2636248650982481\n"
          ]
        }
      ],
      "source": [
        "# Similarity score of query with each section containing only top 5 ranked sentences in that section after applying LSA\n",
        "\n",
        "def text_similarity(text1, text2):\n",
        "    # Ensure that the input is a string\n",
        "    text1 = str(text1)\n",
        "    text2 = str(text2)\n",
        "\n",
        "    # Tokenize and lemmatize the texts, applying lowercasing to individual words\n",
        "    tokens1 = [word.lower() for word in sent_tokenize(text1)]\n",
        "    tokens2 = [word.lower() for word in sent_tokenize(text2)]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens1 = [lemmatizer.lemmatize(token) for token in tokens1]\n",
        "    tokens2 = [lemmatizer.lemmatize(token) for token in tokens2]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = stopwords.words('english')\n",
        "    tokens1 = [token for token in tokens1 if token not in stop_words]\n",
        "    tokens2 = [token for token in tokens2 if token not in stop_words]\n",
        "\n",
        "    # Join tokens into strings\n",
        "    text1_processed = ' '.join(tokens1)\n",
        "    text2_processed = ' '.join(tokens2)\n",
        "\n",
        "    # Create the TF-IDF vectors\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    vector1 = vectorizer.fit_transform([text1_processed])\n",
        "    vector2 = vectorizer.transform([text2_processed])\n",
        "\n",
        "    # Calculate the cosine similarity (single value for whole texts)\n",
        "    similarity = cosine_similarity(vector1, vector2)[0][0]\n",
        "\n",
        "    return similarity\n",
        "\n",
        "# Example usage:\n",
        "text2 = 'Urban environment analysis using satellite imagery and deep learning.'\n",
        "\n",
        "# Calculate similarity with each section header\n",
        "similarity_scores = {}\n",
        "for section in sections:\n",
        "    similarity_score = text_similarity(' '.join(top_ranked_sentences[section]), text2)\n",
        "    similarity_scores[section] = similarity_score\n",
        "\n",
        "# Display similarity scores\n",
        "for section, score in similarity_scores.items():\n",
        "    print(f\"Similarity score with Section {section} after LSA': {score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jypg1727I15G",
        "outputId": "1d735a22-1453-421b-9129-44d4f315dffb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Similarity Score without LSA: 0.24977406456087278\n"
          ]
        }
      ],
      "source": [
        "# Getting average similarity score of the sections with entire text without LSA applied\n",
        "\n",
        "# Example text 2\n",
        "text2 = 'Urban environment analysis using satellite imagery and deep learning.'\n",
        "\n",
        "# Calculate similarity scores for each section header\n",
        "similarity_scores = []\n",
        "for section_text in section_extraction:\n",
        "    similarity_score = text_similarity(section_text, text2)\n",
        "    similarity_scores.append(similarity_score)\n",
        "\n",
        "# Calculate the average similarity score\n",
        "average_similarity = sum(similarity_scores) / len(similarity_scores)\n",
        "\n",
        "print(\"Average Similarity Score without LSA:\", average_similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J37Nj3l6pqOt",
        "outputId": "4e434c10-e383-4bc2-f91a-9a83f4597746"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Similarity Score with Top Ranked Sentences after LSA: 0.2624273225396148\n"
          ]
        }
      ],
      "source": [
        "# Getting average similarity score of section only with top 5 ranked sentences after LSA applied\n",
        "\n",
        "def text_similarity(text1, text2):\n",
        "    # Ensure that the input is a string\n",
        "    text1 = str(text1)\n",
        "    text2 = str(text2)\n",
        "\n",
        "    # Tokenize and lemmatize the texts, applying lowercasing to individual words\n",
        "    tokens1 = [word.lower() for word in sent_tokenize(text1)]\n",
        "    tokens2 = [word.lower() for word in sent_tokenize(text2)]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens1 = [lemmatizer.lemmatize(token) for token in tokens1]\n",
        "    tokens2 = [lemmatizer.lemmatize(token) for token in tokens2]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = stopwords.words('english')\n",
        "    tokens1 = [token for token in tokens1 if token not in stop_words]\n",
        "    tokens2 = [token for token in tokens2 if token not in stop_words]\n",
        "\n",
        "    # Join tokens into strings\n",
        "    text1_processed = ' '.join(tokens1)\n",
        "    text2_processed = ' '.join(tokens2)\n",
        "\n",
        "    # Create the TF-IDF vectors\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    vector1 = vectorizer.fit_transform([text1_processed])\n",
        "    vector2 = vectorizer.transform([text2_processed])\n",
        "\n",
        "    # Calculate the cosine similarity (single value for whole texts)\n",
        "    similarity = cosine_similarity(vector1, vector2)[0][0]\n",
        "\n",
        "    return similarity\n",
        "\n",
        "# Example usage:\n",
        "text2 = 'Urban environment analysis using satellite imagery and deep learning.'\n",
        "\n",
        "# Calculate similarity scores for each section header using top ranked sentences\n",
        "similarity_scores = []\n",
        "for section, top_sentences in top_ranked_sentences.items():\n",
        "    section_text = ' '.join(top_sentences)\n",
        "    similarity_score = text_similarity(section_text, text2)\n",
        "    similarity_scores.append(similarity_score)\n",
        "\n",
        "# Calculate the average similarity score\n",
        "average_similarity = sum(similarity_scores) / len(similarity_scores)\n",
        "\n",
        "print(\"Average Similarity Score with Top Ranked Sentences after LSA:\", average_similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtPb0HnyLLjH",
        "outputId": "51759a34-27c1-4a3a-d0cf-a28af12279e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------------------------------+------------------+\n",
            "| Section with entire pdf text using TF-IDF | Similarity Score |\n",
            "+-------------------------------------------+------------------+\n",
            "|                  ABSTRACT                 |     0.311626     |\n",
            "|               1 INTRODUCTION              |     0.293917     |\n",
            "|                2 LITERATURE               |     0.285943     |\n",
            "|      3 THE URBAN ENVIRONMENTS DATASET     |     0.199718     |\n",
            "|            4 EXPERIMENTAL SETUP           |     0.163936     |\n",
            "|          5 RESULTS AND DISCUSSION         |     0.232066     |\n",
            "|               6 CONCLUSIONS               |     0.261212     |\n",
            "|             Average Similarity            |     0.249774     |\n",
            "+-------------------------------------------+------------------+\n"
          ]
        }
      ],
      "source": [
        "# Table for similarity score of sections without LSA\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "# Example usage:\n",
        "text2 = 'Urban environment analysis using satellite imagery and deep learning.'\n",
        "\n",
        "# Calculate similarity with each section header\n",
        "similarity_scores = {}\n",
        "for section in sections:\n",
        "   \n",
        "    similarity_score = text_similarity(section_extraction[sections.index(section)], text2)\n",
        "    similarity_scores[section] = similarity_score\n",
        "\n",
        "# Display similarity scores in a table\n",
        "table = PrettyTable()\n",
        "table.field_names = [\"Section with entire pdf text using TF-IDF\", \"Similarity Score\"]\n",
        "\n",
        "for section, score in similarity_scores.items():\n",
        "    table.add_row([section, f\"{score:.6f}\"])\n",
        "\n",
        "# Calculate and add the average similarity score to the table\n",
        "if similarity_scores.values != 0 :\n",
        "    average_similarity = sum(similarity_scores.values()) / len(similarity_scores)\n",
        "    table.add_row([\"Average Similarity\", f\"{average_similarity:.6f}\"])\n",
        "else:\n",
        "    pass\n",
        "\n",
        "# Print the table\n",
        "print(table)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nB7uBzAqNU5",
        "outputId": "ddad28cf-0f20-46c5-c338-0fa95909f8f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------+------------------+\n",
            "|   Section with LSA for TF-IDF    | Similarity Score |\n",
            "+----------------------------------+------------------+\n",
            "|             ABSTRACT             |     0.281581     |\n",
            "|          1 INTRODUCTION          |     0.238501     |\n",
            "|           2 LITERATURE           |     0.353919     |\n",
            "| 3 THE URBAN ENVIRONMENTS DATASET |     0.213504     |\n",
            "|       4 EXPERIMENTAL SETUP       |     0.313728     |\n",
            "|     5 RESULTS AND DISCUSSION     |     0.172133     |\n",
            "|          6 CONCLUSIONS           |     0.263625     |\n",
            "| Average Similarity (Top Ranked)  |     0.262427     |\n",
            "+----------------------------------+------------------+\n"
          ]
        }
      ],
      "source": [
        "# Table for similarity score of sections with top ranked sentences of LSA\n",
        "\n",
        "# Example text 2\n",
        "text2 = 'Urban environment analysis using satellite imagery and deep learning.'\n",
        "\n",
        "# Calculate similarity scores for each section using top-ranked sentences\n",
        "similarity_scores_top_ranked = {}\n",
        "for section, top_sentences in top_ranked_sentences.items():\n",
        "    section_text = ' '.join(top_sentences)\n",
        "    similarity_score = text_similarity(section_text, text2)\n",
        "    similarity_scores_top_ranked[section] = similarity_score\n",
        "\n",
        "# Display similarity scores in a table for top-ranked sentences\n",
        "table_top_ranked = PrettyTable()\n",
        "table_top_ranked.field_names = [\"Section with LSA for TF-IDF\", \"Similarity Score\"]\n",
        "\n",
        "for section, score in similarity_scores_top_ranked.items():\n",
        "    table_top_ranked.add_row([section, f\"{score:.6f}\"])\n",
        "\n",
        "# Calculate and add the average similarity score to the table\n",
        "average_similarity_top_ranked = sum(similarity_scores_top_ranked.values()) / len(similarity_scores_top_ranked)\n",
        "table_top_ranked.add_row([\"Average Similarity (Top Ranked)\", f\"{average_similarity_top_ranked:.6f}\"])\n",
        "\n",
        "# Print the table for top-ranked sentences\n",
        "print(table_top_ranked)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCTbXl5CQ_Oo"
      },
      "source": [
        "### Evaluation Metric for validating the performance of LSA summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'satellite_imagery'"
            ]
          },
          "execution_count": 166,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Getting the name of the input pdf\n",
        "\n",
        "import os\n",
        "file_name = os.path.basename(pdf_path)\n",
        "file_name_without_extension = os.path.splitext(file_name)[0]\n",
        "file_name_without_extension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary dictionary for satellite_imagery.pdf : {'ABSTRACT': 'Urban planning applications, including energy audits and investments, necessitate a comprehensive understanding of built infrastructure and its environment, encompassing both low-level physical features and higher-level concepts like socio-economic land use classes. Obtaining such data is costly and labor-intensive, posing challenges, especially in developing countries. Our approach involves analyzing land use patterns in urban areas using large-scale satellite imagery and advanced computer vision techniques. We leverage deep convolutional neural networks, train models on a dataset derived from open-source surveys, specifically the Urban Atlas classification dataset, and make our findings and dataset accessible to machine learning researchers for remote-sensing applications.', '1 INTRODUCTION': 'Urban land use classification plays a crucial role in various applications, including urban planning, zoning, business permits, real estate, and infrastructure development. Traditionally, this task is labor-intensive and infrequent, limiting data availability in developing countries. However, advancements in satellite technology and deep learning models for image classification enable a more democratic and inclusive analysis of urban environments. This paper contributes to deep learning literature and smart cities studies by contrasting convolutional architectures, such as VGG-16 and ResNet, for land use classification using a novel dataset from 10 European cities. The dataset, publicly available, pairs ground truth labels from the Urban Atlas survey with satellite imagery, facilitating automated classification in data-poor regions. The work emphasizes the importance of a comprehensive understanding of urban environments for future analyses and algorithm development. The paper reviews related studies, describes the curated dataset, details the deep learning architectures, and concludes with code availability for transparency and reproducibility.', '2 LITERATURE': 'This paper delves into the intersection of remote sensing data and machine learning, specifically deep learning architectures, for studying urban environments. It highlights the recent emergence of deep learning methods in land use classification using satellite imagery, citing early studies utilizing convolutional neural networks on datasets like UC Merced and DeepSat. The work emphasizes the scarcity of benchmark datasets for land use classification and discusses existing efforts, including projects like TerraPattern and DeepOSM, which use open-source data for similar purposes. The paper also touches on related studies that explore urban environments using other imagery sources like Google Street View and ground-level imagery, as well as mobile phone call records for inferring land use classes.', '3 THE URBAN ENVIRONMENTS DATASET': 'The Urban Atlas, an open-source dataset, encompasses approximately 300 European cities, each with a population of over 100,000 inhabitants, providing standardized land use classifications. Originating from a European Union initiative between 2005-2011, the dataset includes 20 standardized land use classes organized in detailed polygons. For this study, 10 final classes were selected for analysis, reflecting higher-level socio-economic and cultural functions of land. The authors employed a carefully designed sampling strategy, leveraging freely-available data sources such as the Google Maps Static API, to obtain high-quality satellite imagery and ground truth labels for training convolutional architectures in urban environment classification. The resulting dataset covers six cities and is used for both training and validation, demonstrating a balance in land use classes and ensuring representative samples for analysis.', '4 EXPERIMENTAL SETUP': 'The paper conducts experiments comparing the VGG-16 and ResNet architectures for urban environment classification. Transfer learning is employed, with models pre-trained on ImageNet and DeepSat datasets before fine-tuning on the Urban Atlas dataset. Results indicate that pre-training on DeepSat and refining on Urban Atlas yields the best accuracy improvements. The study also explores the transferability of models across different geographical locations, focusing on the similarity of urban environments in cities like Berlin and Barcelona. Feature extraction is performed using the last layer of the convolutional architectures, and a KD-tree is constructed to quantify similarity in local urban environments.', '5 RESULTS AND DISCUSSION': 'The study evaluates the performance of convolutional architectures, particularly ResNet-50, in classifying urban land use from satellite imagery across six European cities. Model predictions are compared with ground truth grids, demonstrating qualitative agreement in capturing urban land use classifications. Despite the complexity of the task, the models achieve satisfactory classification performance, especially when considering the challenging nature of high-level, subjective concepts in urban planning. The analysis further explores transfer learning across cities, per-class model performance, and the sensitivity of results to spatial scale, providing insights into the similarities and differences in urban environments. The study also employs t-SNE embeddings to visualize and compare urban land use classes across the six cities, highlighting variations and commonalities.', '6 CONCLUSIONS': 'In this paper, we present an adaptive knowledge distillation approach to improve NMT for low-resource languages. We address the inefficiency of the original transfer learning and multilingual learning by making wiser use of all high-resource languages and models in an effective collaborative learning manner.'}\n"
          ]
        }
      ],
      "source": [
        "# Fetching reference summaries (created by ChatGPT) from an external excel file\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "data = pd.read_excel('reference_summaries.xlsx')\n",
        "\n",
        "pdf_name = file_name_without_extension + '.pdf'\n",
        "\n",
        "row = data[data['pdf_name'] == pdf_name]\n",
        "\n",
        "if not row.empty:\n",
        "    summary = row['Summaries'].iloc[0]\n",
        "    try:\n",
        "        summary_dict = ast.literal_eval(summary)\n",
        "        print(\"Summary dictionary for\", pdf_name, \":\", summary_dict)\n",
        "    except ValueError:\n",
        "        print(\"Error: Summary string for\", pdf_name, \"is not in a valid dictionary format.\")\n",
        "else:\n",
        "    print(\"PDF name\", pdf_name, \"not found in the Excel file.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2znKFZQ0PX8",
        "outputId": "d75d516a-e963-4b0b-b68a-e23038725bee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scores for Section 'ABSTRACT' (Sections have the entire text in the generated summary):\n",
            "BLEU Score: 0.09949653282384059\n",
            "METEOR Score: 0.5166063983503665\n",
            "ROUGE-1 F-measure: 0.44568245125348194\n",
            "ROUGE-2 F-measure: 0.24649859943977592\n",
            "ROUGE-L F-measure: 0.38440111420612816\n",
            "\n",
            "Scores for Section '1 INTRODUCTION' (Sections have the entire text in the generated summary):\n",
            "BLEU Score: 0.044791149165453105\n",
            "METEOR Score: 0.3657405536472015\n",
            "ROUGE-1 F-measure: 0.25311601150527324\n",
            "ROUGE-2 F-measure: 0.11719500480307395\n",
            "ROUGE-L F-measure: 0.18216682646212848\n",
            "\n",
            "Scores for Section '2 LITERATURE' (Sections have the entire text in the generated summary):\n",
            "BLEU Score: 0.01851513377673706\n",
            "METEOR Score: 0.26721936344577857\n",
            "ROUGE-1 F-measure: 0.15247364152473644\n",
            "ROUGE-2 F-measure: 0.06823720552396426\n",
            "ROUGE-L F-measure: 0.09894566098945662\n",
            "\n",
            "Scores for Section '3 THE URBAN ENVIRONMENTS DATASET' (Sections have the entire text in the generated summary):\n",
            "BLEU Score: 0.017753096140671592\n",
            "METEOR Score: 0.28819424200857835\n",
            "ROUGE-1 F-measure: 0.17974105102817975\n",
            "ROUGE-2 F-measure: 0.08695652173913043\n",
            "ROUGE-L F-measure: 0.1172886519421173\n",
            "\n",
            "Scores for Section '4 EXPERIMENTAL SETUP' (Sections have the entire text in the generated summary):\n",
            "BLEU Score: 0.029988475579097826\n",
            "METEOR Score: 0.3497278559361358\n",
            "ROUGE-1 F-measure: 0.20909090909090908\n",
            "ROUGE-2 F-measure: 0.09339407744874716\n",
            "ROUGE-L F-measure: 0.15000000000000002\n",
            "\n",
            "Scores for Section '5 RESULTS AND DISCUSSION' (Sections have the entire text in the generated summary):\n",
            "BLEU Score: 0.008408579266881306\n",
            "METEOR Score: 0.17619301484356759\n",
            "ROUGE-1 F-measure: 0.07589447054571738\n",
            "ROUGE-2 F-measure: 0.03182640144665461\n",
            "ROUGE-L F-measure: 0.04842790025298156\n",
            "\n",
            "Scores for Section '6 CONCLUSIONS' (Sections have the entire text in the generated summary):\n",
            "BLEU Score: 0.0013449332693866722\n",
            "METEOR Score: 0.13768686073957515\n",
            "ROUGE-1 F-measure: 0.06537530266343826\n",
            "ROUGE-2 F-measure: 0.012135922330097089\n",
            "ROUGE-L F-measure: 0.04842615012106537\n",
            "\n",
            "\n",
            "Generated Summary with Content of Top-Ranked Sentences:\n",
            "ABSTRACT\n",
            "Urban planning applications (energy audits, investment, etc.) re-\n",
            "quire an understanding of built infrastructure and its environment,\n",
            "i.e., both low-level, physical features (amount of vegetation, build-\n",
            "ing area and geometry etc. ), as well as higher-level concepts such\n",
            "as land use classes (which encode expert understanding of socio-\n",
            "economic end uses). /T_his kind of data is expensive and labor-\n",
            "intensive to obtain, which limits its availability (particularly in\n",
            "developing countries). We analyze pa/t_terns in land use in urban\n",
            "neighborhoods using large-scale satellite imagery data (which is\n",
            "available worldwide from third-party providers) and state-of-the-\n",
            "art computer vision techniques based on deep convolutional neural\n",
            "networks. For supervision, given the limited availability of standard\n",
            "benchmarks for remote-sensing data, we obtain ground truth land\n",
            "use class labels carefully sampled from open-source surveys, in\n",
            "particular the Urban Atlas land classi/f_ication dataset of 20 land use\n",
            "classes across 300 European cities. We use this data to train and\n",
            "compare deep architectures which have recently shown good per-\n",
            "formance on standard computer vision tasks (image classi/f_ication\n",
            "and segmentation), including on geospatial data. Furthermore, we\n",
            "show that the deep representations extracted from satellite imagery\n",
            "of urban environments can be used to compare neighborhoods\n",
            "across several cities. We make our dataset available for other ma-\n",
            "chine learning researchers to use for remote-sensing applications. CCS CONCEPTS\n",
            "Computing methodologies Computer vision; Neural net-\n",
            "works; Applied computing Environmental sciences;\n",
            "KEYWORDS\n",
            "Satellite imagery, land use classi/f_ication, convolutional networks 1 INTRODUCTION\n",
            "Land use classi/f_ication is an important input for applications rang-\n",
            "ing from urban planning, zoning and the issuing of business per-\n",
            "mits, to real-estate construction and evaluation to infrastructure\n",
            "Corresponding author. Permission to make digital or hard copies of part or all of this work for personal or\n",
            "classroom use is granted without fee provided that copies are not made or distributed\n",
            "for pro/f_it or commercial advantage and that copies bear this notice and the full citation\n",
            "on the /f_irst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). KDD17, August 1317, 2017, Halifax, NS, Canada. 2017 Copyright held by the owner/author(s). 978-1-4503-4887-4/17/08. DOI:  Urban land use classi/f_ication is typically based on\n",
            "surveys performed by trained professionals. As such, this task\n",
            "is labor-intensive, infrequent, slow, and costly. As a result, such\n",
            "data are mostly available in developed countries and big cities that\n",
            "have the resources and the vision necessary to collect and curate it;\n",
            "this information is usually not available in many poorer regions,\n",
            "including many developing countries [ 9] where it is mostly needed. /T_his paper builds on two recent trends that promise to make\n",
            "the analysis of urban environments a more democratic and inclu-\n",
            "sive task. On the one hand, recent years have seen signi/f_icant\n",
            "improvements in satellite technology and its deployment (primar-\n",
            "ily through commercial operators), which allows to obtain high\n",
            "and medium-resolution imagery of most urbanized areas of the\n",
            "Earth with an almost daily revisit rate. On the other hand, the\n",
            "recent breakthroughs in computer vision methods, in particular\n",
            "deep learning models for image classi/f_ication and object detection,\n",
            "now make possible to obtain a much more accurate representation\n",
            "of the composition built infrastructure and its environments. Our contributions are to both the applied deep learning literature,\n",
            "and to the incipient study of smart cities using remote sensing\n",
            "data. We contrast state-of-the-art convolutional architectures (the\n",
            "VGG-16 [ 19] and ResNet [ 7] networks) to train classi/f_iers that recog-\n",
            "nize broad land use classes from satellite imagery. We then use the\n",
            "features extracted from the model to perform a large-scale compar-\n",
            "ison of urban environments. For this, we construct a novel dataset\n",
            "for land use classi/f_ication, pairing carefully sampled locations with\n",
            "ground truth land use class labels obtained from the Urban Atlas\n",
            "survey [ 22] with satellite imagery obtained from Google Mapss\n",
            "static API. Our dataset - which we have made available publicly\n",
            "for other researchers - covers, for now, 10 cities in Europe (chosen\n",
            "out of the original 300) with 10 land use classes (from the original\n",
            "20). As the Urban Atlas is a widely-used, standardized dataset for\n",
            "land use classi/f_ication, we hope that making this dataset available\n",
            "will encourage the development analyses and algorithms for ana-\n",
            "lyzing the built infrastructure in urban environments. Moreover,\n",
            "given that satellite imagery is available virtually everywhere on\n",
            "the globe, the methods presented here allow for automated, rapid\n",
            "classi/f_ication of urban environments that can potentially be applied\n",
            "to locations where survey and zoning data is not available. Land use classi/f_ication refers to the combination of physical\n",
            "land a/t_tributes and what cultural and socio-economic function land\n",
            "serves (which is a subjective judgement by experts) [ 2]. In this paper,\n",
            "we take the view that land use classes are just a useful discretization\n",
            "of a more continuous spectrum of pa/t_terns in the organization of\n",
            "urban environments. /T_his viewpoint is illustrated in Figure 2: while\n",
            "arXiv:1704.02965v2  [cs.CV]  13 Sep 2017Figure 1: Urban land use maps for six example cities. We compare the ground truth ( top row ) with the predicted land use maps,\n",
            "either from using separate data collected from the same city ( middle row ), or using data from all other cities ( bo/t_tom row ). Figure 2: Le/f_t: Comparing urban environments via deep hi-\n",
            "erarchical representations of satellite image samples. Right:\n",
            "approach outline - data collection, classi/f_ication, feature ex-\n",
            "traction, clustering, validation. some a/t_tributes (e.g., amount of built structures or vegetation) are\n",
            "directly interpretable, some others may not be. Nevertheless, these\n",
            "pa/t_terns in/f_luence, and are in/f_luenced by, socio-economic factors\n",
            "(e.g., economic activity), resource use (energy), and dynamic human\n",
            "behavior (e.g., mobility, building occupancy). We see the work\n",
            "on cheaply curating a large-scale land use classi/f_ication dataset\n",
            "and comparing neighborhoods using deep representations that\n",
            "this paper puts forth as a necessary /f_irst step towards a granular\n",
            "understanding of urban environments in data-poor regions. Subsequently, in Section 2 we review related studies that apply\n",
            "deep learning methods and other machine learning techniques\n",
            "to problems of land use classi/f_ication, object detection, and image\n",
            "segmentation in aerial imagery. In Section 3 we describe the datasetwe curated based on the Urban Atlas survey. Section 4 reviews the\n",
            "deep learning architectures we used. Section 5 describes model\n",
            "validation and analysis results. We conclude in Section 6. All the code used to acquire, process, and analyze the data, as\n",
            "well as to train the models discussed in this paper is available at 2 LITERATURE\n",
            "/T_he literature on the use of remote sensing data for applications in\n",
            "land use cover, urban planning, environmental science, and others,\n",
            "has a long and rich history. /T_his paper however is concerned more\n",
            "narrowly with newer work that employs widely-available data\n",
            "and machine learning models - and in particular deep learning\n",
            "architectures - to study urban environments. Deep learning methods have only recently started to be deployed\n",
            "to the analysis of satellite imagery. As such, land use classi/f_ication\n",
            "using these tools is still a very incipient literature. Probably the /f_irst\n",
            "studies (yet currently only 1-2 years old) include the application\n",
            "of convolutional neural networks to land use classi/f_ication [ 2] us-\n",
            "ing the UC Merced land use dataset [ 25] (of 2100 images spanning\n",
            "21 classes) and the classi/f_ication of agricultural images of coee\n",
            "plantations [ 17]. Similar early studies on land use classi/f_ication\n",
            "that employ deep learning techniques are [ 21], [18], and [ 15]. In\n",
            "[11], a spatial pyramid pooling technique is employed for land use\n",
            "classi/f_ication using satellite imagery. /T_he authors of these studies\n",
            "adapted architectures pre-trained to recognize natural images from\n",
            "the ImageNet dataset (such as the VGG16 [ 19], which we also use),\n",
            "and /f_ine-tuned them on their (much smaller) land use data. More\n",
            "recent studies use the DeepSat land use benchmark dataset [ 1],\n",
            "which we also use and describe in more detail in Section 2.1. An-\n",
            "other topic that is closely related to ours is remote-sensing image\n",
            "segmentation and object detection, where modern deep learning\n",
            "models have also started to be applied. Some of the earliest work\n",
            "that develops and applies deep neural networks for this tasks is thatof [13]. Examples of recent studies include [ 26] and [ 12], where the\n",
            "authors propose a semantic image segmentation technique com-\n",
            "bining texture features and boundary detection in an end-to-end\n",
            "trainable architecture. Remote-sensing data and deep learning methods have been put\n",
            "to use to other related ends, e.g., geo-localization of ground-level\n",
            "photos via satellite images [ 3,24] or predicting ground-level scene\n",
            "images from corresponding aerial imagery [ 27]. Other applications\n",
            "have included predicting survey estimates on poverty levels in\n",
            "several countries in Africa by /f_irst learning to predict levels of night\n",
            "lights (considered as proxies of economic activity and measured\n",
            "by satellites) from day-time, visual-range imagery from Google\n",
            "Maps, then transferring the learning from this la/t_ter task to the\n",
            "former [ 9]. Our work takes a similar approach, in that we aim to use\n",
            "remote-sensing data (which is widely-available for most parts of\n",
            "the world) to infer land use types in those locations where ground\n",
            "truth surveys are not available. Urban environments have been analyzed using other types of\n",
            "imagery data that have become recently available. In [ 4,14], the\n",
            "authors propose to use the same type of imagery from Google Street\n",
            "View to measure the relationship between urban appearance and\n",
            "quality of life measures such as perceived safety. For this, they\n",
            "hand-cra/f_t standard image features widely used in the computer\n",
            "vision community, and train a shallow machine learning classi/f_ier\n",
            "(a support vector machine). In a similar fashion, [ 5] trained a\n",
            "convolutional neural network on ground-level Street View imagery\n",
            "paired with a crowd-sourced mechanism for collecting ground truth\n",
            "labels to predict subjective perceptions of urban environments such\n",
            "as beauty, wealth, and liveliness. Land use classi/f_ication has been studied with other new data\n",
            "sources in recent years. For example, ground-level imagery has been\n",
            "employed to accurately predict land use classes on an university\n",
            "campus [ 28]. Another related literature strand is work that uses\n",
            "mobile phone call records to extract spatial and temporal mobility\n",
            "pa/t_terns, which are then used to infer land use classes for several\n",
            "cities [ 6,10,20]. Our work builds on some of the ideas for sampling\n",
            "geospatial data presented there. 2.1 Existing land use benchmark datasets\n",
            "Public benchmark data for land use classi/f_ication using aerial im-\n",
            "agery are still in relatively short supply. Presently there are two\n",
            "such datasets that we are aware of, discussed below. UC Merced. /T_his dataset was published in 2010 [ 25] and con-\n",
            "tains 2100 256 256, 1 m/pxaerial RGB images over 21 land use\n",
            "classes. It is considered a solved problem, as modern neural net-\n",
            "work based classi/f_iers [2] have achieved >95% accuracy on it. DeepSat. /T_he DeepSat [ 1] dataset1was released in 2015. It\n",
            "contains two benchmarks: the Sat-4 data of 500 ,000 images over 4\n",
            "land use classes ( barren land, trees, grassland, other ), and the Sat-6\n",
            "data of 405 ,000 images over 6 land use classes ( barren land, trees,\n",
            "grassland, roads, buildings, water bodies ). All the samples are 28 28\n",
            "in size at a 1 m/pxspatial resolution and contain 4 channels (red,\n",
            "green, blue, and NIR - near infrared). Currently less than two years\n",
            "old, this dataset is already a solved problem, with previous studies\n",
            "[15] (and our own experiments) achieving classi/f_ication accuracies\n",
            "1Available at  saikat/deepsat/.of over 99% using convolutional architectures. While useful as input\n",
            "for pre-training more complex models, (e.g., image segmentation),\n",
            "this dataset does not allow to take the further steps for detailed\n",
            "land use analysis and comparison of urban environments across\n",
            "cities, which gap we hope our dataset will address. Other open-source eorts. /T_here are several other projects\n",
            "that we are aware of related to land use classi/f_ication using open-\n",
            "source data. /T_he TerraPa/t_tern2project uses satellite imagery from\n",
            "Google Maps (just like we do) paired with truth labels over a large\n",
            "number (450) of detailed classes obtained using the Open Street\n",
            "Map API3. (Open Street Maps is a comprehensive, open-access,\n",
            "crowd-sourced mapping system.) /T_he projects intended use is as\n",
            "a search tool for satellite imagery, and as such, the classes they\n",
            "employ are very speci/f_ic, e.g., baseball diamonds, churches, or\n",
            "roundabouts. /T_he authors use a ResNet architecture [ 7] to train a\n",
            "classi/f_ication model, which they use to embed images in a high-\n",
            "dimensional feature space, where similar images to an input image\n",
            "can be identi/f_ied. A second open-source project related to ours is\n",
            "the DeepOSM4, in which the authors take the same approach of\n",
            "pairing OpenStreetMap labels with satellite imagery obtained from\n",
            "Google Maps, and use a convolutional architecture for classi/f_ication. /T_hese are excellent starting points from a practical standpoint,\n",
            "allowing interested researchers to quickly familiarize themselves\n",
            "with programming aspects of data collection, API calls, etc. 3 THE URBAN ENVIRONMENTS DATASET\n",
            "3.1 Urban Atlas: a standard in land use analysis\n",
            "/T_he Urban Atlas [ 22] is an open-source, standardized land use\n",
            "dataset that covers 300 European cities of 100 ,000 inhabitants or\n",
            "more, distributed relatively evenly across major geographical and\n",
            "geopolitical regions. /T_he dataset was created between 2005-2011 as\n",
            "part of a major eort by the European Union to provide a uniform\n",
            "framework for the geospatial analysis of urban areas in Europe. Land use classi/f_ication is encoded via detailed polygons organized\n",
            "in commonly-used GIS/ESRI shape /f_iles. /T_he dataset covers 20\n",
            "standardized land use classes. In this work we selected classes of\n",
            "interest and consolidated them into 10 /f_inal classes used for analysis\n",
            "(see Figure 3). Producing the original Urban Atlas dataset required\n",
            "fusing several data sources: high and medium-resolution satellite\n",
            "imagery, topographic maps, navigation and road layout data, and\n",
            "local zoning (cadastral) databases. More information on the method-\n",
            "ology used by the Urban Atlas researchers can be obtained from\n",
            "the European Environment Agency5. We chose expressly to use\n",
            "the Urban Atlas dataset over other sources (described in Section 2.1\n",
            "because i)it is a comprehensive and consistent survey at a large\n",
            "scale, which has been extensively curated by experts and used in\n",
            "research, planning, and socio-economic work over the past decade,\n",
            "andii)the land use classes re/f_lect higher-level (socio-economic,\n",
            "cultural) functions of the land as used in applications. We note that there is a wide variance in the distribution of land\n",
            "use classes across and within the 300 cities. Figure 3 illustrates\n",
            "the dierences in the distribution in ground truth polygon areas\n",
            "2\n",
            "3\n",
            "4\n",
            "5 3: Ground truth land use distribution (by area) for\n",
            "three example cities in the Urban Environments dataset. for each of the classes for three example cities (Budapest, Rome,\n",
            "Barcelona) from the dataset (from Eastern, Central, and Western\n",
            "Europe, respectively). /T_his wide disparity in the spatial distribution\n",
            "pa/t_terns of dierent land use classes and across dierent cities\n",
            "motivates us to design a careful sampling procedure for collecting\n",
            "training data, described in detail below. 3.2 Data sampling and acquisition\n",
            "We set out to develop a strategy to obtain high-quality samples\n",
            "of the type (satellite image, ground truth label) to use in training\n",
            "convolutional architectures for image classi/f_ication. Our /f_irst re-\n",
            "quirement is to do this solely with freely-available data sources,\n",
            "as to keep costs very low or close to zero. For this, we chose to\n",
            "use the Google Maps Static API6as a source of satellite imagery. /T_his service allows for 25 ,000 API requests/day free of charge. For\n",
            "a given sampling location given by (latitude, longitude), we ob-\n",
            "tained 224 224 images at a zoom level 17 (around 1 .20m/pxspatial\n",
            "resolution, or 250m250mcoverage for an image). /T_he goals of our sampling strategy are twofold. First, we want\n",
            "to ensure that the resulting dataset is as much as possible balanced\n",
            "with respect to the land use classes. /T_he challenge is that the classes\n",
            "are highly imbalanced among the ground truth polygons in the\n",
            "dataset (e.g., many more polygons are agricultural land and isolated\n",
            "structures than airports). Second, the satellite images should be\n",
            "representative of the ground truth class associated to them. To this\n",
            "end, we require that the image contain at least 25% (by area) of\n",
            "the associated ground truth polygon. /T_hus, our strategy to obtain\n",
            "training samples is as follows (for a given city):\n",
            "Sort ground truth polygons in decreasing order according to\n",
            "their size, and retain only those polygons with areas larger than\n",
            "1\n",
            "4(2241.2m)2=0.06km2;\n",
            "From each decile of the distribution of areas, sample a propor-\n",
            "tionally larger number of polygons, such that some of the smaller\n",
            "polygons also are picked, and more of the larger ones;\n",
            "For each picked polygon, sample a number of images propor-\n",
            "tional to the area of the polygon, and assign each image the\n",
            "polygon class as ground truth label;\n",
            "6\n",
            "Figure 4: Example satellite images for the original land use\n",
            "classes in the Urban Atlas dataset. Example satellite images for each of the 10 land use classes in\n",
            "the Urban Environments dataset are given in Figure 4. Note the\n",
            "signi/f_icant variety (in color schemes, textures, etc) in environments\n",
            "denoted as having the same land use class. /T_his is because of several\n",
            "factors, including the time of the year when the image was acquired\n",
            "(e.g., agricultural lands appear dierent in the spring than in the\n",
            "fall), the dierent physical form and appearance of environments\n",
            "that serve the same socioeconomic or cultural function (e.g., green\n",
            "urban areas may look very dierent in dierent cities or in even\n",
            "in dierent parts of the same city; what counts as dense urban\n",
            "fabric in one city may not be dense at all in other cities), and\n",
            "change in the landscape during the several years that have passed\n",
            "since the compilation of the Urban Atlas dataset and the time of\n",
            "acquisition of the satellite image (e.g., construction sites may not\n",
            "re/f_lect accurately anymore the reality on the ground). Apart from these training images, we constructed ground truth\n",
            "rasters to validate model output for each city. For that, we de/f_ined\n",
            "uniform validation grids of 100 100 (25km25km )around the\n",
            "(geographical) center of a given city of interest. We take a satellite\n",
            "image sample in each grid cell, and assign to it as label the class\n",
            "of the polygon that has the maximum intersection area with that\n",
            "cell. Examples of land use maps for the six cities we analyze here\n",
            "are given in Figure 1 (top row). /T_here, each grid cell is assigned the\n",
            "class of the ground truth polygon whose intersection with the cell\n",
            "has maximum coverage fraction by area. Classes are color-coded\n",
            "following the original Urban Atlas documentation. In Table 1 we present summaries of the training (le/f_t) and vali-\n",
            "dation (right) datasets we used for the analysis in this paper. /T_he\n",
            "validation dataset consists of the images sampled at the centers of\n",
            "each cell in the 25 km25kmgrid as discussed above. /T_his dataset\n",
            "consists of 140,000 images distributed across 10 urban environ-\n",
            "ment classes from 6 cities: Roma (Rome), Madrid, Berlin, Budapest,\n",
            "Barcelona, and Athina (Athens). Because of the high variation in\n",
            "appearance upon visual inspection, we chose to consolidate severalclasses from the original dataset, in particular classes that indicated\n",
            "urban fabric into High Density Urban Fabric, Medium Density\n",
            "Urban Fabric, and Low Density Urban Fabric. As mentioned above\n",
            "and illustrated in Figure 3, we did notice a great disparity in the\n",
            "numbers and distribution of ground truth polygons for other ex-\n",
            "ample cities that we investigated in the Urban Atlas dataset. As\n",
            "such,for the analysis in this paper, we have chosen cities where\n",
            "enough ground truth polygons were available for each class (that\n",
            "is, at least 50 samples) to allow for statistical comparisons. 4 EXPERIMENTAL SETUP\n",
            "4.1 Neural network architectures and training\n",
            "For all experiments in this paper we compared the VGG-16 [ 19]\n",
            "and ResNet [7, 8] architectures. VGG-16. /T_his architecture [ 19] has become one of the most pop-\n",
            "ular models in computer vision for classi/f_ication and segmentation\n",
            "tasks. It consists of 16 trainable layers organized in blocks. It starts\n",
            "with a 5-block convolutional base of neurons with 3 3 receptive\n",
            "/f_ields (alternated with max-pooling layers that eectively increase\n",
            "the receptive /f_ield of neurons further downstream). Following each\n",
            "convolutional layer is a ReLU activation function [ 19]. /T_he feature\n",
            "maps thus obtained are fed into a set of fully-connected layers (a\n",
            "deep neural network classi/f_ier). See Table 2 for a summary. ResNet. /T_his architecture [ 7,8] has achieved state-of-the-art\n",
            "performance on image classi/f_ication on several popular natural\n",
            "image benchmark datasets. It consists of blocks of convolutional\n",
            "layers, each of which is followed by a ReLU non-linearity. As before,\n",
            "each block in the convolutional base is followed by a max-pooling\n",
            "operation. Finally, the output of the last convolutional layer serves\n",
            "as input feature map for a fully-connected layer with a so/f_tmax\n",
            "activation function. /T_he key dierence in this architecture is that\n",
            "shortcut connections are implemented that skip blocks of convo-\n",
            "lutional layers, allowing the network to learn residual mappings\n",
            "between layer input and output. Here we used an implementation\n",
            "with 50 trainable layers per [7]. See Table 3 for a summary. Transfer learning. As it is common practice in the literature,\n",
            "we have experimented with training our models on the problem of\n",
            "interest (urban environment classi/f_ication) starting from architec-\n",
            "tures pre-trained on datasets from other domains ( transfer learning ). /T_his procedure has been shown to yield both be/t_ter performance\n",
            "and faster training times, as the network already has learned to\n",
            "recognize basic shapes and pa/t_terns that are characteristic of im-\n",
            "ages across many domains (e.g., [ 9,12,15]). We have implemented\n",
            "the following approaches: 1)we used models pre-trained on the\n",
            "ImageNet dataset, then further /f_ine-tuned them on the Urban Atlas\n",
            "dataset; and 2)we pre-trained on the DeepSat dataset (See Section\n",
            "2), then further re/f_ined on the Urban Atlas dataset. As expected,\n",
            "the la/t_ter strategy - /f_irst training a model (itself pre-trained on Ima-\n",
            "geNet data) on the DeepSat benchmark, and the further re/f_ining\n",
            "on the Urban Atlas dataset - yielded the best results, achieving\n",
            "increases of around 5% accuracy for a given training time. Given the large amount of variation in the visual appearance\n",
            "of urban environments across dierent cities (because of dierent\n",
            "climates, dierent architecture styles, various other socio-economic\n",
            "factors), it is of interest to study to what extent a model learned on\n",
            "one geographical location can be applied to a dierent geographicallocation. As such, we perform experiments in which we train a\n",
            "model for one (or more) cities, then apply the model to a dierent set\n",
            "of cities. Intuitively, one would expect that, the more neighborhoods\n",
            "and other urban features at one location are similar to those at a\n",
            "dierent location, the be/t_ter learning would transfer, and the higher\n",
            "the classi/f_ication accuracy obtained would be. Results for these\n",
            "experiments are summarized in Figure 6. 4.2 Comparing urban environments\n",
            "We next used the convolutional architectures to extract features\n",
            "for validation images. As in other recent studies (e.g., [ 9]), we use\n",
            "the last layer of a network as feature extractor. /T_his amounts to\n",
            "feature vectors of D=4096 dimensions for the VGG16 architecture\n",
            "andD=2048 dimensions for the ResNet-50 architecture. /T_he\n",
            "codes xRDare the image representations that either network\n",
            "derives as most representative to discriminate the high-level land\n",
            "use concepts it is trained to predict. We would like to study how similar dierent classes of urban\n",
            "environments are across two example cities (here we picked Berlin\n",
            "and Barcelona, which are fairly dierent from a cultural and archi-\n",
            "tectural standpoint). For this, we focus only on the 25 km25km,\n",
            "100100-cell grids around the city center as in Figure 1. To be able\n",
            "to quantify similarity in local urban environments, we construct\n",
            "a KD-tree T(using a high-performance implementation available\n",
            "in the Python package scikit-learn [16]) using all the gridded\n",
            "samples. /T_his data structure allows to /f_ind k-nearest neighbors of a\n",
            "query image in an ecient way. In this way, the feature space can\n",
            "be probed in an ecient way. 5 RESULTS AND DISCUSSION\n",
            "In Figure 1 we show model performance on the 100 100 (25 km\n",
            "25km) raster grids we used for testing. /T_he top row shows ground\n",
            "truth grids, where the class in each cell was assigned as the most\n",
            "prevalent land use class by area (see also Section 3). /T_he bo/t_tom row\n",
            "shows model predictions, where each cell in a raster is painted in\n",
            "the color corresponding to the maximum probability class estimated\n",
            "by the model (here ResNet-50). Columns in the /f_igure show results\n",
            "for each of the 6 cities we used in our dataset. Even at a /f_irst visual\n",
            "inspection, the model is able to recreate from satellite imagery\n",
            "qualitatively the urban land use classi/f_ication map. Further, looking at the individual classes separately and the con-\n",
            "/f_idence of the model in its predictions (the probability distribution\n",
            "over classes computed by the model), the picture is again qualita-\n",
            "tively very encouraging. In Figure 5 we show grayscale raster maps\n",
            "encoding the spatial layout of the class probability distribution for\n",
            "one example city, Barcelona. Particularly good qualitative agree-\n",
            "ment is observed for agricultural lands, water bodies, industrial,\n",
            "public, and commercial land, forests, green urban areas, low density\n",
            "urban fabric, airports, and sports and leisure facilities. /T_he model\n",
            "appears to struggle with reconstructing the spatial distribution of\n",
            "roads, which is not unexpected, given that roads typically appear\n",
            "in many other scenes that have a dierent functional classi/f_ication\n",
            "for urban planning purposes.Table 1: Urban Environments dataset: sample size summary. (a) Dataset used for training & validation ( 80%and20%, respectively)\n",
            "class/city athina barcelona berlin budapest madrid roma class\n",
            "total\n",
            "Agricultural + Semi-\n",
            "natural areas + Wet-\n",
            "lands4347 2987 7602 2211 4662 4043 25852\n",
            "Airports 382 452 232 138 124 142 1470\n",
            "Forests 1806 2438 7397 1550 2685 2057 17933\n",
            "Green urban areas 990 722 1840 1342 1243 1401 7538\n",
            "High Density Urban\n",
            "Fabric967 996 8975 6993 2533 3103 23567\n",
            "Industrial, commer-\n",
            "cial, public, military\n",
            "and pr1887 2116 4761 1850 3203 2334 16151\n",
            "Low Density Urban\n",
            "Fabric1424 1520 2144 575 2794 3689 12146\n",
            "Medium Density Ur-\n",
            "ban Fabric2144 1128 6124 1661 1833 2100 14990\n",
            "Sports and leisure fa-\n",
            "cilities750 1185 2268 1305 1397 1336 8241\n",
            "Water bodies 537 408 1919 807 805 619 5095\n",
            "city total 15234 13952 43262 18432 21279 20824 132983(b)25km25kmground truth test grids (fractions of city total)\n",
            "class / city athina barcelona berlin budapest madrid roma\n",
            "Agricultural + Semi-\n",
            "natural areas + Wet-\n",
            "lands0.350 0.261 0.106 0.181 0.395 0.473\n",
            "Airports 0.003 0.030 0.013 0.000 0.044 0.006\n",
            "Forests 0.031 0.192 0.087 0.211 0.013 0.019\n",
            "Green urban areas 0.038 0.030 0.072 0.027 0.125 0.054\n",
            "High Density Urban\n",
            "Fabric0.389 0.217 0.284 0.365 0.170 0.215\n",
            "Industrial, commer-\n",
            "cial, public, military\n",
            "and pr0.109 0.160 0.190 0.096 0.138 0.129\n",
            "Low Density Urban\n",
            "Fabric0.016 0.044 0.012 0.006 0.036 0.029\n",
            "Medium Density Ur-\n",
            "ban Fabric0.041 0.025 0.129 0.045 0.042 0.047\n",
            "Sports and leisure fa-\n",
            "cilities0.017 0.034 0.080 0.025 0.036 0.025\n",
            "Water bodies 0.005 0.006 0.026 0.044 <0.001 0.004\n",
            "Figure 5: Barcelona: ground truth ( top) and predicted probabilities ( bo/t_tom ). Table 2: /T_he VGG16 architecture [19]. Block 1 Block 2 Block 3 Block 4 Block 5 Block 6\n",
            "Conv(3,64)\n",
            "Conv(3,64)\n",
            "Max-\n",
            "Pool(2,2)Conv(3,128)\n",
            "Conv(3,128)\n",
            "Max-\n",
            "Pool(2,2)Conv(3,256)\n",
            "Conv(3,256)\n",
            "Conv(3,256)\n",
            "Max-\n",
            "Pool(2,2)Conv(3,512)\n",
            "Conv(3,512)\n",
            "Conv(3,512)\n",
            "Max-\n",
            "Pool(2,2)Conv(3,512)\n",
            "Conv(3,512)\n",
            "Conv(3,512)\n",
            "Max-\n",
            "Pool(2,2)FC(4096)\n",
            "FC(4096)\n",
            "FC(Nclasses )\n",
            "So/f_tMax\n",
            "Table 3: /T_he ResNet-50 architecture [7]. Block 1 Block 2 Block 3 Block 4 Block 5 Block 6\n",
            "Conv(7,64)\n",
            "Max-\n",
            "Pool(3,2)3x[Conv(1,64)\n",
            "Conv(3,64)\n",
            "Conv(3,256)]4x[Conv(1,128)\n",
            "Conv(3,128)\n",
            "Conv(1,512)]6x[Conv(1,256)\n",
            "Conv(3,256)\n",
            "Conv(1,1024)]3x[Conv(1,512)\n",
            "Conv(3,512)\n",
            "Conv(1,2048)]FC(Nclasses )\n",
            "So/f_tMax\n",
            "5.1 Classi/f_ication results\n",
            "We performed experiments training the two architectures described\n",
            "in Section 4 on datasets for each of the 6 cities considered, and for\n",
            "a combined dataset ( all) of all the cities. /T_he diagonal in Figure 6\n",
            "summarizes the (validation set) classi/f_ication performance for each\n",
            "model. All /f_igures are averages computed over balanced subsets\n",
            "of 2000 samples each. While accuracies or 0.700.80 may not\n",
            "look as impressive as those obtained by convolutional architectures\n",
            "on well-studied benchmarks and other classi/f_ication tasks (e.g.,natural images from ImageNet or small aerial patches from Deep-\n",
            "Sat), this only a/t_tests to the diculty of the task of understanding\n",
            "high-level, subjective concepts of urban planning in complex ur-\n",
            "ban environments. First, satellite imagery typically contains much\n",
            "more semantic variation than natural images (as also noted, e.g.,\n",
            "in [2,13]), i.e., there is no central concept that the image is of\n",
            "(unlike the image of a cat or a /f_lower). Second, the type of labels we\n",
            "use for supervision are higher-level concepts (such as low density\n",
            "urban fabric, or sports and leisure facilities), which are much\n",
            "less speci/f_ic than more physical land features e.g., buildings or\n",
            "trees (which are classes used in the DeepSat dataset). Moreover,\n",
            "top-down imagery poses speci/f_ic challenges to convolutional archi-\n",
            "tectures, as these models are inherently not rotationally-symmetric. Urban environments, especially from from a top-down point of\n",
            "view, come in many complex layouts, for which rotations are ir-\n",
            "relevant. Nevertheless, these results are encouraging, especially\n",
            "since this is a harder problem by focusing on wider-area images and\n",
            "on higher-level, subjective concepts used in urban planning rather\n",
            "than on the standard, lower-level physical features such as in [ 1] or\n",
            "[17]. /T_his suggests that such models may be useful feature extrac-\n",
            "tors. Moreover, as more researchers tackle problems with the aid of\n",
            "satellite imagery (which is still a relatively under-researched source\n",
            "of data in the machine learning community), more open-sourceFigure 6: Transferability (classi/f_ication accuracy) of models\n",
            "learned at one location and applied at another. Training on\n",
            "a more diverse set of cities ( all) yields encouraging results\n",
            "compared with just pairwise training/testing. datasets (like this one) are released, performance will certainly im-\n",
            "prove. For the remainder of this section we report results using\n",
            "the ResNet-50 architecture [ 7], as it consistently yielded (if only\n",
            "slightly) be/t_ter classi/f_ication results in our experiments than the\n",
            "VGG-16 architecture. Transfer learning and classi/f_ication performance. Next,\n",
            "we investigated how models trained in one se/t_ting (city or set of\n",
            "cities) perform when applied to other geographical locations. Figure\n",
            "6 summarizes these experiments. In general, performance is poor\n",
            "when training on samples from a given city and testing on samples\n",
            "from a dierent city (the o-diagonal terms). /T_his is expected, as\n",
            "these environments can be very dierent in appearance for cities as\n",
            "dierent as e.g., Budapest and Barcelona. However, we notice that\n",
            "a more diverse set ( all) yields be/t_ter performance when applied at\n",
            "dierent locations than models trained on individual cities. /T_his is\n",
            "encouraging for our purpose of analyzing the high level similarity\n",
            "of urban neighborhoods via satellite imagery. We next looked at per-class model performance to understand\n",
            "what types of environments are harder for the model to distin-\n",
            "guish. Figure 7 shows such an example analysis for three example\n",
            "cities, of which a pair is similar according to Figure 6 (Rome and\n",
            "Barcelona), and another dissimilar (Rome and Budapest). /T_he le/f_t\n",
            "panel shows model performance when training on samples from\n",
            "Barcelona, and predicting on test samples from Barcelona (intra-\n",
            "city). /T_he middle panel shows training on Rome, and predicting\n",
            "on test samples in Barcelona, which can be assumed to be sim-\n",
            "ilar to Rome from a cultural and architectural standpoint (both\n",
            "Latin cities in warm climates). /T_he right /f_igure shows training on\n",
            "Barcelona, and predicting on test samples in Budapest, which can\n",
            "be assumed a rather dierent city from a cultural and architectural\n",
            "standpoint. For all cases, the classes that the model most struggles\n",
            "with are High Density Urban Fabric, Low Density Urban Fabric,\n",
            "and Medium Density Urban Fabric. Considerable overlap can be\n",
            "noticed between these classes - which is not surprising given the\n",
            "highly subjective nature of these concepts. Other examples where\n",
            "the model performance is lower is forests and low-density urban\n",
            "areas being sometimes misclassifed as green urban areas, which,again, is not surprising. /T_his is especially apparent in the cross-city\n",
            "case, where the model struggles with telling apart these classes. For\n",
            "both the case of training and testing on dierent cities (Budapest\n",
            "and Barcelona) and on similar cities (Rome and Barcelona), we\n",
            "note that airports and forests are relatively easier to distinguish. However, more subjective, high-level urban-planning concepts such\n",
            "as high density urban fabric are harder to infer (and more easily\n",
            "confused with medium density or low density urban fabric) in\n",
            "the case of more similar cities (Rome and Barcelona) rather than\n",
            "dissimilar cities (Budapest and Barcelona). Urban environments\n",
            "containing sports and leisure facilities and green areas are under\n",
            "this view more similar between Rome and Barcelona than they are\n",
            "between Budapest and Barcelona. Choosing the spatial scale: sensitivity analysis. So far, we\n",
            "have presented results assuming that tiles of 250 mis an appropriate\n",
            "spatial scale for this analysis. Our intuition suggested that tiles of\n",
            "this size have enough variation and information to be recognized\n",
            "(even by humans) as belonging to one of the high-level concepts\n",
            "of land use classes that we study in this paper. However, one\n",
            "can /f_ind arguments in favor of smaller tile sizes, e.g., in many\n",
            "cities the size of a typical city block is 100 m. /T_hus, we trained\n",
            "models at dierent spatial scales and computed test-set accuracy\n",
            "values for three example cities, Barcelona, Roma, and Budapest\n",
            "- see Figure 8. It is apparent that, for all example cities, smaller\n",
            "spatial scales (50 m, 100m, 150m) that we analyzed yield poorer\n",
            "performance than the scale we chose for the analysis in this paper\n",
            "(250m). /T_his is likely because images at smaller scales do not capture\n",
            "enough variation in urban form (number and type of buildings,\n",
            "relative amount of vegetation, roads etc.) to allow for discriminating\n",
            "between concepts that are fairly high-level. /T_his is in contrast with a\n",
            "benchmark such as DeepSat [ 1] that focuses on lower-level, physical\n",
            "concepts (trees, buildings, etc.). /T_here, a good spatial scale is\n",
            "by necessity smaller (28 mfor DeepSat), as variation in appearance\n",
            "and compositional elements is unwanted. 5.2 Comparing urban environments\n",
            "Finally, we set to understand, at least on an initial qualitative level,\n",
            "how similar urban environments are to one another, across formal\n",
            "land use classes and geographies. Our /f_irst experiment was to\n",
            "project sample images for each class and city in this analysis to\n",
            "lower-dimensional manifolds, using the t-SNE algorithm [ 23]. /T_his\n",
            "serves the purpose of both visualization (as t-SNE is widely used\n",
            "for visualizing high-dimensional data), as well as for providing an\n",
            "initial, coarse continuous representation of urban land use classes. In our experiments, we used balanced samples of size N=6000, or\n",
            "100 samples for each of the 10 classes for each city. We extracted\n",
            "features for each of these samples using the allmodels (trained\n",
            "on a train set with samples across all cities except for the test one). Figure 9 visualizes such t-SNE embeddings for the six cities in\n",
            "our analysis. For most cities, classes such as low density urban\n",
            "fabric, forests, and water bodies are well-resolved, while sports\n",
            "and leisure facilities seem to consistently blend into other types of\n",
            "environments (which is not surprising, given that these types of\n",
            "facilities can be found within many types of locations that have a\n",
            "dierent formal urban planning class assigned). Intriguing dier-\n",
            "ences emerge in this picture among the cities. For example, greenFigure 7: Example classi/f_ication confusion matrix for land use inference. Le/f_t: training and testing on Barcelona; Middle :\n",
            "training on Rome, testing on Barcelona; Right: training on Rome, predicting on Budapest. Figure 8: Sensitivity of training patch size vs test accuracy. Figure 9: t-SNE visualization (the /f_irst 2 dimensions) of ur-\n",
            "ban environments (satellite image samples) across six cities. urban spaces seem fairly well resolved for most cities. Commercial\n",
            "neighborhoods in Barcelona seem more integrated with the other\n",
            "types of environments in the city, whereas for Berlin they appear\n",
            "more distinct. Urban water bodies are more embedded with urban\n",
            "parks for Barcelona than for other cities. Such reasoning (with\n",
            "more rigorous quantitative analysis) can serve as coarse way to\n",
            "benchmark and compare neighborhoods as input to further analysis\n",
            "about e.g., energy use, livelihood, or trac in urban environments. Figure 10: Comparing urban environments across cities\n",
            "(with reference to Barcelona) We show relative inter-city\n",
            "similarity measures computed as the sum of squares across\n",
            "the clusters in Figure 9. We further illustrate how similar the six cities we used through-\n",
            "out this analysis are starting o the embeddings plots in Figure 9. For each land use class, we compute intra-city sum of squares in\n",
            "the 2-d t-SNE embedding, and display the results in Figure 10. Note\n",
            "that the distances are always shown with Barcelona as a reference\n",
            "point (chosen arbitrarily). For each panel, the normalization is with\n",
            "respect to the largest inter-city distance for that land use class. /T_his\n",
            "visualization aids quick understanding of similarity between urban\n",
            "environments. For example, agricultural lands in Barcelona are\n",
            "most dissimilar to those in Budapest. Airports in Barcelona are\n",
            "most similar to those in Athens, and most dissimilar to those in\n",
            "Berlin and Budapest. Barcelonas forests and parks are most dissim-\n",
            "ilar to Budapests. Water bodies in Barcelona are very dissimilar to\n",
            "all other cities. /T_his point is enforced by Figure 11 below, which\n",
            "suggests that areas marked as water bodies in Barcelona are ocean\n",
            "waterfronts, whereas this class for all other cities represents rivers\n",
            "or lakes.Figure 11: Samples from three urban environments across\n",
            "our 6 example cities. We sampled the 2-d t-SNE embedding\n",
            "of Figure 9 and queried for the closest real sample to the\n",
            "centroid using an ecient KD-tree search. Finally, we explore the feature maps extracted by the convolu-\n",
            "tional model in order to illustrate how similar the six cities we\n",
            "used throughout this analysis are across three example environ-\n",
            "ments, green urban areas, water bodies, and medium density urban\n",
            "fabric. For each city and land use class, we start o the centroid of\n",
            "the point cloud in the 2-d space of Figure 9, and /f_ind the nearest\n",
            "several samples using the KD-tree method described in Section 4. We present the results in Figure 11. Visual inspection indicates\n",
            "that the model has learned useful feature maps about urban envi-\n",
            "ronments: the sample image patches show a very good qualitative\n",
            "agreement with the region of the space where theyre sampled from,\n",
            "indicated by the land use class of neighboring points. /Q_ualitatively,\n",
            "it is clear that the features extracted from the top layer of the con-\n",
            "volutional model allow a comparison between urban environments\n",
            "by high-level concepts used in urban planning. 6 CONCLUSIONS\n",
            "/T_his paper has investigated the use of convolutional neural net-\n",
            "works for analyzing urban environments through satellite imagery\n",
            "at the scale of entire cities. Given the current relative dearth of\n",
            "labeled satellite imagery in the machine learning community, we\n",
            "have constructed an open dataset of over 140 ,000 samples over 10\n",
            "consistent land use classes from 6 cities in Europe. As we continue\n",
            "to improve, curate, and expand this dataset, we hope that it can help\n",
            "other researchers in machine learning, smart cities, urban planning,\n",
            "and related /f_ields in their work on understanding cities. We set out to study similarity and variability across urban envi-\n",
            "ronments, as being able to quantify such pa/t_terns will enable richer\n",
            "applications in topics such as urban energy analysis, infrastructure\n",
            "benchmarking, and socio-economic composition of communities. We formulated this as a two-step task: /f_irst predicting urban land\n",
            "use classes from satellite imagery, then turning this (rigid) clas-\n",
            "si/f_ication into a continuous spectrum by embedding the features\n",
            "extracted from the convolutional classi/f_ier into a lower-dimensional\n",
            "manifold. We show that the classi/f_ication task achieves encour-\n",
            "aging results, given the large variety in physical appearance of\n",
            "urban environments having the same functional class. Moreover,we exemplify how the features extracted from the convolutional\n",
            "network allow for identifying neighbors of any given query im-\n",
            "age, allowing a rich comparison analysis of urban environments by\n",
            "their visual composition. /T_he analysis in this paper shows that some types urban envi-\n",
            "ronments are easier to infer than others, both in the intra- and\n",
            "inter-city cases. For example, in all our experiments, the models\n",
            "had most trouble telling apart high, medium, and low den-\n",
            "sity urban environments, a/t_testing to the subjectivity of such a\n",
            "high-level classi/f_ication for urban planning purposes. However,\n",
            "agricultural lands, forests, and airports tend to be visually similar\n",
            "across dierent cities - and the amount of relative dissimilarity can\n",
            "be quanti/f_ied using the methods in this paper. Green urban areas\n",
            "(parks) are generally similar to forests or to leisure facilities, and\n",
            "the models do be/t_ter in the intra-city case than predicting across\n",
            "cities. How industrial areas look is again less geography-speci/f_ic:\n",
            "inter-city similarity is consistently larger than intra-city similarity. As such, for several classes we can expect learning to transfer from\n",
            "one geography to another. /T_hus, while it is not news that some\n",
            "cities are more similar than others (Barcelona is visually closer to\n",
            "Athens than it is to Berlin), the methodology in this paper allows\n",
            "for a more quantitative and practical comparison of similarity. By leveraging satellite data (available virtually world-wide), this\n",
            "approach may allow for a low-cost way to analyze urban envi-\n",
            "ronments in locations where ground truth information on urban\n",
            "planning is not available. As future directions of this work, we\n",
            "plan to i)continue to develop more rigorous ways to compare and\n",
            "benchmark urban neighborhoods, going deeper to physical ele-\n",
            "ments (vegetation, buildings, roads etc. ); ii)improve and further\n",
            "curate the open Urban Environments dataset; and iii)extend this\n",
            "type of analysis to more cities across other geographical locations. A PRACTICAL TRAINING DETAILS. We split our training data into a training set (80% of the data) and a\n",
            "validation set (the remaining 20%). /T_his is separate from the data\n",
            "sampled for the ground truth raster grids for each city, which we\n",
            "only used at test time. We implemented the architectures in the\n",
            "open-source deep learning framework Keras7(with a TensorFlow8\n",
            "backend). In all our experiments, we used popular data augmenta-\n",
            "tion techniques, including random horizontal and vertical /f_lipping\n",
            "of the input images, random shearing (up to 0 .1 radians), random\n",
            "scaling (up to 120%), random rotations (by at most 15 degrees either\n",
            "direction). Input images were 224 2243 pixels in size (RGB\n",
            "bands). For all experiments, we used stochastic gradient descent\n",
            "(with its Adadelta variant) to optimize the network loss function (a\n",
            "standard multi-class cross-entropy), starting with a learning rate of\n",
            "0.1, and halving the rate each 10 epochs. We trained our networks\n",
            "for at most 100 epochs, with 2000 samples in each epoch, stopping\n",
            "the learning process when the accuracy on the validation set did\n",
            "not improve for more than 10 epochs. Given the inherent imbalance\n",
            "of the classes, we explicitly enforced that the minibatches used for\n",
            "training were relatively balanced by a weighted sampling proce-\n",
            "dure. For training, we used a cluster of 4 NVIDIA K80 GPUs, and\n",
            "tested our models on a cluster of 48 CPUs. 7\n",
            "8www.tensor/f_low.org \n"
          ]
        }
      ],
      "source": [
        "# Evaluation of section summaries without LSA applied and the reference summary has been taken from ChatGPT\n",
        "\n",
        "from nltk.translate import meteor_score\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Define reference summaries for each section\n",
        "reference_summaries = summary_dict\n",
        "\n",
        "def calculate_scores_for_section(reference, hypothesis):\n",
        "    reference_tokens = word_tokenize(reference)\n",
        "    hypothesis_tokens = word_tokenize(hypothesis)\n",
        "\n",
        "    smoothing_function = SmoothingFunction().method1  \n",
        "    bleu_score = sentence_bleu([reference_tokens], hypothesis_tokens, smoothing_function=smoothing_function)\n",
        "\n",
        "    meteor_score_value = meteor_score.meteor_score([reference_tokens], hypothesis_tokens)\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge_scores = scorer.score(reference, hypothesis)\n",
        "    rouge1 = rouge_scores['rouge1'].fmeasure\n",
        "    rouge2 = rouge_scores['rouge2'].fmeasure\n",
        "    rougel = rouge_scores['rougeL'].fmeasure\n",
        "\n",
        "    return bleu_score, meteor_score_value, rouge1, rouge2, rougel\n",
        "\n",
        "# Initialize generated summary with the content of each section in top-ranked sentences\n",
        "generated_summary = \"\"\n",
        "\n",
        "# Calculate scores for each section using top-ranked sentences\n",
        "bleu_scores_top_ranked = {}\n",
        "meteor_scores_top_ranked = {}\n",
        "rouge1_scores_top_ranked = {}\n",
        "rouge2_scores_top_ranked = {}\n",
        "rougel_scores_top_ranked = {}\n",
        "\n",
        "for section, top_sentences in section_sentences.items():\n",
        "    section_text = ' '.join(top_sentences)\n",
        "    generated_summary += section_text + \" \"  # Include the content of each section\n",
        "\n",
        "    # Use the corresponding reference summary for each section\n",
        "    reference_summary_for_section = reference_summaries[section]\n",
        "\n",
        "    bleu_score, meteor_score_val, rouge1, rouge2, rougel = calculate_scores_for_section(reference_summary_for_section, section_text)\n",
        "\n",
        "    bleu_scores_top_ranked[section] = bleu_score\n",
        "    meteor_scores_top_ranked[section] = meteor_score_val\n",
        "    rouge1_scores_top_ranked[section] = rouge1\n",
        "    rouge2_scores_top_ranked[section] = rouge2\n",
        "    rougel_scores_top_ranked[section] = rougel\n",
        "\n",
        "# Display scores for each section using top-ranked sentences\n",
        "for section in section_sentences.keys():\n",
        "    print(f\"Scores for Section '{section}' (Sections have the entire text in the generated summary):\")\n",
        "    print(\"BLEU Score:\", bleu_scores_top_ranked[section])\n",
        "    print(\"METEOR Score:\", meteor_scores_top_ranked[section])\n",
        "    print(\"ROUGE-1 F-measure:\", rouge1_scores_top_ranked[section])\n",
        "    print(\"ROUGE-2 F-measure:\", rouge2_scores_top_ranked[section])\n",
        "    print(\"ROUGE-L F-measure:\", rougel_scores_top_ranked[section])\n",
        "    print()\n",
        "\n",
        "# Display the generated summary\n",
        "print(\"\\nGenerated Summary with Content of Top-Ranked Sentences:\")\n",
        "print(generated_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------+-------+--------+---------+---------+---------+\n",
            "|         Section with LSA         |  BLEU | METEOR | ROUGE-1 | ROUGE-2 | ROUGE-L |\n",
            "+----------------------------------+-------+--------+---------+---------+---------+\n",
            "|             ABSTRACT             | 0.099 | 0.517  |  0.446  |  0.246  |  0.384  |\n",
            "|          1 INTRODUCTION          | 0.045 | 0.366  |  0.253  |  0.117  |  0.182  |\n",
            "|           2 LITERATURE           | 0.019 | 0.267  |  0.152  |  0.068  |  0.099  |\n",
            "| 3 THE URBAN ENVIRONMENTS DATASET | 0.018 | 0.288  |  0.180  |  0.087  |  0.117  |\n",
            "|       4 EXPERIMENTAL SETUP       | 0.030 | 0.350  |  0.209  |  0.093  |  0.150  |\n",
            "|     5 RESULTS AND DISCUSSION     | 0.008 | 0.176  |  0.076  |  0.032  |  0.048  |\n",
            "|          6 CONCLUSIONS           | 0.001 | 0.138  |  0.065  |  0.012  |  0.048  |\n",
            "+----------------------------------+-------+--------+---------+---------+---------+\n"
          ]
        }
      ],
      "source": [
        "# Combine the scores from all dictionaries\n",
        "combined_scores = {}\n",
        "for section in bleu_scores_top_ranked.keys():\n",
        "    combined_scores[section] = {\n",
        "        \"BLEU\": bleu_scores_top_ranked.get(section, 0),\n",
        "        \"METEOR\": meteor_scores_top_ranked.get(section, 0),\n",
        "        \"ROUGE-1\": rouge1_scores_top_ranked.get(section, 0),\n",
        "        \"ROUGE-2\": rouge2_scores_top_ranked.get(section, 0),\n",
        "        \"ROUGE-L\": rougel_scores_top_ranked.get(section, 0)\n",
        "    }\n",
        "\n",
        "# Create a PrettyTable instance\n",
        "table = PrettyTable()\n",
        "table.field_names = [\"Section with LSA\", \"BLEU\", \"METEOR\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"]\n",
        "\n",
        "# Populate the table with scores\n",
        "for section, section_scores in combined_scores.items():\n",
        "    bleu = \"{:.3f}\".format(section_scores[\"BLEU\"])\n",
        "    meteor = \"{:.3f}\".format(section_scores[\"METEOR\"])\n",
        "    rouge_1 = \"{:.3f}\".format(section_scores[\"ROUGE-1\"])\n",
        "    rouge_2 = \"{:.3f}\".format(section_scores[\"ROUGE-2\"])\n",
        "    rouge_l = \"{:.3f}\".format(section_scores[\"ROUGE-L\"])\n",
        "    \n",
        "    table.add_row([section, bleu, meteor, rouge_1, rouge_2, rouge_l])\n",
        "\n",
        "# Print the table\n",
        "print(table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4P6TKn96apD",
        "outputId": "075b11a0-a010-44e1-af76-3d77f656af62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scores for Section 'ABSTRACT' (LSA has been applied on the generated summary):\n",
            "BLEU Score: 0.14988785736534846\n",
            "METEOR Score: 0.49273633998265387\n",
            "ROUGE-1 F-measure: 0.47547169811320755\n",
            "ROUGE-2 F-measure: 0.25095057034220536\n",
            "ROUGE-L F-measure: 0.3245283018867925\n",
            "\n",
            "Scores for Section '1 INTRODUCTION' (LSA has been applied on the generated summary):\n",
            "BLEU Score: 0.0642875450292066\n",
            "METEOR Score: 0.25847931019665704\n",
            "ROUGE-1 F-measure: 0.36065573770491804\n",
            "ROUGE-2 F-measure: 0.1188118811881188\n",
            "ROUGE-L F-measure: 0.17049180327868857\n",
            "\n",
            "Scores for Section '2 LITERATURE' (LSA has been applied on the generated summary):\n",
            "BLEU Score: 0.011628239299252495\n",
            "METEOR Score: 0.3011450499559893\n",
            "ROUGE-1 F-measure: 0.4\n",
            "ROUGE-2 F-measure: 0.08450704225352114\n",
            "ROUGE-L F-measure: 0.1581395348837209\n",
            "\n",
            "Scores for Section '3 THE URBAN ENVIRONMENTS DATASET' (LSA has been applied on the generated summary):\n",
            "BLEU Score: 0.01779787222800846\n",
            "METEOR Score: 0.30193331508307386\n",
            "ROUGE-1 F-measure: 0.3986486486486487\n",
            "ROUGE-2 F-measure: 0.09523809523809523\n",
            "ROUGE-L F-measure: 0.18918918918918917\n",
            "\n",
            "Scores for Section '4 EXPERIMENTAL SETUP' (LSA has been applied on the generated summary):\n",
            "BLEU Score: 0.1389536265384305\n",
            "METEOR Score: 0.3970065332298512\n",
            "ROUGE-1 F-measure: 0.5\n",
            "ROUGE-2 F-measure: 0.23423423423423423\n",
            "ROUGE-L F-measure: 0.33035714285714285\n",
            "\n",
            "Scores for Section '5 RESULTS AND DISCUSSION' (LSA has been applied on the generated summary):\n",
            "BLEU Score: 0.049320034780675155\n",
            "METEOR Score: 0.1849566700065039\n",
            "ROUGE-1 F-measure: 0.2953586497890296\n",
            "ROUGE-2 F-measure: 0.09361702127659574\n",
            "ROUGE-L F-measure: 0.19409282700421943\n",
            "\n",
            "Scores for Section '6 CONCLUSIONS' (LSA has been applied on the generated summary):\n",
            "BLEU Score: 0.0048369135701678626\n",
            "METEOR Score: 0.22302978945309954\n",
            "ROUGE-1 F-measure: 0.15384615384615385\n",
            "ROUGE-2 F-measure: 0.010362694300518135\n",
            "ROUGE-L F-measure: 0.10256410256410257\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluation of LSA applied summaries with reference summary of ChatGPT\n",
        "\n",
        "# Define reference summaries for each section\n",
        "reference_summaries = summary_dict\n",
        "\n",
        "# Function to calculate scores for a section\n",
        "def calculate_scores_for_section(reference, hypothesis):\n",
        "    reference_tokens = word_tokenize(reference)\n",
        "    hypothesis_tokens = word_tokenize(hypothesis)\n",
        "\n",
        "    smoothing_function = SmoothingFunction().method1  # Define smoothing function\n",
        "    bleu_score = sentence_bleu([reference_tokens], hypothesis_tokens, smoothing_function=smoothing_function)\n",
        "    meteor_score_val = meteor_score.meteor_score([reference_tokens], hypothesis_tokens)\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge_scores = scorer.score(reference, hypothesis)\n",
        "    rouge1 = rouge_scores['rouge1'].fmeasure\n",
        "    rouge2 = rouge_scores['rouge2'].fmeasure\n",
        "    rougel = rouge_scores['rougeL'].fmeasure\n",
        "\n",
        "    return bleu_score, meteor_score_val, rouge1, rouge2, rougel\n",
        "\n",
        "# Example text 2\n",
        "generated_summary = top_ranked_sentences  # Use top-ranked sentences directly\n",
        "\n",
        "# Calculate scores for each section using top-ranked sentences\n",
        "scores_top_ranked = {}\n",
        "\n",
        "for section, sentences in generated_summary.items():\n",
        "    section_text = ' '.join(sentences)\n",
        "\n",
        "    # Use the corresponding reference summary for each section\n",
        "    reference_summary_for_section = reference_summaries[section]\n",
        "\n",
        "    bleu_score, meteor_score_val, rouge1, rouge2, rougel = calculate_scores_for_section(reference_summary_for_section, section_text)\n",
        "\n",
        "    scores_top_ranked[section] = {\n",
        "        'BLEU Score': bleu_score,\n",
        "        'METEOR Score': meteor_score_val,\n",
        "        'ROUGE-1 F-measure': rouge1,\n",
        "        'ROUGE-2 F-measure': rouge2,\n",
        "        'ROUGE-L F-measure': rougel\n",
        "    }\n",
        "\n",
        "# Display scores for each section using top-ranked sentences\n",
        "for section, scores in scores_top_ranked.items():\n",
        "    print(f\"Scores for Section '{section}' (LSA has been applied on the generated summary):\")\n",
        "    for metric, value in scores.items():\n",
        "        print(f\"{metric}: {value}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------+-------+--------+---------+---------+---------+\n",
            "|  Section with LSA applied text   |  BLEU | METEOR | ROUGE-1 | ROUGE-2 | ROUGE-L |\n",
            "+----------------------------------+-------+--------+---------+---------+---------+\n",
            "|             ABSTRACT             | 0.150 | 0.493  |  0.475  |  0.251  |  0.325  |\n",
            "|          1 INTRODUCTION          | 0.064 | 0.258  |  0.361  |  0.119  |  0.170  |\n",
            "|           2 LITERATURE           | 0.012 | 0.301  |  0.400  |  0.085  |  0.158  |\n",
            "| 3 THE URBAN ENVIRONMENTS DATASET | 0.018 | 0.302  |  0.399  |  0.095  |  0.189  |\n",
            "|       4 EXPERIMENTAL SETUP       | 0.139 | 0.397  |  0.500  |  0.234  |  0.330  |\n",
            "|     5 RESULTS AND DISCUSSION     | 0.049 | 0.185  |  0.295  |  0.094  |  0.194  |\n",
            "|          6 CONCLUSIONS           | 0.005 | 0.223  |  0.154  |  0.010  |  0.103  |\n",
            "+----------------------------------+-------+--------+---------+---------+---------+\n"
          ]
        }
      ],
      "source": [
        "# Create a PrettyTable instance\n",
        "evaluation_table = PrettyTable()\n",
        "evaluation_table.field_names = [\"Section with LSA applied text\", \"BLEU\", \"METEOR\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"]\n",
        "\n",
        "# Populate the table with scores\n",
        "for section, section_scores in scores_top_ranked.items():\n",
        "    bleu = \"{:.3f}\".format(section_scores[\"BLEU Score\"])\n",
        "    meteor = \"{:.3f}\".format(section_scores[\"METEOR Score\"])\n",
        "    rouge_1 = \"{:.3f}\".format(section_scores[\"ROUGE-1 F-measure\"])\n",
        "    rouge_2 = \"{:.3f}\".format(section_scores[\"ROUGE-2 F-measure\"])\n",
        "    rouge_l = \"{:.3f}\".format(section_scores[\"ROUGE-L F-measure\"])\n",
        "    \n",
        "    evaluation_table.add_row([section, bleu, meteor, rouge_1, rouge_2, rouge_l])\n",
        "\n",
        "# Print the table\n",
        "print(evaluation_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fia88zb2Qk2K"
      },
      "source": [
        "##### GLOVE Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RenomK72SFRn"
      },
      "source": [
        "### Similarity scores using GLOVE embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RN3bmYub-_wr",
        "outputId": "75161351-e8e9-478d-a172-3cc0740144ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity score with Section with entire text and Glove 'ABSTRACT': 0.8967602252960205\n",
            "Similarity score with Section with entire text and Glove '1 INTRODUCTION': 0.8886377215385437\n",
            "Similarity score with Section with entire text and Glove '2 LITERATURE': 0.8910382390022278\n",
            "Similarity score with Section with entire text and Glove '3 THE URBAN ENVIRONMENTS DATASET': 0.8619346618652344\n",
            "Similarity score with Section with entire text and Glove '4 EXPERIMENTAL SETUP': 0.8776224851608276\n",
            "Similarity score with Section with entire text and Glove '5 RESULTS AND DISCUSSION': 0.8686850070953369\n",
            "Similarity score with Section with entire text and Glove '6 CONCLUSIONS': 0.8883303999900818\n"
          ]
        }
      ],
      "source": [
        "# Using Glove for sections with entire text( Query is being matched with the sections but the sections have all the text)\n",
        "\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the pre-trained model with GloVe vectors\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "def text_similarity_with_glove(text1, text2):\n",
        "    # Ensure that the input is a string\n",
        "    text1 = str(text1)\n",
        "    text2 = str(text2)\n",
        "\n",
        "    # Tokenize and lemmatize the texts, applying lowercasing to individual words\n",
        "    tokens1 = [word.lower() for word in sent_tokenize(text1)]\n",
        "    tokens2 = [word.lower() for word in sent_tokenize(text2)]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens1 = [lemmatizer.lemmatize(token) for token in tokens1]\n",
        "    tokens2 = [lemmatizer.lemmatize(token) for token in tokens2]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = stopwords.words('english')\n",
        "    tokens1 = [token for token in tokens1 if token not in stop_words]\n",
        "    tokens2 = [token for token in tokens2 if token not in stop_words]\n",
        "\n",
        "    # Join tokens into strings\n",
        "    text1_processed = ' '.join(tokens1)\n",
        "    text2_processed = ' '.join(tokens2)\n",
        "\n",
        "    # Use spacy to get GloVe vectors for the processed texts\n",
        "    vector1 = nlp(text1_processed).vector\n",
        "    vector2 = nlp(text2_processed).vector\n",
        "\n",
        "    # Calculate the cosine similarity\n",
        "    similarity = cosine_similarity([vector1], [vector2])[0][0]\n",
        "\n",
        "    return similarity\n",
        "\n",
        "# Example usage:\n",
        "text2 = 'Urban environment analysis using satellite imagery and deep learning.'\n",
        "\n",
        "# Calculate similarity with each section header\n",
        "similarity_scores_glove = {}\n",
        "for section in sections:\n",
        "    similarity_score = text_similarity_with_glove(section_extraction[sections.index(section)], text2)\n",
        "    similarity_scores_glove[section] = similarity_score\n",
        "\n",
        "# Display similarity scores\n",
        "for section, score in similarity_scores_glove.items():\n",
        "    print(f\"Similarity score with Section with entire text and Glove '{section}': {score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng9RXHTAsGCb",
        "outputId": "d738335a-fd9d-4e7d-cb09-add276e88673"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity score with Section using LSA and Glove 'ABSTRACT': 0.5112610459327698\n",
            "Similarity score with Section using LSA and Glove '1 INTRODUCTION': 0.16116014122962952\n",
            "Similarity score with Section using LSA and Glove '2 LITERATURE': 0.2086666226387024\n",
            "Similarity score with Section using LSA and Glove '3 THE URBAN ENVIRONMENTS DATASET': 0.7029306292533875\n",
            "Similarity score with Section using LSA and Glove '4 EXPERIMENTAL SETUP': 0.2775696814060211\n",
            "Similarity score with Section using LSA and Glove '5 RESULTS AND DISCUSSION': 0.5496720671653748\n",
            "Similarity score with Section using LSA and Glove '6 CONCLUSIONS': 0.18521957099437714\n"
          ]
        }
      ],
      "source": [
        "# Using Glove with only top ranked sentences(Matching the query to the sections and LSA has been applied)\n",
        "\n",
        "# Load the pre-trained model with GloVe vectors\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "def text_similarity_with_glove(text1, text2):\n",
        "    # Ensure that the input is a string\n",
        "    text1 = str(text1)\n",
        "    text2 = str(text2)\n",
        "\n",
        "    # Tokenize and lemmatize the texts, applying lowercasing to individual words\n",
        "    tokens1 = [word.lower() for word in sent_tokenize(text1)]\n",
        "    tokens2 = [word.lower() for word in sent_tokenize(text2)]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens1 = [lemmatizer.lemmatize(token) for token in tokens1]\n",
        "    tokens2 = [lemmatizer.lemmatize(token) for token in tokens2]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = stopwords.words('english')\n",
        "    tokens1 = [token for token in tokens1 if token not in stop_words]\n",
        "    tokens2 = [token for token in tokens2 if token not in stop_words]\n",
        "\n",
        "    # Join tokens into strings\n",
        "    text1_processed = ' '.join(tokens1)\n",
        "    text2_processed = ' '.join(tokens2)\n",
        "\n",
        "    # Use spacy to get GloVe vectors for the processed texts\n",
        "    vector1 = nlp(text1_processed).vector\n",
        "    vector2 = nlp(text2_processed).vector\n",
        "\n",
        "    # Calculate the cosine similarity\n",
        "    similarity = cosine_similarity([vector1], [vector2])[0][0]\n",
        "\n",
        "    return similarity\n",
        "\n",
        "# Example usage:\n",
        "text2 = 'Urban environment analysis using satellite imagery and deep learning.'\n",
        "\n",
        "# Calculate similarity with each section header\n",
        "similarity_scores_glove = {}\n",
        "for section, top_sentence in zip(sections, top_ranked_sentences):\n",
        "    similarity_score = text_similarity_with_glove(top_sentence, text2)\n",
        "    similarity_scores_glove[section] = similarity_score\n",
        "\n",
        "# Display similarity scores\n",
        "for section, score in similarity_scores_glove.items():\n",
        "    print(f\"Similarity score with Section using LSA and Glove '{section}': {score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRadm7dqQrNa"
      },
      "source": [
        "### Measuring Similarity Scores of user query with Title of pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8GPC2WcQwiY"
      },
      "source": [
        "##### GLOVE embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLMrbHNkAlfC",
        "outputId": "a18aabb1-e168-4c2f-d90a-7759707d4510"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity score with Title 'using convolutional networks satellite imagery identify pa/t_terns urban environments large scale' using GloVe: 0.8312693157209148\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy model with GloVe vectors\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Your text2\n",
        "text2 = 'Urban environment analysis using satellite imagery and deep learning.'\n",
        "\n",
        "# Function to calculate similarity score between title and text2 using GloVe vectors\n",
        "def glove_similarity(title, text2):\n",
        "    # Ensure that the input is a string\n",
        "    title = str(title)\n",
        "    text2 = str(text2)\n",
        "\n",
        "    # Process the texts using spaCy\n",
        "    doc_title = nlp(title)\n",
        "    doc_text2 = nlp(text2)\n",
        "\n",
        "    # Calculate the similarity between doc_title and doc_text2\n",
        "    similarity = doc_title.similarity(doc_text2)\n",
        "\n",
        "    return similarity\n",
        "\n",
        "# Calculate similarity between clean_title and text2\n",
        "similarity_score = glove_similarity(clean_title, text2)\n",
        "\n",
        "# Display similarity score\n",
        "print(f\"Similarity score with Title '{clean_title}' using GloVe: {similarity_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Asu7TXm9QzUZ"
      },
      "source": [
        "##### TF-IDF embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kg0RcgxmE3Q7",
        "outputId": "ef26af91-32c8-4d7d-c3fc-27c0bc49d44d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity score with Title 'using convolutional networks satellite imagery identify pa/t_terns urban environments large scale' using TF-IDF: 0.5773502691896258\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import spacy\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Your text2\n",
        "text2 = 'Urban environment analysis using satellite imagery and deep learning.'\n",
        "\n",
        "# Function to calculate similarity score between title and text2 using TF-IDF\n",
        "def tfidf_similarity(title, text2):\n",
        "    # Ensure that the input is a string\n",
        "    title = str(title)\n",
        "    text2 = str(text2)\n",
        "\n",
        "    # Process the texts using spaCy\n",
        "    doc_title = nlp(title)\n",
        "    doc_text2 = nlp(text2)\n",
        "\n",
        "    # Create the TF-IDF vectors\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    vector_title = vectorizer.fit_transform([title])\n",
        "    vector_text2 = vectorizer.transform([text2])\n",
        "\n",
        "    # Calculate the cosine similarity\n",
        "    similarity = cosine_similarity(vector_title, vector_text2)[0][0]\n",
        "\n",
        "    return similarity\n",
        "\n",
        "# Calculate similarity between clean_title and text2 using TF-IDF\n",
        "similarity_score_tfidf = tfidf_similarity(clean_title, text2)\n",
        "\n",
        "# Display similarity score\n",
        "print(f\"Similarity score with Title '{clean_title}' using TF-IDF: {similarity_score_tfidf}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uMfO_y-Sxju"
      },
      "source": [
        "### Using BART for Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715,
          "referenced_widgets": [
            "53a2c605aa6c48d9a477640d8ab6a626",
            "9c5baf9354404c5797ea71382dd6dd9e",
            "234432e1ecde44b6896e79f59b131212",
            "e927c13aa3f3435b99246d971b9eb991",
            "e4cae05ffe9c40af91e0355ca9270144",
            "001fbb24fb174287a626b59746db20a8",
            "9c5f66606e914a438b83021c9d92e528",
            "18d0ac2e72834627a029b72014658dff",
            "ba1a3da3d58448329ae51cf22bdec942",
            "7c62e52a054c45138e2367760436292f",
            "b4ade3c36eec40ac8f2e561dd49f782e",
            "13c8700811ac4085b4b4bc0a6e2d1fdc",
            "f88ca7783c0e4cf1b8c48416954be9fa",
            "503ad6c38a36404187e603bb7c19f09a",
            "d24312bd58cd46f8bd67dcad7f7c0abf",
            "78f738debafb41e187df36fb0c0dd95d",
            "cd4dc2877e6b4476933f30a9c85dd505",
            "79f7cf49f34f43e0ba5d34c24e16eb79",
            "f3d7fb2ae6d5443c8ce9fdd86efa9dc5",
            "ab010386bdbc49d390b6c6f38748ecf9",
            "928fd9a7d22343868a3e4d7ac1894b3d",
            "bd184e8978ff4e478dbc55b7913db520",
            "084d950233944c23969c6149d0ba3339",
            "b732b5efa06c40919ec56e32615001dd",
            "3d7504dd6bc84023bf4146d2ef9dd943",
            "f6c970d581f64eb384314aea681e7fab",
            "16fc798327944ed48a61a92abe428cfb",
            "54f7ddaf3bfc4167ad9f79e9532c97d9",
            "33bea5a6f1d8476fbadd9a31c3c19b28",
            "f3c815cec1c746d99ba87b4301b28533",
            "e6d3087d159c47e091c84b68f8445909",
            "7b142f541b674f39943fdb608eb6b609",
            "39b237685f0349c793aa174031b713e5",
            "3dcec7100df8494f86b40ae9a0c9c5be",
            "a240eee2bb2247e28fb277c3a21d6d94",
            "3b66b37c69a34bad88b1883b7c427c5c",
            "eb79129176a04e588a91d6af832dc102",
            "c61875b24178469da8f5d981e40757f6",
            "1925738060e143b59d6fe2fe410180d6",
            "a8410f78c7dd430c9ab5492474dd484e",
            "ca864117c4954faf810cb7dcd46f6ef8",
            "25f6229d98da414b823619208c126971",
            "9e8f07b07ff1446a9870b495e53bc5d5",
            "e3122766de1040108c7ec4d21fa1b4e7",
            "8daabe287199404d8312c4a4a16ab776",
            "8f71244c651c4845b6097977ef57ae7a",
            "7be6b2658dda4252b5736fa498de47c1",
            "0a0838f5f5074777ab8aa36090b3c4b7",
            "bb4dbb1612a34e078da04d9f6bc30499",
            "5f6eabe89e2b4644abdb802cc0586344",
            "b0e4082b87924dbd95ac77971cc1c469",
            "bf993f82b629477eb717c69578269dc9",
            "271215ea98e14dbd87dda26c91c80570",
            "a6955e5406d54c428d924b7ee11e9fa6",
            "9d8408c03a17455a853f8260df5262d8",
            "886bc3ebb2db481b82a47a18e3e665f0",
            "0e71d683940046a98f708eb322b27c17",
            "6f79848242864262ab5327db8e225ca5",
            "8cbbe888ba9149a7b1f63c151d72b221",
            "63a5e3ff7c0841afb02c1adaf939f021",
            "50d2845af39d453cab95402dcd29a320",
            "ae25132809e8489cb0023a85d70b51db",
            "95f08c1da9d541978379c85a79fcf787",
            "5b540e3b6d3f404392c6666206c73fff",
            "5572a83ddb46419288be031db81905a0",
            "553a9003a69b4d109ef45320eb42ade8"
          ]
        },
        "id": "Gr-sn1WkU7s7",
        "outputId": "23c6d972-8b68-4071-801c-21cb1a6d199e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary for ABSTRACT:\n",
            "We analyze pa/t_terns in land use in urban neighborhoods using large-scale satellite imagery data and computer vision techniques based on deep convolutional neuralnetworks. We use this data to train and compare deep architectures which have recently shown good per-formance on standard computer vision tasks (image classi/f_ication and segmentation), including on geospatial data. CCS CONCEPTS: Computer vision; Neural net-agicallyworks; Applied computing Environmental sciences;KEYWORDS: Land use, satellite imagery,convolutional networks, land use class i/f/ication, convolutionian networks. The paper is based on the Urban Atlas land use dataset of 20 land use classes across 300 European cities. For more information, visit the CCS ConcepTS website. The study was published in the open-source journal, The Open Data Project (ODP) (http://www.opendataproject.org/doi/full/10.1177/1556/1555/1/2/2.1).\n",
            "\n",
            "Summary for 1 INTRODUCTION:\n",
            "The methods presented here allow for automated, rapid classi/f_ication of urban environments that can potentially be applied to locations where survey and zoning data is not available. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee. If copies are not made or distributed for pro/f-it or commercial advantage, the notice and the full citation will appear on the /f_irst page. For confidential support call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, see www.samaritans.org for details. In the U.S. call the National Suicide Prevention Lifeline on 1-800-273-8255 or visit http://www.suicidepreventionlifeline.org/. For confidential help in the UK, call 08457 909090 or click here. For support in the Middle East and Africa, call the SAMARitans on 8457 90 9090 or visit www.suicidesprevention.org. For more information on how to help people in the developing world, visit the World Health Organization ( WHO)\n",
            "\n",
            "Summary for 2 LITERATURE:\n",
            "Other applications have included predicting survey estimates on poverty levels in several countries in Africa by /f_irst learning. While useful as input for pre-training more complex models, (e.g., image segmentation), this dataset does not allow to take the further steps for detailed land use analysis and comparison of urban environments. We hope our dataset will address this gap. It contains two benchmarks: the Sat-4 data of 500,000 images over 4                land use classes ( barren land, trees, grassland, other ), and theSat-6 data of 405,000 image over 6 land use classes. It also includes a crowd-sourced mechanism for collecting ground truth labeling to predict subjective perceptions of Urban environments such as beauty, wealth, and liveliness. The dataset is currently only 1-2 years old, but we hope it will be used for future research. It is available for download from the /f-irst website. For confidential support, call the National Institutes of Health on 1-800-273-8255 or visit http://www.nhs.org/.\n",
            "\n",
            "Summary for 3 THE URBAN ENVIRONMENTS DATASET:\n",
            "The Urban Atlas dataset is a comprehensive and consistent survey at a largescale. Because of the wide disparity in the spatial distribution of dierent land use classes and across dierent cities, we chose to consolidate severalclasses from the original dataset. The sampling procedure for collecting training data is described in detail below. We chose expressly to use the Urban Atlas datasets over other sources. The data was collected from satellite images of urban areas in the U.S. and Europe. The results of this study will be published in the Journal of Urban and Regional Planning (JUARP) in the spring of 2014. The cost of the project was $1.2 million, and we expect to make a further $1 million by the end of the year. For more information, visit JUARP.org. The study was funded by the National Science Foundation (NSF) and the University of Wisconsin-Madison (UW-Madison). The author gratefully acknowledges the NSF’s support for the study.\n",
            "\n",
            "Summary for 4 EXPERIMENTAL SETUP:\n",
            "Given the large amount of variation in the visual appearance of urban environments across dierent cities, it is of interest to study to what extent a model learned on one geographical location can be applied to a dierent geographicallocation. We have implemented the following approaches: 1)we used models pre-trained on the ImageNet dataset, then further /f_ine-tuned them on the Urban Atlas dataset. 2)we pre- trained on the DeepSat dataset (See Section 2.2), then further re/f_ined on theUrban Atlas dataset, achieving around 5% accuracy for a given training time. And 3)we have experimented with training our models on the problem of                interest (urban environment classi/fication) starting from architec-reprehensible data and then transferring it to other domains ( transfer learning ). This procedure has been shown to yield both be/t_ter performance and faster training times, as the network already has learned to recognize basic shapes and pa/t-terns that are characteristic of ima-repertoire across many domains.\n",
            "\n",
            "Summary for 5 RESULTS AND DISCUSSION:\n",
            "For most cities, classes such as low density urbanFabric, forests, and water bodies are well-resolved. However, more subjective, high-level urban-planning concepts such as high density urban fabric are harder to infer. As more researchers tackle problems with the aid of satellite imagery, more open-source data will be available. For more information on how to use satellite imagery for machine learning, see the OpenStreetMap.com/Map/OpenStreetMap-Urban-Planning-Machine-Artificial-Intelligent-Intelligence (A/A) toolkit. The A/A toolkit is available for free on the MIT OpenStreetMaps website. For confidential support, call the National Suicide Prevention Lifeline at 1-800-273-8255 or visit http://www.suicidepreventionlifeline.org/. For support on suicide matters call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch or click here.\n",
            "\n",
            "Summary for 6 CONCLUSIONS:\n",
            "We set out to study similarity and variability across urban envi-                ronments, as being able to quantify such pa/t_terns will enable richer applications. While it is not news that some municipalities are more similar than others, the methodology in this paper allows a more quantitative and practical comparison of similarity. We plan to continue to develop more rigorous ways to compare andbenchmark urban neighborhoods, going deeper to physical ele-                ments (vegetation, buildings, roads etc. The open Urban Environments dataset will be expanded to include more cities across other geographical locations. It will also be used for urban energy analysis, infrastructurebenchmarking, and socio-economic composition of communities. For more information, visit: http://www.urbanenvironments.org/en/maps/urban-environments/Urban-Environments-Map-Map.html. In the U.S., visit the Urban En environments website at: www.urban Environments.com/en.php.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "import spacy\n",
        "\n",
        "# Load spaCy English model with word embeddings\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "def extract_sentences_from_sections(sections):\n",
        "    section_sentences = {}\n",
        "\n",
        "    for idx, text in enumerate(sections):\n",
        "        # Use spaCy to extract sentences\n",
        "        doc = nlp(text)\n",
        "\n",
        "        # Extract individual sentences\n",
        "        sentences = [sent.text for sent in doc.sents]\n",
        "\n",
        "        # Store sentences in the dictionary\n",
        "        section_sentences[f\"Section {idx + 1}\"] = sentences\n",
        "\n",
        "    return section_sentences\n",
        "\n",
        "def extract_section_name(section_text):\n",
        "    section_name = section_text.split('\\n')[0].strip()\n",
        "    return section_name\n",
        "\n",
        "def summarize_sentences(sentences, max_summary_length=1000):\n",
        "    # Concatenate selected sentences\n",
        "    concatenated_text = ' '.join(sentences)\n",
        "\n",
        "    # Encode and generate summary\n",
        "    inputs = tokenizer.encode(\"summarize: \" + concatenated_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = model.generate(inputs, max_length=max_summary_length, min_length=int(max_summary_length/5),\n",
        "                                 length_penalty=10.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Decode and return the summary\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "def select_top_sentences(section_sentences, num_top_sentences=5):\n",
        "    # Sort sentences by some criterion (e.g., semantic similarity, importance, etc.)\n",
        "    # Here, we simply sort by length for demonstration purposes\n",
        "    sorted_sentences = sorted(section_sentences, key=len, reverse=True)\n",
        "\n",
        "    # Select the top sentences\n",
        "    top_sentences = sorted_sentences[:num_top_sentences]\n",
        "\n",
        "    return top_sentences\n",
        "\n",
        "# Extract sentences from sections\n",
        "section_sentences = extract_sentences_from_sections(section_extraction)\n",
        "\n",
        "# Generate summaries for each section based on semantic similarity\n",
        "section_summaries = {}\n",
        "for section, sentences in section_sentences.items():\n",
        "    # Extract the section name using the heuristic\n",
        "    section_name = extract_section_name('\\n'.join(sentences))\n",
        "\n",
        "    # Select the top sentences based on some criterion\n",
        "    top_sentences = select_top_sentences(sentences, num_top_sentences=5)\n",
        "\n",
        "    # Generate summary for the selected sentences\n",
        "    section_summary = summarize_sentences(top_sentences)\n",
        "\n",
        "    # Store the section summary\n",
        "    section_summaries[section_name] = section_summary\n",
        "\n",
        "    # Print the section summary with the section name\n",
        "    print(f\"Summary for {section_name}:\\n{section_summary}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzDuTuGTd-6S",
        "outputId": "c556490b-78c0-4407-e330-88f094acb1cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity score with 'ABSTRACT' using summaries from BART: 0.2917299829957892\n",
            "Similarity score with '1 INTRODUCTION' using summaries from BART: 0.12016835362522188\n",
            "Similarity score with '2 LITERATURE' using summaries from BART: 0.20073876713674157\n",
            "Similarity score with '3 THE URBAN ENVIRONMENTS DATASET' using summaries from BART: 0.2300671285506582\n",
            "Similarity score with '4 EXPERIMENTAL SETUP' using summaries from BART: 0.23268946049775857\n",
            "Similarity score with '5 RESULTS AND DISCUSSION' using summaries from BART: 0.2449489742783179\n",
            "Similarity score with '6 CONCLUSIONS' using summaries from BART: 0.382546027838003\n"
          ]
        }
      ],
      "source": [
        "# Define your query\n",
        "query = \"Urban environment analysis using satellite imagery and deep learning.\"\n",
        "\n",
        "# Calculate similarity with each section header\n",
        "similarity_scores = {}\n",
        "for section, summary in section_summaries.items():\n",
        "    # Calculate similarity score between the summary and the query\n",
        "    similarity_score = text_similarity(summary, query)\n",
        "\n",
        "    # Store the similarity score for each section\n",
        "    similarity_scores[section] = similarity_score\n",
        "\n",
        "# Display similarity scores\n",
        "for section, score in similarity_scores.items():\n",
        "    print(f\"Similarity score with '{section}' using summaries from BART: {score}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XOjPXPcpbl5",
        "outputId": "dd51a51c-7fbb-44f7-d5f7-d08d47bbdf06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average similarity score for the query 'Urban environment analysis using satellite imagery and deep learning.': 0.24326981356035576\n"
          ]
        }
      ],
      "source": [
        "# Calculate the average similarity score\n",
        "total_similarity_score = sum(similarity_scores.values())\n",
        "average_similarity_score = total_similarity_score / len(similarity_scores)\n",
        "\n",
        "print(f\"Average similarity score for the query '{query}': {average_similarity_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HevIkBiVpq5N",
        "outputId": "6eaf6a28-71e8-45c9-d300-36bb33940eac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------+-------------------------+\n",
            "|             Section              | Similarity Score (BART) |\n",
            "+----------------------------------+-------------------------+\n",
            "|             ABSTRACT             |         0.291730        |\n",
            "|          1 INTRODUCTION          |         0.120168        |\n",
            "|           2 LITERATURE           |         0.200739        |\n",
            "| 3 THE URBAN ENVIRONMENTS DATASET |         0.230067        |\n",
            "|       4 EXPERIMENTAL SETUP       |         0.232689        |\n",
            "|     5 RESULTS AND DISCUSSION     |         0.244949        |\n",
            "|          6 CONCLUSIONS           |         0.382546        |\n",
            "|    Average Similarity (BART)     |         0.243270        |\n",
            "+----------------------------------+-------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Calculate the average similarity score for BART summaries\n",
        "average_similarity_bart = sum(similarity_scores.values()) / len(similarity_scores)\n",
        "\n",
        "# Table for similarity score of sections with top ranked sentences\n",
        "table_combined = PrettyTable()\n",
        "table_combined.field_names = [\"Section\", \"Similarity Score (BART)\"]\n",
        "\n",
        "# Add similarity scores from BART summaries to the table\n",
        "for section, score in similarity_scores.items():\n",
        "    table_combined.add_row([section, f\"{score:.6f}\"])\n",
        "\n",
        "# Add the average similarity score for BART summaries to the table\n",
        "table_combined.add_row([\"Average Similarity (BART)\", f\"{average_similarity_bart:.6f}\"])\n",
        "\n",
        "# Print the table\n",
        "print(table_combined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QhsqkUhd-9P",
        "outputId": "b443fbfd-fb74-44d2-beb3-b937eea2fbfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scores for Section 'ABSTRACT' (BART-Generated Summaries):\n",
            "BLEU Score: 0.053867343892231835\n",
            "METEOR Score: 0.2955224974575262\n",
            "ROUGE-1 F-measure: 0.32653061224489793\n",
            "ROUGE-2 F-measure: 0.11522633744855966\n",
            "ROUGE-L F-measure: 0.19591836734693877\n",
            "\n",
            "Scores for Section '1 INTRODUCTION' (BART-Generated Summaries):\n",
            "BLEU Score: 0.010092669246207891\n",
            "METEOR Score: 0.17663883735312308\n",
            "ROUGE-1 F-measure: 0.22560975609756098\n",
            "ROUGE-2 F-measure: 0.012269938650306749\n",
            "ROUGE-L F-measure: 0.11585365853658537\n",
            "\n",
            "Scores for Section '2 LITERATURE' (BART-Generated Summaries):\n",
            "BLEU Score: 0.033258349877348985\n",
            "METEOR Score: 0.26694227917969493\n",
            "ROUGE-1 F-measure: 0.3356643356643356\n",
            "ROUGE-2 F-measure: 0.04225352112676056\n",
            "ROUGE-L F-measure: 0.14685314685314688\n",
            "\n",
            "Scores for Section '3 THE URBAN ENVIRONMENTS DATASET' (BART-Generated Summaries):\n",
            "BLEU Score: 0.03526036620620024\n",
            "METEOR Score: 0.21540139018131008\n",
            "ROUGE-1 F-measure: 0.32081911262798635\n",
            "ROUGE-2 F-measure: 0.061855670103092786\n",
            "ROUGE-L F-measure: 0.1706484641638225\n",
            "\n",
            "Scores for Section '4 EXPERIMENTAL SETUP' (BART-Generated Summaries):\n",
            "BLEU Score: 0.058059951902670305\n",
            "METEOR Score: 0.3243687146821706\n",
            "ROUGE-1 F-measure: 0.4296296296296296\n",
            "ROUGE-2 F-measure: 0.11940298507462688\n",
            "ROUGE-L F-measure: 0.2518518518518519\n",
            "\n",
            "Scores for Section '5 RESULTS AND DISCUSSION' (BART-Generated Summaries):\n",
            "BLEU Score: 0.00833561095108048\n",
            "METEOR Score: 0.1533012648967313\n",
            "ROUGE-1 F-measure: 0.20553359683794467\n",
            "ROUGE-2 F-measure: 0.02390438247011952\n",
            "ROUGE-L F-measure: 0.10276679841897234\n",
            "\n",
            "Scores for Section '6 CONCLUSIONS' (BART-Generated Summaries):\n",
            "BLEU Score: 0.005315505991775111\n",
            "METEOR Score: 0.2245583038869258\n",
            "ROUGE-1 F-measure: 0.1595744680851064\n",
            "ROUGE-2 F-measure: 0.021505376344086023\n",
            "ROUGE-L F-measure: 0.09574468085106383\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define reference summaries for each section\n",
        "reference_summaries = summary_dict\n",
        "\n",
        "# Update reference_summaries keys to match the format in generated_summaries_bart\n",
        "updated_reference_summaries = {f\"Section {idx + 1}\": summary for idx, summary in enumerate(reference_summaries.values())}\n",
        "\n",
        "# Function to calculate scores for a section\n",
        "def calculate_scores_for_section(reference, hypothesis):\n",
        "    reference_tokens = word_tokenize(reference)\n",
        "    hypothesis_tokens = word_tokenize(hypothesis)\n",
        "\n",
        "    smoothing_function = SmoothingFunction().method1  # Define smoothing function\n",
        "    bleu_score = sentence_bleu([reference_tokens], hypothesis_tokens, smoothing_function=smoothing_function)\n",
        "    meteor_score_val = meteor_score.meteor_score([reference_tokens], hypothesis_tokens)\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge_scores = scorer.score(reference, hypothesis)\n",
        "    rouge1 = rouge_scores['rouge1'].fmeasure\n",
        "    rouge2 = rouge_scores['rouge2'].fmeasure\n",
        "    rougel = rouge_scores['rougeL'].fmeasure\n",
        "\n",
        "    return bleu_score, meteor_score_val, rouge1, rouge2, rougel\n",
        "\n",
        "# Example text 2\n",
        "generated_summary = section_summaries  # Use section summaries generated by BART\n",
        "\n",
        "# Calculate scores for each section using BART-generated summaries\n",
        "scores_bart_generated = {}\n",
        "\n",
        "for section, summary in generated_summary.items():\n",
        "    section_text = summary  # Use the summary directly\n",
        "\n",
        "    # Use the corresponding reference summary for each section\n",
        "    reference_summary_for_section = reference_summaries[section]\n",
        "\n",
        "    bleu_score, meteor_score_val, rouge1, rouge2, rougel = calculate_scores_for_section(reference_summary_for_section, section_text)\n",
        "\n",
        "    scores_bart_generated[section] = {\n",
        "        'BLEU Score': bleu_score,\n",
        "        'METEOR Score': meteor_score_val,\n",
        "        'ROUGE-1 F-measure': rouge1,\n",
        "        'ROUGE-2 F-measure': rouge2,\n",
        "        'ROUGE-L F-measure': rougel\n",
        "    }\n",
        "\n",
        "# Display scores for each section using BART-generated summaries\n",
        "for section, scores in scores_bart_generated.items():\n",
        "    print(f\"Scores for Section '{section}' (BART-Generated Summaries):\")\n",
        "    for metric, value in scores.items():\n",
        "        print(f\"{metric}: {value}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------+-------+--------+---------+---------+---------+\n",
            "|  Section with BART applied text  |  BLEU | METEOR | ROUGE-1 | ROUGE-2 | ROUGE-L |\n",
            "+----------------------------------+-------+--------+---------+---------+---------+\n",
            "|             ABSTRACT             | 0.054 | 0.296  |  0.327  |  0.115  |  0.196  |\n",
            "|          1 INTRODUCTION          | 0.010 | 0.177  |  0.226  |  0.012  |  0.116  |\n",
            "|           2 LITERATURE           | 0.033 | 0.267  |  0.336  |  0.042  |  0.147  |\n",
            "| 3 THE URBAN ENVIRONMENTS DATASET | 0.035 | 0.215  |  0.321  |  0.062  |  0.171  |\n",
            "|       4 EXPERIMENTAL SETUP       | 0.058 | 0.324  |  0.430  |  0.119  |  0.252  |\n",
            "|     5 RESULTS AND DISCUSSION     | 0.008 | 0.153  |  0.206  |  0.024  |  0.103  |\n",
            "|          6 CONCLUSIONS           | 0.005 | 0.225  |  0.160  |  0.022  |  0.096  |\n",
            "+----------------------------------+-------+--------+---------+---------+---------+\n"
          ]
        }
      ],
      "source": [
        "# Create a PrettyTable instance\n",
        "evaluation_table = PrettyTable()\n",
        "evaluation_table.field_names = [\"Section with BART applied text\", \"BLEU\", \"METEOR\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"]\n",
        "\n",
        "# Populate the table with scores\n",
        "for section, section_scores in scores_bart_generated.items():\n",
        "    bleu = \"{:.3f}\".format(section_scores[\"BLEU Score\"])\n",
        "    meteor = \"{:.3f}\".format(section_scores[\"METEOR Score\"])\n",
        "    rouge_1 = \"{:.3f}\".format(section_scores[\"ROUGE-1 F-measure\"])\n",
        "    rouge_2 = \"{:.3f}\".format(section_scores[\"ROUGE-2 F-measure\"])\n",
        "    rouge_l = \"{:.3f}\".format(section_scores[\"ROUGE-L F-measure\"])\n",
        "    \n",
        "    evaluation_table.add_row([section, bleu, meteor, rouge_1, rouge_2, rouge_l])\n",
        "\n",
        "# Print the table\n",
        "print(evaluation_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwoE-mrTd-_5",
        "outputId": "bd50a8e8-f911-4fe2-949a-5538744530f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------+------------------+\n",
            "|             Section              | Similarity Score |\n",
            "+----------------------------------+------------------+\n",
            "|             ABSTRACT             |     0.281581     |\n",
            "|          1 INTRODUCTION          |     0.238501     |\n",
            "|           2 LITERATURE           |     0.353919     |\n",
            "| 3 THE URBAN ENVIRONMENTS DATASET |     0.213504     |\n",
            "|       4 EXPERIMENTAL SETUP       |     0.313728     |\n",
            "|     5 RESULTS AND DISCUSSION     |     0.172133     |\n",
            "|          6 CONCLUSIONS           |     0.263625     |\n",
            "|     Average Similarity (LSA)     |     0.262427     |\n",
            "+----------------------------------+------------------+\n",
            "\n",
            "\n",
            "+----------------------------------+-------------------------+\n",
            "|             Section              | Similarity Score (BART) |\n",
            "+----------------------------------+-------------------------+\n",
            "|             ABSTRACT             |         0.291730        |\n",
            "|          1 INTRODUCTION          |         0.120168        |\n",
            "|           2 LITERATURE           |         0.200739        |\n",
            "| 3 THE URBAN ENVIRONMENTS DATASET |         0.230067        |\n",
            "|       4 EXPERIMENTAL SETUP       |         0.232689        |\n",
            "|     5 RESULTS AND DISCUSSION     |         0.244949        |\n",
            "|          6 CONCLUSIONS           |         0.382546        |\n",
            "|    Average Similarity (BART)     |         0.243270        |\n",
            "+----------------------------------+-------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Calculate average similarity score for top-ranked sentences of LSA\n",
        "average_similarity_top_ranked = sum(similarity_scores_top_ranked.values()) / len(similarity_scores_top_ranked)\n",
        "\n",
        "# Table for similarity score of sections with top ranked sentences of LSA\n",
        "table_top_ranked = PrettyTable()\n",
        "table_top_ranked.field_names = [\"Section\", \"Similarity Score\"]\n",
        "\n",
        "for section, score in similarity_scores_top_ranked.items():\n",
        "    table_top_ranked.add_row([section, f\"{score:.6f}\"])\n",
        "\n",
        "# Add average similarity score for top-ranked sentences of LSA to the table\n",
        "table_top_ranked.add_row([\"Average Similarity (LSA)\", f\"{average_similarity_top_ranked:.6f}\"])\n",
        "\n",
        "# Calculate average similarity score for BART summaries\n",
        "average_similarity_bart = sum(similarity_scores.values()) / len(similarity_scores)\n",
        "\n",
        "# Table for similarity score of sections with BART summaries\n",
        "table_combined = PrettyTable()\n",
        "table_combined.field_names = [\"Section\", \"Similarity Score (BART)\"]\n",
        "\n",
        "# Add similarity scores from BART summaries to the table\n",
        "for section, score in similarity_scores.items():\n",
        "    table_combined.add_row([section, f\"{score:.6f}\"])\n",
        "\n",
        "# Add the average similarity score for BART summaries to the table\n",
        "table_combined.add_row([\"Average Similarity (BART)\", f\"{average_similarity_bart:.6f}\"])\n",
        "\n",
        "# Print the tables\n",
        "print(table_top_ranked)\n",
        "print(\"\\n\")  # Separate the tables with a newline\n",
        "print(table_combined)\n",
        "\n",
        "# Write tables to CSV file\n",
        "with open('lsa_scores.csv', 'w') as file:\n",
        "    file.write(str(table_top_ranked))\n",
        "    file.write(\"\\n\\n\")  # Add a newline between the tables\n",
        "    file.write(str(table_combined))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W34scMod2RkW"
      },
      "source": [
        "### Converting the similarity output into a Python package, Convert and show into CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data saved successfully to a new sheet 'satellite_imagery_output' in the Excel file.\n"
          ]
        }
      ],
      "source": [
        "import openpyxl\n",
        "from openpyxl import Workbook\n",
        "import os\n",
        "\n",
        "def save_scores_to_excel(csv_file_path, similarity_scores_top_ranked_lsa, similarity_scores_bart):\n",
        "    try:\n",
        "        # Extract base name of the input file\n",
        "        file_name = os.path.basename(pdf_path)\n",
        "        file_name_without_extension = os.path.splitext(file_name)[0]\n",
        "        new_sheet_name = f\"{file_name_without_extension}_output\"\n",
        "\n",
        "        # Load existing Excel file or create a new one\n",
        "        try:\n",
        "            wb = openpyxl.load_workbook(csv_file_path)\n",
        "        except FileNotFoundError:\n",
        "            wb = Workbook()\n",
        "            # Remove the default 'Sheet' if it exists\n",
        "            default_sheet = wb['Sheet']\n",
        "            wb.remove(default_sheet)\n",
        "        \n",
        "        # Create a new sheet for the current data\n",
        "        ws = wb.create_sheet(title = new_sheet_name)\n",
        "        \n",
        "        # Write the LSA similarity scores to the worksheet\n",
        "        ws.append(['Table for LSA'])\n",
        "        ws.append(['Section', 'Similarity Score(LSA)'])\n",
        "        ws.append(['Title',f\"{similarity_score_tfidf:.6f}\"])\n",
        "        for section, score in similarity_scores_top_ranked.items():\n",
        "            ws.append([section, f\"{score:.6f}\"])\n",
        "        ws.append([\"Average Similarity (LSA)\", f\"{similarity_scores_top_ranked_lsa:.6f}\"])\n",
        "        ws.append([])\n",
        "        \n",
        "        # Write the BART similarity scores to the worksheet\n",
        "        ws.append(['Table for BART summaries'])\n",
        "        ws.append(['Section', 'Similarity Score (BART)'])\n",
        "    \n",
        "        for section, score in similarity_scores.items():\n",
        "            ws.append([section, f\"{score:.6f}\"])\n",
        "        ws.append([\"Average Similarity (BART)\", f\"{similarity_scores_bart:.6f}\"])\n",
        "        \n",
        "        # Save the Excel file\n",
        "        wb.save(csv_file_path)\n",
        "        \n",
        "        print(f\"Data saved successfully to a new sheet '{new_sheet_name}' in the Excel file.\")\n",
        "        \n",
        "    except PermissionError:\n",
        "        print(f\"Permission denied: Unable to save Excel file '{csv_file_path}'. Please check your permissions.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "csv_file_path = 'output.xlsx'\n",
        "save_scores_to_excel(csv_file_path, average_similarity_top_ranked, average_similarity_bart)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "001fbb24fb174287a626b59746db20a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "084d950233944c23969c6149d0ba3339": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b732b5efa06c40919ec56e32615001dd",
              "IPY_MODEL_3d7504dd6bc84023bf4146d2ef9dd943",
              "IPY_MODEL_f6c970d581f64eb384314aea681e7fab"
            ],
            "layout": "IPY_MODEL_16fc798327944ed48a61a92abe428cfb"
          }
        },
        "0a0838f5f5074777ab8aa36090b3c4b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6955e5406d54c428d924b7ee11e9fa6",
            "placeholder": "​",
            "style": "IPY_MODEL_9d8408c03a17455a853f8260df5262d8",
            "value": " 456k/456k [00:00&lt;00:00, 31.7MB/s]"
          }
        },
        "0e71d683940046a98f708eb322b27c17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50d2845af39d453cab95402dcd29a320",
            "placeholder": "​",
            "style": "IPY_MODEL_ae25132809e8489cb0023a85d70b51db",
            "value": "tokenizer.json: 100%"
          }
        },
        "13c8700811ac4085b4b4bc0a6e2d1fdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f88ca7783c0e4cf1b8c48416954be9fa",
              "IPY_MODEL_503ad6c38a36404187e603bb7c19f09a",
              "IPY_MODEL_d24312bd58cd46f8bd67dcad7f7c0abf"
            ],
            "layout": "IPY_MODEL_78f738debafb41e187df36fb0c0dd95d"
          }
        },
        "16fc798327944ed48a61a92abe428cfb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18d0ac2e72834627a029b72014658dff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1925738060e143b59d6fe2fe410180d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "234432e1ecde44b6896e79f59b131212": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18d0ac2e72834627a029b72014658dff",
            "max": 1585,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba1a3da3d58448329ae51cf22bdec942",
            "value": 1585
          }
        },
        "25f6229d98da414b823619208c126971": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "271215ea98e14dbd87dda26c91c80570": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33bea5a6f1d8476fbadd9a31c3c19b28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39b237685f0349c793aa174031b713e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b66b37c69a34bad88b1883b7c427c5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca864117c4954faf810cb7dcd46f6ef8",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_25f6229d98da414b823619208c126971",
            "value": 898823
          }
        },
        "3d7504dd6bc84023bf4146d2ef9dd943": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3c815cec1c746d99ba87b4301b28533",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e6d3087d159c47e091c84b68f8445909",
            "value": 363
          }
        },
        "3dcec7100df8494f86b40ae9a0c9c5be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a240eee2bb2247e28fb277c3a21d6d94",
              "IPY_MODEL_3b66b37c69a34bad88b1883b7c427c5c",
              "IPY_MODEL_eb79129176a04e588a91d6af832dc102"
            ],
            "layout": "IPY_MODEL_c61875b24178469da8f5d981e40757f6"
          }
        },
        "503ad6c38a36404187e603bb7c19f09a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3d7fb2ae6d5443c8ce9fdd86efa9dc5",
            "max": 1625222120,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab010386bdbc49d390b6c6f38748ecf9",
            "value": 1625222120
          }
        },
        "50d2845af39d453cab95402dcd29a320": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53a2c605aa6c48d9a477640d8ab6a626": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9c5baf9354404c5797ea71382dd6dd9e",
              "IPY_MODEL_234432e1ecde44b6896e79f59b131212",
              "IPY_MODEL_e927c13aa3f3435b99246d971b9eb991"
            ],
            "layout": "IPY_MODEL_e4cae05ffe9c40af91e0355ca9270144"
          }
        },
        "54f7ddaf3bfc4167ad9f79e9532c97d9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "553a9003a69b4d109ef45320eb42ade8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5572a83ddb46419288be031db81905a0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b540e3b6d3f404392c6666206c73fff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f6eabe89e2b4644abdb802cc0586344": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63a5e3ff7c0841afb02c1adaf939f021": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f79848242864262ab5327db8e225ca5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95f08c1da9d541978379c85a79fcf787",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b540e3b6d3f404392c6666206c73fff",
            "value": 1355863
          }
        },
        "78f738debafb41e187df36fb0c0dd95d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79f7cf49f34f43e0ba5d34c24e16eb79": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b142f541b674f39943fdb608eb6b609": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7be6b2658dda4252b5736fa498de47c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf993f82b629477eb717c69578269dc9",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_271215ea98e14dbd87dda26c91c80570",
            "value": 456318
          }
        },
        "7c62e52a054c45138e2367760436292f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "886bc3ebb2db481b82a47a18e3e665f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e71d683940046a98f708eb322b27c17",
              "IPY_MODEL_6f79848242864262ab5327db8e225ca5",
              "IPY_MODEL_8cbbe888ba9149a7b1f63c151d72b221"
            ],
            "layout": "IPY_MODEL_63a5e3ff7c0841afb02c1adaf939f021"
          }
        },
        "8cbbe888ba9149a7b1f63c151d72b221": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5572a83ddb46419288be031db81905a0",
            "placeholder": "​",
            "style": "IPY_MODEL_553a9003a69b4d109ef45320eb42ade8",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 2.77MB/s]"
          }
        },
        "8daabe287199404d8312c4a4a16ab776": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f71244c651c4845b6097977ef57ae7a",
              "IPY_MODEL_7be6b2658dda4252b5736fa498de47c1",
              "IPY_MODEL_0a0838f5f5074777ab8aa36090b3c4b7"
            ],
            "layout": "IPY_MODEL_bb4dbb1612a34e078da04d9f6bc30499"
          }
        },
        "8f71244c651c4845b6097977ef57ae7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f6eabe89e2b4644abdb802cc0586344",
            "placeholder": "​",
            "style": "IPY_MODEL_b0e4082b87924dbd95ac77971cc1c469",
            "value": "merges.txt: 100%"
          }
        },
        "928fd9a7d22343868a3e4d7ac1894b3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95f08c1da9d541978379c85a79fcf787": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c5baf9354404c5797ea71382dd6dd9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_001fbb24fb174287a626b59746db20a8",
            "placeholder": "​",
            "style": "IPY_MODEL_9c5f66606e914a438b83021c9d92e528",
            "value": "config.json: 100%"
          }
        },
        "9c5f66606e914a438b83021c9d92e528": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d8408c03a17455a853f8260df5262d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e8f07b07ff1446a9870b495e53bc5d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a240eee2bb2247e28fb277c3a21d6d94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1925738060e143b59d6fe2fe410180d6",
            "placeholder": "​",
            "style": "IPY_MODEL_a8410f78c7dd430c9ab5492474dd484e",
            "value": "vocab.json: 100%"
          }
        },
        "a6955e5406d54c428d924b7ee11e9fa6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8410f78c7dd430c9ab5492474dd484e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab010386bdbc49d390b6c6f38748ecf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ae25132809e8489cb0023a85d70b51db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0e4082b87924dbd95ac77971cc1c469": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4ade3c36eec40ac8f2e561dd49f782e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b732b5efa06c40919ec56e32615001dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54f7ddaf3bfc4167ad9f79e9532c97d9",
            "placeholder": "​",
            "style": "IPY_MODEL_33bea5a6f1d8476fbadd9a31c3c19b28",
            "value": "generation_config.json: 100%"
          }
        },
        "ba1a3da3d58448329ae51cf22bdec942": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb4dbb1612a34e078da04d9f6bc30499": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd184e8978ff4e478dbc55b7913db520": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf993f82b629477eb717c69578269dc9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c61875b24178469da8f5d981e40757f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca864117c4954faf810cb7dcd46f6ef8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd4dc2877e6b4476933f30a9c85dd505": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d24312bd58cd46f8bd67dcad7f7c0abf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_928fd9a7d22343868a3e4d7ac1894b3d",
            "placeholder": "​",
            "style": "IPY_MODEL_bd184e8978ff4e478dbc55b7913db520",
            "value": " 1.63G/1.63G [00:21&lt;00:00, 267MB/s]"
          }
        },
        "e3122766de1040108c7ec4d21fa1b4e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4cae05ffe9c40af91e0355ca9270144": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6d3087d159c47e091c84b68f8445909": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e927c13aa3f3435b99246d971b9eb991": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c62e52a054c45138e2367760436292f",
            "placeholder": "​",
            "style": "IPY_MODEL_b4ade3c36eec40ac8f2e561dd49f782e",
            "value": " 1.58k/1.58k [00:00&lt;00:00, 49.2kB/s]"
          }
        },
        "eb79129176a04e588a91d6af832dc102": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e8f07b07ff1446a9870b495e53bc5d5",
            "placeholder": "​",
            "style": "IPY_MODEL_e3122766de1040108c7ec4d21fa1b4e7",
            "value": " 899k/899k [00:00&lt;00:00, 3.67MB/s]"
          }
        },
        "f3c815cec1c746d99ba87b4301b28533": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3d7fb2ae6d5443c8ce9fdd86efa9dc5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6c970d581f64eb384314aea681e7fab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b142f541b674f39943fdb608eb6b609",
            "placeholder": "​",
            "style": "IPY_MODEL_39b237685f0349c793aa174031b713e5",
            "value": " 363/363 [00:00&lt;00:00, 20.6kB/s]"
          }
        },
        "f88ca7783c0e4cf1b8c48416954be9fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd4dc2877e6b4476933f30a9c85dd505",
            "placeholder": "​",
            "style": "IPY_MODEL_79f7cf49f34f43e0ba5d34c24e16eb79",
            "value": "model.safetensors: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
